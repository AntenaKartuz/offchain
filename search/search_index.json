{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Documentation for version: v0.2.2 Overview offchain is an open-source Python package that makes it easy to index both onchain and offchain NFT metadata. Now anyone can define and standardize the indexing of custom NFT metadata schemas. offchain powers the Zora Indexer, meaning any contributions to metadata parsing implementations will be reflected in the Zora API. Check out the Contributing page to implement custom metadata. Lastly, please review the Core Concepts page before reading the rest of the documentation. Installation with pip: pip install offchain with poetry: poetry add offchain from repository: pip install git+https://github.com/ourzora/offchain.git","title":"Getting Started"},{"location":"#getting-started","text":"Documentation for version: v0.2.2","title":"Getting Started"},{"location":"#overview","text":"offchain is an open-source Python package that makes it easy to index both onchain and offchain NFT metadata. Now anyone can define and standardize the indexing of custom NFT metadata schemas. offchain powers the Zora Indexer, meaning any contributions to metadata parsing implementations will be reflected in the Zora API. Check out the Contributing page to implement custom metadata. Lastly, please review the Core Concepts page before reading the rest of the documentation.","title":"Overview"},{"location":"#installation","text":"with pip: pip install offchain with poetry: poetry add offchain from repository: pip install git+https://github.com/ourzora/offchain.git","title":"Installation"},{"location":"changelog/","text":"Changelog v0.2.2 Go deep on making things as async as they possibly can v0.2.1 Add async support for custom adapters v0.2.0 Add async support for MetadataPipeline v0.1.5 fix zora legacy media parsing update release docs v0.1.4 bug fix: parse mime type for Manifold NFT metadata v0.1.3 bug fix: add a type check to should_parse_token() in OpenseaParser to validate that raw_data is a dict v0.1.2 bug fix: a typo resulted in token field being assigned to the model class, rather than being specified as a type annotation. v0.1.1 add a get_token_metadata() function for simple use cases add basic validation logic for chain_identifier field on Token expose get_token_metadata , Metadata , MetadataFetcher , MetadataPipeline , MetadataProcessingError , and Token as root-level imports v0.1.0 define all interfaces and pipeline components add documentation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v022","text":"Go deep on making things as async as they possibly can","title":"v0.2.2"},{"location":"changelog/#v021","text":"Add async support for custom adapters","title":"v0.2.1"},{"location":"changelog/#v020","text":"Add async support for MetadataPipeline","title":"v0.2.0"},{"location":"changelog/#v015","text":"fix zora legacy media parsing update release docs","title":"v0.1.5"},{"location":"changelog/#v014","text":"bug fix: parse mime type for Manifold NFT metadata","title":"v0.1.4"},{"location":"changelog/#v013","text":"bug fix: add a type check to should_parse_token() in OpenseaParser to validate that raw_data is a dict","title":"v0.1.3"},{"location":"changelog/#v012","text":"bug fix: a typo resulted in token field being assigned to the model class, rather than being specified as a type annotation.","title":"v0.1.2"},{"location":"changelog/#v011","text":"add a get_token_metadata() function for simple use cases add basic validation logic for chain_identifier field on Token expose get_token_metadata , Metadata , MetadataFetcher , MetadataPipeline , MetadataProcessingError , and Token as root-level imports","title":"v0.1.1"},{"location":"changelog/#v010","text":"define all interfaces and pipeline components add documentation","title":"v0.1.0"},{"location":"concepts/","text":"Core Concepts This section will walk through the different components of offchain as well as how it works. Please make sure to read this section first before diving into the rest of the documentation. Key Components Pipeline : Orchestrates the metadata fetching and normalizing process for multiple tokens. Adapter : Parses the metadata url into an acceptable request format for the fetcher i.e. converting an IPFS Hash to a valid URL. Fetcher : Makes network requests to a given uri to fetch data. Parser : Parses raw data into a standardized metadata format ContractCaller: Makes RPC calls to NFT contracts to retrieve the uri if not provided. Parser Types CollectionParsers : Used for specific NFT contract addresses that have unique metadata e.g. Autoglphys, Nouns, and ENS. SchemaParsers : Used for general purpose formatting across many NFT collections e.g. Opensea Metadata Standard. How Offchain Works An overview of how offchain retrieves NFT metadata. The Pipeline is initialized with a ContractCaller , a Fetcher , a list of Adapters , and a list of Parsers . An array of Tokens is passed into the pipeline to fetch. If no uri is provided for the tokens, the ContractCaller will fetch it by calling tokenURI(uint256) on the contract. The Adapters to attempt to format the uri into a valid url. The Fetcher then makes a request to the url and attempts to get JSON metadata for the token. The Parsers attempt to parse the token metadata into an acceptable format. Each parser runs in the order they were passed in. By default, the ordering is CollectionParsers , SchemaParsers , and then CatchallParsers . The pipeline returns the result from the first parser that is successful, unless a metadata_selector_fn is specified. However, the pipeline returns a MetadataProcessingError if no parser is successful for that specific token. Token Interface This interface is how an NFT is passed into the pipeline for fetching. collection_address : The token's contract address. token_id : The unique identifier for a token within a collection. chain_identifier : The network and chain for the token. Defaults to \"ETHEREUM-MAINNET\" if nothing is passed in. uri : The url where the metadata lives. Defaults to fetching from the contract directly if nothing is passed in. # Gets metadata for Azuki #40 from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( collection_address = \"0xED5AF388653567Af2F388E6224dC7C4b3241C544\" , token_id = 40 , chain_identifier = \"ETHEREUM-MAINNET\" , uri = \"https://ikzttp.mypinata.cloud/ipfs/QmQFkLSQysj94s5GvTHPyzTxrawwtjgiiYS2TBLgrvw8CW/40\" ) metadata = pipeline . run ([ token ])[ 0 ] RPC Provider By default, the pipeline uses https://cloudflare-eth.com as the provider for the ContractCaller . This is a free Ethereum RPC provider, which means that it is very easy to exceed the rate-limit. The code below illustrates how to use a custom RPC provider to prevent getting rate-limited if token uris need to be retireved from the contract: from offchain import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller ) Next, check out the Usage section to see how to use offchain .","title":"Core Concepts"},{"location":"concepts/#core-concepts","text":"This section will walk through the different components of offchain as well as how it works. Please make sure to read this section first before diving into the rest of the documentation.","title":"Core Concepts"},{"location":"concepts/#key-components","text":"Pipeline : Orchestrates the metadata fetching and normalizing process for multiple tokens. Adapter : Parses the metadata url into an acceptable request format for the fetcher i.e. converting an IPFS Hash to a valid URL. Fetcher : Makes network requests to a given uri to fetch data. Parser : Parses raw data into a standardized metadata format ContractCaller: Makes RPC calls to NFT contracts to retrieve the uri if not provided.","title":"Key Components"},{"location":"concepts/#parser-types","text":"CollectionParsers : Used for specific NFT contract addresses that have unique metadata e.g. Autoglphys, Nouns, and ENS. SchemaParsers : Used for general purpose formatting across many NFT collections e.g. Opensea Metadata Standard.","title":"Parser Types"},{"location":"concepts/#how-offchain-works","text":"An overview of how offchain retrieves NFT metadata. The Pipeline is initialized with a ContractCaller , a Fetcher , a list of Adapters , and a list of Parsers . An array of Tokens is passed into the pipeline to fetch. If no uri is provided for the tokens, the ContractCaller will fetch it by calling tokenURI(uint256) on the contract. The Adapters to attempt to format the uri into a valid url. The Fetcher then makes a request to the url and attempts to get JSON metadata for the token. The Parsers attempt to parse the token metadata into an acceptable format. Each parser runs in the order they were passed in. By default, the ordering is CollectionParsers , SchemaParsers , and then CatchallParsers . The pipeline returns the result from the first parser that is successful, unless a metadata_selector_fn is specified. However, the pipeline returns a MetadataProcessingError if no parser is successful for that specific token.","title":"How Offchain Works"},{"location":"concepts/#token-interface","text":"This interface is how an NFT is passed into the pipeline for fetching. collection_address : The token's contract address. token_id : The unique identifier for a token within a collection. chain_identifier : The network and chain for the token. Defaults to \"ETHEREUM-MAINNET\" if nothing is passed in. uri : The url where the metadata lives. Defaults to fetching from the contract directly if nothing is passed in. # Gets metadata for Azuki #40 from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( collection_address = \"0xED5AF388653567Af2F388E6224dC7C4b3241C544\" , token_id = 40 , chain_identifier = \"ETHEREUM-MAINNET\" , uri = \"https://ikzttp.mypinata.cloud/ipfs/QmQFkLSQysj94s5GvTHPyzTxrawwtjgiiYS2TBLgrvw8CW/40\" ) metadata = pipeline . run ([ token ])[ 0 ]","title":"Token Interface"},{"location":"concepts/#rpc-provider","text":"By default, the pipeline uses https://cloudflare-eth.com as the provider for the ContractCaller . This is a free Ethereum RPC provider, which means that it is very easy to exceed the rate-limit. The code below illustrates how to use a custom RPC provider to prevent getting rate-limited if token uris need to be retireved from the contract: from offchain import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller ) Next, check out the Usage section to see how to use offchain .","title":"RPC Provider"},{"location":"contributing/collection_parser/","text":"Contributing a Collection Parser A guide on how to contribute a collection parser. Step 1: Determine the Type of Parser Before implementing your parser, familiarize yourself with the BaseParser , CollectionParser , and SchemaParser base classes. A parser will be one of the following: CollectionParser : Runs based on a token's contract address. SchemaParser : Runs based on the shape of the token's metadata. We're building a CollectionParser for ENS because it is the only NFT collection that uses this metadata schema. Collection parsers are great for one-off collections with unique metadata. class ENSParser ( CollectionParser ): pass Step 2: Define the Selection Criteria The next step is to define your parser's selection criteria. This tells the pipeline which tokens to run your parser on. The selection criteria for a collection parser is defined by a _COLLECTION_ADDRESSES class variable, which tells the parser which collection address(es) to run on. The ENS collection parser will only run on tokens with the ENS contract address 0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85 . class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] Step 3: Write the Parsing Implementation Step 3a: Construct the Token URI The token uri is needed to tell the parser where to fetch the metadata from. If the token uri is not passed in as part of the input, the pipeline will attempt to fetch it from the tokenURI(uint256) function on the contract. Note, it is not uncommon for token uris to be base64 encoded data is stored entirely on chain e.g. Nouns, Zorbs. ENS hosts their own metadata service and token uris are constructed in the following format: https://metadata.ens.domains/<chain_name>/<collection_address>/<token_id>/ class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) Let's use this ENS NFT as an example: Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) If we pass it into the parser, we'll get the following uri: https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396 Step 3b: Fetch Metadata From the Token URI Now we can use the Fetcher to get the raw JSON data from the token uri. By default, the parser is initialized with a Fetcher instance with an HTTP adapter. raw_data = self . fetcher . fetch_content ( token . uri ) This should return the following data from the ENS metadata service: { \"is_normalized\" : true , \"name\" : \"steev.eth\" , \"description\" : \"steev.eth, an ENS name.\" , \"attributes\" : [ { \"trait_type\" : \"Created Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Segment Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Character Set\" , \"display_type\" : \"string\" , \"value\" : \"letter\" }, { \"trait_type\" : \"Registration Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Expiration Date\" , \"display_type\" : \"date\" , \"value\" : 1822465450000 } ], \"name_length\" : 5 , \"segment_length\" : 5 , \"url\" : \"https://app.ens.domains/name/steev.eth\" , \"version\" : 0 , \"background_image\" : \"https://metadata.ens.domains/mainnet/avatar/steev.eth\" , \"image\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" , \"image_url\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" } Step 3c: Standardize the New Metadata Format The next step is to convert the metadata into the standardized metadata format . Each field in the new metadata format should either map a field in the standardized metadata format or be added as an MetadataField under the additional_fields property. In the case of ENS, the metadata format has the following fields: { \"name\" : \"ENS name\" , \"description\" : \"Short ENS name description\" , \"attributes\" : \"Custom traits about ENS\" , \"name_length\" : \"Character length of ens name\" , \"url\" : \"ENS App URL of the name\" , \"version\" : \"ENS NFT version\" , \"background_image\" : \"Origin URL of avatar image\" , \"image_url\" : \"URL of ENS NFT image\" } Each of these fields can be mapped into the standard metadata format: Standard Metadata Field New Metadata Field token raw_data standard attributes attributes name name description description mime_type image image_url content background_image additional_fields name_length, url, version And this is how it would look programatically: class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), ) Step 4: Registering a Parser After writing your custom parser, you'll want to register it to the ParserRegistry . The ParserRegistry tracks all parsers and is used by the metadata pipeline to know which parsers to run by default. @ParserRegistry . register class ENSParser ( CollectionParser ): ... Note, in order to have the parser be registered, you'll also need to import it in offchain/metadata/parsers/__init__.py . If you're developing locally, you still need to import the ParserRegistry to register your parser. The parser must be registered in order for it to be run by default in the MetadataPipeline . In the example below, we register the ENSParser class locally and run get_token_metadata() , which leverages the MetadataPipeline . from offchain import get_token_metadata from offchain.metadata import CollectionParser from offchain.metadata.registries.parser_registry import ParserRegistry @ParserRegistry . register class ENSParser ( CollectionParser ): ... get_token_metadata ( collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = \"ETHEREUM-MAINNET\" ) Step 5: Testing the Parser Step 5a: Write Unit Tests You'll want to write tests to verify that your parser works as expected. At minimum, the should_parse_token() and parse_metadata() functions should be tested because the pipeline will call those directly. It's important to verify that the should_parse_token() function returns True if and only if a token is meant to be parsed by that parser. Given a token, parse_metadata() should normalize the raw data into the standardized metadata format. Since making network requests can be flaky, it's preferable to mock the data that would be returned by the server that hosts the metadata information. def test_ens_parser_should_parse_token ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . should_parse_token ( token = token ) == True def test_ens_parser_parses_metadata ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () fetcher . fetch_mime_type_and_size = MagicMock ( return_value = ( \"application/json\" , 41145 )) fetcher . fetch_content = MagicMock ( return_value = mocked_raw_data ) parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . parse_metadata ( token = token , raw_data = None ) == expected_metadata In addition to testing your parser, you'll need to verify that the parser has been registered and added to the pipeline correctly. The tests in tests/metadata/registries/test_parser_registry.py should break if the not modified to include your new parser class. Step 5b: Testing Manually It's always good practice to test manually as well. We can set up our pipeline using the example NFT from earlier: from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) metadata = pipeline . run ([ token ])[ 0 ] This should give us the following standardized metadata: Metadata ( token = Token ( collection_address = '0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85' token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = 'ETHEREUM-MAINNET' , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396/' ), raw_data = { 'is_normalized' : True , 'name' : 'steev.eth' , 'description' : 'steev.eth, an ENS name.' , 'attributes' : [ { 'trait_type' : 'Created Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Segment Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Character Set' , 'display_type' : 'string' , 'value' : 'letter' }, { 'trait_type' : 'Registration Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Expiration Date' , 'display_type' : 'date' , 'value' : 1822465450000 } ], 'name_length' : 5 , 'segment_length' : 5 , 'url' : 'https://app.ens.domains/name/steev.eth' , 'version' : 0 , 'background_image' : 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , 'image' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , 'image_url' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' }, attributes = [ Attribute ( trait_type = 'Created Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Segment Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Character Set' , value = 'letter' , display_type = 'string' ), Attribute ( trait_type = 'Registration Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Expiration Date' , value = '1822465450000' , display_type = 'date' ) ], standard = COLLECTION_STANDARD , name = 'steev.eth' , description = 'steev.eth, an ENS name.' , mime_type = 'application/json' , image = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , mime_type = 'image/svg+xml' ), content = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , mime_type = None ), additional_fields = [ MetadataField ( field_name = 'name_length' , type = TEXT , description = 'Character length of ens name' , value = 5 ), MetadataField ( field_name = 'url' , type = TEXT , description = 'ENS App URL of the name' , value = 'https://app.ens.domains/name/steev.eth' ) ] ) ENS collection parser source code Source code in offchain/metadata/parsers/collection/ens.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 @ParserRegistry . register class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ CollectionAddress . ENS ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): # type: ignore[no-untyped-def] try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: # type: ignore[type-arg] # noqa: E501 additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: # type: ignore[type-arg] # noqa: E501 attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return # type: ignore[return-value] return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) # noqa: E501 try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: # type: ignore[no-untyped-def, type-arg] # noqa: E501 ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } /\" f \" { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) # type: ignore[assignment] mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), ) async def gen_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = await self . fetcher . gen_fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception as e : logger . error ( f \" { self . __class__ . __name__ } fail to fetch image { image_uri =} . { str ( e ) } \" ) async def gen_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) # noqa: E501 try : content_type , size = await self . fetcher . gen_fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception as e : logger . error ( f \" { self . __class__ . __name__ } fail to fetch background image { bg_image_uri =} . { str ( e ) } \" ) async def _gen_parse_metadata_impl ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: # type: ignore[no-untyped-def, type-arg] # noqa: E501 ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } /\" f \" { token . collection_address . lower () } / { token . token_id } /\" ) raw_data , mime_type_and_size = await asyncio . gather ( self . fetcher . gen_fetch_content ( token . uri ), self . fetcher . gen_fetch_mime_type_and_size ( token . uri ), ) mime_type , _ = mime_type_and_size image , background_image = await asyncio . gather ( self . gen_image ( raw_data = raw_data ), self . gen_background_image ( raw_data = raw_data ), ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = image , content = background_image , additional_fields = self . get_additional_fields ( raw_data = raw_data ), )","title":"Collection Parser"},{"location":"contributing/collection_parser/#contributing-a-collection-parser","text":"A guide on how to contribute a collection parser.","title":"Contributing a Collection Parser"},{"location":"contributing/collection_parser/#step-1-determine-the-type-of-parser","text":"Before implementing your parser, familiarize yourself with the BaseParser , CollectionParser , and SchemaParser base classes. A parser will be one of the following: CollectionParser : Runs based on a token's contract address. SchemaParser : Runs based on the shape of the token's metadata. We're building a CollectionParser for ENS because it is the only NFT collection that uses this metadata schema. Collection parsers are great for one-off collections with unique metadata. class ENSParser ( CollectionParser ): pass","title":"Step 1: Determine the Type of Parser"},{"location":"contributing/collection_parser/#step-2-define-the-selection-criteria","text":"The next step is to define your parser's selection criteria. This tells the pipeline which tokens to run your parser on. The selection criteria for a collection parser is defined by a _COLLECTION_ADDRESSES class variable, which tells the parser which collection address(es) to run on. The ENS collection parser will only run on tokens with the ENS contract address 0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85 . class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ]","title":"Step 2: Define the Selection Criteria"},{"location":"contributing/collection_parser/#step-3-write-the-parsing-implementation","text":"","title":"Step 3: Write the Parsing Implementation"},{"location":"contributing/collection_parser/#step-3a-construct-the-token-uri","text":"The token uri is needed to tell the parser where to fetch the metadata from. If the token uri is not passed in as part of the input, the pipeline will attempt to fetch it from the tokenURI(uint256) function on the contract. Note, it is not uncommon for token uris to be base64 encoded data is stored entirely on chain e.g. Nouns, Zorbs. ENS hosts their own metadata service and token uris are constructed in the following format: https://metadata.ens.domains/<chain_name>/<collection_address>/<token_id>/ class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) Let's use this ENS NFT as an example: Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) If we pass it into the parser, we'll get the following uri: https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396","title":"Step 3a: Construct the Token URI"},{"location":"contributing/collection_parser/#step-3b-fetch-metadata-from-the-token-uri","text":"Now we can use the Fetcher to get the raw JSON data from the token uri. By default, the parser is initialized with a Fetcher instance with an HTTP adapter. raw_data = self . fetcher . fetch_content ( token . uri ) This should return the following data from the ENS metadata service: { \"is_normalized\" : true , \"name\" : \"steev.eth\" , \"description\" : \"steev.eth, an ENS name.\" , \"attributes\" : [ { \"trait_type\" : \"Created Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Segment Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Character Set\" , \"display_type\" : \"string\" , \"value\" : \"letter\" }, { \"trait_type\" : \"Registration Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Expiration Date\" , \"display_type\" : \"date\" , \"value\" : 1822465450000 } ], \"name_length\" : 5 , \"segment_length\" : 5 , \"url\" : \"https://app.ens.domains/name/steev.eth\" , \"version\" : 0 , \"background_image\" : \"https://metadata.ens.domains/mainnet/avatar/steev.eth\" , \"image\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" , \"image_url\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" }","title":"Step 3b: Fetch Metadata From the Token URI"},{"location":"contributing/collection_parser/#step-3c-standardize-the-new-metadata-format","text":"The next step is to convert the metadata into the standardized metadata format . Each field in the new metadata format should either map a field in the standardized metadata format or be added as an MetadataField under the additional_fields property. In the case of ENS, the metadata format has the following fields: { \"name\" : \"ENS name\" , \"description\" : \"Short ENS name description\" , \"attributes\" : \"Custom traits about ENS\" , \"name_length\" : \"Character length of ens name\" , \"url\" : \"ENS App URL of the name\" , \"version\" : \"ENS NFT version\" , \"background_image\" : \"Origin URL of avatar image\" , \"image_url\" : \"URL of ENS NFT image\" } Each of these fields can be mapped into the standard metadata format: Standard Metadata Field New Metadata Field token raw_data standard attributes attributes name name description description mime_type image image_url content background_image additional_fields name_length, url, version And this is how it would look programatically: class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), )","title":"Step 3c: Standardize the New Metadata Format"},{"location":"contributing/collection_parser/#step-4-registering-a-parser","text":"After writing your custom parser, you'll want to register it to the ParserRegistry . The ParserRegistry tracks all parsers and is used by the metadata pipeline to know which parsers to run by default. @ParserRegistry . register class ENSParser ( CollectionParser ): ... Note, in order to have the parser be registered, you'll also need to import it in offchain/metadata/parsers/__init__.py . If you're developing locally, you still need to import the ParserRegistry to register your parser. The parser must be registered in order for it to be run by default in the MetadataPipeline . In the example below, we register the ENSParser class locally and run get_token_metadata() , which leverages the MetadataPipeline . from offchain import get_token_metadata from offchain.metadata import CollectionParser from offchain.metadata.registries.parser_registry import ParserRegistry @ParserRegistry . register class ENSParser ( CollectionParser ): ... get_token_metadata ( collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = \"ETHEREUM-MAINNET\" )","title":"Step 4: Registering a Parser"},{"location":"contributing/collection_parser/#step-5-testing-the-parser","text":"","title":"Step 5: Testing the Parser"},{"location":"contributing/collection_parser/#step-5a-write-unit-tests","text":"You'll want to write tests to verify that your parser works as expected. At minimum, the should_parse_token() and parse_metadata() functions should be tested because the pipeline will call those directly. It's important to verify that the should_parse_token() function returns True if and only if a token is meant to be parsed by that parser. Given a token, parse_metadata() should normalize the raw data into the standardized metadata format. Since making network requests can be flaky, it's preferable to mock the data that would be returned by the server that hosts the metadata information. def test_ens_parser_should_parse_token ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . should_parse_token ( token = token ) == True def test_ens_parser_parses_metadata ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () fetcher . fetch_mime_type_and_size = MagicMock ( return_value = ( \"application/json\" , 41145 )) fetcher . fetch_content = MagicMock ( return_value = mocked_raw_data ) parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . parse_metadata ( token = token , raw_data = None ) == expected_metadata In addition to testing your parser, you'll need to verify that the parser has been registered and added to the pipeline correctly. The tests in tests/metadata/registries/test_parser_registry.py should break if the not modified to include your new parser class.","title":"Step 5a: Write Unit Tests"},{"location":"contributing/collection_parser/#step-5b-testing-manually","text":"It's always good practice to test manually as well. We can set up our pipeline using the example NFT from earlier: from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) metadata = pipeline . run ([ token ])[ 0 ] This should give us the following standardized metadata: Metadata ( token = Token ( collection_address = '0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85' token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = 'ETHEREUM-MAINNET' , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396/' ), raw_data = { 'is_normalized' : True , 'name' : 'steev.eth' , 'description' : 'steev.eth, an ENS name.' , 'attributes' : [ { 'trait_type' : 'Created Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Segment Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Character Set' , 'display_type' : 'string' , 'value' : 'letter' }, { 'trait_type' : 'Registration Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Expiration Date' , 'display_type' : 'date' , 'value' : 1822465450000 } ], 'name_length' : 5 , 'segment_length' : 5 , 'url' : 'https://app.ens.domains/name/steev.eth' , 'version' : 0 , 'background_image' : 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , 'image' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , 'image_url' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' }, attributes = [ Attribute ( trait_type = 'Created Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Segment Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Character Set' , value = 'letter' , display_type = 'string' ), Attribute ( trait_type = 'Registration Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Expiration Date' , value = '1822465450000' , display_type = 'date' ) ], standard = COLLECTION_STANDARD , name = 'steev.eth' , description = 'steev.eth, an ENS name.' , mime_type = 'application/json' , image = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , mime_type = 'image/svg+xml' ), content = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , mime_type = None ), additional_fields = [ MetadataField ( field_name = 'name_length' , type = TEXT , description = 'Character length of ens name' , value = 5 ), MetadataField ( field_name = 'url' , type = TEXT , description = 'ENS App URL of the name' , value = 'https://app.ens.domains/name/steev.eth' ) ] )","title":"Step 5b: Testing Manually"},{"location":"contributing/collection_parser/#ens-collection-parser-source-code","text":"Source code in offchain/metadata/parsers/collection/ens.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 @ParserRegistry . register class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ CollectionAddress . ENS ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): # type: ignore[no-untyped-def] try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: # type: ignore[type-arg] # noqa: E501 additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: # type: ignore[type-arg] # noqa: E501 attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return # type: ignore[return-value] return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) # noqa: E501 try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: # type: ignore[no-untyped-def, type-arg] # noqa: E501 ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } /\" f \" { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) # type: ignore[assignment] mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), ) async def gen_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = await self . fetcher . gen_fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception as e : logger . error ( f \" { self . __class__ . __name__ } fail to fetch image { image_uri =} . { str ( e ) } \" ) async def gen_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: # type: ignore[return, type-arg] # noqa: E501 bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) # noqa: E501 try : content_type , size = await self . fetcher . gen_fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception as e : logger . error ( f \" { self . __class__ . __name__ } fail to fetch background image { bg_image_uri =} . { str ( e ) } \" ) async def _gen_parse_metadata_impl ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: # type: ignore[no-untyped-def, type-arg] # noqa: E501 ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } /\" f \" { token . collection_address . lower () } / { token . token_id } /\" ) raw_data , mime_type_and_size = await asyncio . gather ( self . fetcher . gen_fetch_content ( token . uri ), self . fetcher . gen_fetch_mime_type_and_size ( token . uri ), ) mime_type , _ = mime_type_and_size image , background_image = await asyncio . gather ( self . gen_image ( raw_data = raw_data ), self . gen_background_image ( raw_data = raw_data ), ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = image , content = background_image , additional_fields = self . get_additional_fields ( raw_data = raw_data ), )","title":"ENS collection parser source code"},{"location":"contributing/guidelines/","text":"Guidelines This section is an overview for the 3 main types of contributions that are possible for offchain . Contributing a Collection Parser Collection parsers are used on specific NFT collections that have unique metadata e.g. Autoglphys, Nouns, and ENS. If you have a specific NFT collection you'd like to support, you can contribute a collection parser. Contributing a Schema Parser Schema parsers are used for general purpose formatting across many NFT collections e.g. Opensea Metadata Standard. If you have a new NFT metadata format that will be used in many different NFT contracts, you can contribute a schema parser. Contributing an Adapter Adapters are used to parse the metadata url into an acceptable request format for the fetcher to retrieve the data. For example if retrieving the IPFS hash from a contract the adapter will reformat the URI so a request can be made to an IPFS gateway. from IPFS Hash: ipfs://QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm/9559.json to Valid Format: https://gateway.pinata.cloud/ipfs/QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm/9559.json Currently supported URL formats: HTTP Data URI (base64 encoded onchain) IPFS Arweave If you have another url format you'd like to support, you can write a custom adapter to handle it.","title":"Guidelines"},{"location":"contributing/guidelines/#guidelines","text":"This section is an overview for the 3 main types of contributions that are possible for offchain .","title":"Guidelines"},{"location":"contributing/guidelines/#contributing-a-collection-parser","text":"Collection parsers are used on specific NFT collections that have unique metadata e.g. Autoglphys, Nouns, and ENS. If you have a specific NFT collection you'd like to support, you can contribute a collection parser.","title":"Contributing a Collection Parser"},{"location":"contributing/guidelines/#contributing-a-schema-parser","text":"Schema parsers are used for general purpose formatting across many NFT collections e.g. Opensea Metadata Standard. If you have a new NFT metadata format that will be used in many different NFT contracts, you can contribute a schema parser.","title":"Contributing a Schema Parser"},{"location":"contributing/guidelines/#contributing-an-adapter","text":"Adapters are used to parse the metadata url into an acceptable request format for the fetcher to retrieve the data. For example if retrieving the IPFS hash from a contract the adapter will reformat the URI so a request can be made to an IPFS gateway. from IPFS Hash: ipfs://QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm/9559.json to Valid Format: https://gateway.pinata.cloud/ipfs/QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm/9559.json Currently supported URL formats: HTTP Data URI (base64 encoded onchain) IPFS Arweave If you have another url format you'd like to support, you can write a custom adapter to handle it.","title":"Contributing an Adapter"},{"location":"contributing/publish_release/","text":"Publishing a Release This guide is specific to contributors and serves to document how we publish releases of the offchain library. It might be an interesting read, but you'll likely never use anything out of it on a day-to-day basis. Prerequisites There is a bit of polish that needs to be done before pushing your code. Knowing your Version Before merging your pull request, you'll want to make sure that the application version defined in pyproject.toml has been incremented, following semantic versioning. This will be used later when tagging the release and in the changelog. If we're currently on version 0.0.1 , and this pull request is a patch release, we'll update our version to 0.0.2 . Example [tool.poetry] name = \"offchain\" version = \"0.0.2\" Create a Changelog Entry You'll want to ensure you've written a version entry to the changelog . This will not only be published in the docs, but it will be extracted and written as the description for the release. Entries are formatted like: # Changelog ## v0.0.2 - element - another element - the best element ## v0.0.1 - worst element - another solid element - the sad element Tip Make sure the changelog file has a new line at the end. You can add as many elements as you'd like, but make sure the versions are separated by new lines in descending order and are appended with a v like shown in the above example. Update the Index Version You'll also want to update the version that's notated on the index page. Similarly to how the changelog version is formatted, make sure the version is appended with a v . Example Documentation for version: **v0.0.2** Once you've confirmed the version is correct, your changelog entry has been committed, and you have approval on your pull request, merge it into main to begin the next steps. Tag a Release Now that your changes are merged into main , we can now tag this release and configure it on GitHub. When running the command below, this will create a tag for v0.0.2 and sign it using your GPG key. Make sure your versions are appended with v . This will be the version published to PyPi later. Warning All releases must be made with a GPG signed tag. Don't create a tag without signing it. git tag -a v0.0.2 -sm \"v0.0.2\" git push origin v0.0.2 When pushing the tag, the CI pipeline will validate that your tag matches the version defined in pyproject.toml . If it doesn't, the version will automatically update to match the tag version. This is only for safety and should never happen. If the versions match and are good to go, a release branch will be created, including the commits from the tag. Publish the Release on GitHub Once you've created and pushed the tag, you should notice a draft release on GitHub. The title should be v0.0.2 , and the content should be from the changelog for that release. Success Validate that everything is correct and publish the release! This will automatically publish the release to PyPi. And you're done, congratulations! You've successfully published a new version of the offchain library!","title":"Publishing a Release"},{"location":"contributing/publish_release/#publishing-a-release","text":"This guide is specific to contributors and serves to document how we publish releases of the offchain library. It might be an interesting read, but you'll likely never use anything out of it on a day-to-day basis.","title":"Publishing a Release"},{"location":"contributing/publish_release/#prerequisites","text":"There is a bit of polish that needs to be done before pushing your code.","title":"Prerequisites"},{"location":"contributing/publish_release/#knowing-your-version","text":"Before merging your pull request, you'll want to make sure that the application version defined in pyproject.toml has been incremented, following semantic versioning. This will be used later when tagging the release and in the changelog. If we're currently on version 0.0.1 , and this pull request is a patch release, we'll update our version to 0.0.2 . Example [tool.poetry] name = \"offchain\" version = \"0.0.2\"","title":"Knowing your Version"},{"location":"contributing/publish_release/#create-a-changelog-entry","text":"You'll want to ensure you've written a version entry to the changelog . This will not only be published in the docs, but it will be extracted and written as the description for the release. Entries are formatted like: # Changelog ## v0.0.2 - element - another element - the best element ## v0.0.1 - worst element - another solid element - the sad element Tip Make sure the changelog file has a new line at the end. You can add as many elements as you'd like, but make sure the versions are separated by new lines in descending order and are appended with a v like shown in the above example.","title":"Create a Changelog Entry"},{"location":"contributing/publish_release/#update-the-index-version","text":"You'll also want to update the version that's notated on the index page. Similarly to how the changelog version is formatted, make sure the version is appended with a v . Example Documentation for version: **v0.0.2** Once you've confirmed the version is correct, your changelog entry has been committed, and you have approval on your pull request, merge it into main to begin the next steps.","title":"Update the Index Version"},{"location":"contributing/publish_release/#tag-a-release","text":"Now that your changes are merged into main , we can now tag this release and configure it on GitHub. When running the command below, this will create a tag for v0.0.2 and sign it using your GPG key. Make sure your versions are appended with v . This will be the version published to PyPi later. Warning All releases must be made with a GPG signed tag. Don't create a tag without signing it. git tag -a v0.0.2 -sm \"v0.0.2\" git push origin v0.0.2 When pushing the tag, the CI pipeline will validate that your tag matches the version defined in pyproject.toml . If it doesn't, the version will automatically update to match the tag version. This is only for safety and should never happen. If the versions match and are good to go, a release branch will be created, including the commits from the tag.","title":"Tag a Release"},{"location":"contributing/publish_release/#publish-the-release-on-github","text":"Once you've created and pushed the tag, you should notice a draft release on GitHub. The title should be v0.0.2 , and the content should be from the changelog for that release. Success Validate that everything is correct and publish the release! This will automatically publish the release to PyPi. And you're done, congratulations! You've successfully published a new version of the offchain library!","title":"Publish the Release on GitHub"},{"location":"contributing/schema_parser/","text":"Contributing a Schema Paser A guide on how to contribute a schema parser. We'll learn how to build a parser for NFTs that follow the OpenSea Metadata Standard and go over important considerations. Step 1: Determine the Type of Parser Before implementing your parser, familiarize yourself with the BaseParser , CollectionParser , and SchemaParser base classes. A parser will be one of the following: CollectionParser : Runs based on a token's contract address. SchemaParser : Runs based on the shape of the token's metadata. Since, we're building a parser for NFTs that follow the OpenSea Metadata Standard , we'll use a SchemaParser . Schema parsers are used for metadata standards that will be used across many different NFT collections. class OpenseaParser ( SchemaParser ): Step 2: Define the Selection Criteria The next step is to define your parser's selection criteria. This tells the pipeline which tokens to run your parser on. For a schema parser you'll need to override the should_parse_token() method of BaseParser to implement custom selection logic based on the shape of the metadata. For instance, if the new metadata schema contains a unique field, checking for the existence of that field would qualify as selection criteria. In this case both background_color and youtube_url are criteria for checking if an NFT follows the OpenSea Metadata Standard . def should_parse_token ( self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" return ( raw_data is not None and isinstance ( raw_data , dict ) and ( raw_data . get ( \"background_color\" ) is not None or raw_data . get ( \"youtube_url\" ) is not None ) ) Step 3: Write the Parsing Implementation If the token uri is not passed in as part of the input, the pipeline will attempt to fetch it from the tokenURI(uint256) function on the contract. Otherwise, it is expected that the parser will construct the token uri. Let's use the Song a Day collection as an example: Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x19b703f65aa7e1e775bd06c2aa0d0d08c80f1c45\" , token_id = 1351 , ) If we pass it into the parser, we'll get the following uri: ipfs://Qmb9X7yBk5iKzgnS5pfAfReFT8FVaLcf7cJGxxUFWzRMyk/1351 , which returns metadata for token #1351. Once you have the token uri, we can use the Fetcher to fetch the raw JSON data from the token uri. By default, the parser is initialized with a Fetcher instance with an IPFS adapter. raw_data = self . fetcher . fetch_content ( token . uri ) This should return the following data from the IPFS Gateway: { \"name\" : \"Obligatory Song About the iPhone 5\" , \"description\" : \"A new song, everyday, forever. Song A Day is an ever-growing collection of unique songs created by Jonathan Mann, starting January 1st, 2009. Each NFT is a 1:1 representation of that days song, and grants access to SongADAO, the orgninzation that controls all the rights and revenue to the songs. Own a piece of the collection to help govern the future of music. http://songaday.world\" , \"token_id\" : 1351 , \"image\" : \"ipfs://QmX2ZdS13khEYqpC8Jz4nm7Ub3He3g5Ws22z3QhunC2k58/1351\" , \"animation_url\" : \"ipfs://QmVHjFbGEqXfYuoPpJR4vmRacGM29KR5UenqbidJex8muB/1351\" , \"external_url\" : \"https://songaday.world/song/1351\" , \"youtube_url\" : \"https://www.youtube.com/watch?v=xBOaUo1GT0g\" , \"attributes\" : [ { \"trait_type\" : \"Date\" , \"value\" : \"2012-09-12\" }, { \"trait_type\" : \"Location\" , \"value\" : \"Brooklyn Studio\" }, { \"trait_type\" : \"Topic\" , \"value\" : \"Apple\" }, { \"trait_type\" : \"Instrument\" , \"value\" : \"Electric Guitar\" }, { \"trait_type\" : \"Mood\" , \"value\" : \"Bored\" }, { \"trait_type\" : \"Beard\" , \"value\" : \"Shadow\" }, { \"trait_type\" : \"Genre\" , \"value\" : \"Rock\" }, { \"trait_type\" : \"Style\" , \"value\" : \"Fun\" }, { \"trait_type\" : \"Length\" , \"value\" : \"0:47\" }, { \"trait_type\" : \"Key\" , \"value\" : \"E\" }, { \"trait_type\" : \"Tempo\" , \"value\" : \"90\" }, { \"trait_type\" : \"Song A Day\" , \"value\" : \"1351\" }, { \"trait_type\" : \"Year\" , \"value\" : \"2012\" }, { \"trait_type\" : \"Instrument\" , \"value\" : \"Bass\" }, { \"trait_type\" : \"Instrument\" , \"value\" : \"Drums\" }, { \"trait_type\" : \"Style\" , \"value\" : \"Catchy\" }, { \"trait_type\" : \"Proper Noun\" , \"value\" : \"iPhone\" } ] } The next step is to convert the metadata into the standardized metadata format . Each field in the new metadata format should either map a field in the standardized metadata format or be added as an MetadataField under the additional_fields property. In the case of Song a Day, the metadata format has the following fields: { \"name\" : \"NFT name\" , \"description\" : \"More info about the NFT\" , \"attributes\" : \"Custom traits\" , \"image\" : \"Image media asset\" , \"animation_url\" : \"Media asset for videos\" , \"external_url\" : \"External linking\" , \"youtube_url\" : \"Video on hosted on Youtube\" } Each of these fields can be mapped into the standard metadata format: Standard Metadata Field New Metadata Field token raw_data standard attributes attributes name name description description mime_type image image content animation_url additional_fields external_url, youtube_url Code used for parsing collections that use the OpenSea Metadata Standard : def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" mime , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) attributes = [ self . parse_attribute ( attribute ) for attribute in raw_data . get ( \"attributes\" , [])] image = None image_uri = raw_data . get ( \"image\" ) or raw_data . get ( \"image_data\" ) if image_uri : image_mime , image_size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image = MediaDetails ( size = image_size , uri = image_uri , mime_type = image_mime ) content = None content_uri = raw_data . get ( \"animation_url\" ) if content_uri : content_mime , content_size = self . fetcher . fetch_mime_type_and_size ( content_uri ) content = MediaDetails ( uri = content_uri , size = content_size , mime_type = content_mime ) if image and image . mime_type : mime = image . mime_type if content and content . mime_type : mime = content . mime_type return Metadata ( token = token , raw_data = raw_data , attributes = attributes , name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime , image = image , content = content , additional_fields = self . parse_additional_fields ( raw_data ), ) # Helper Functions def parse_attribute ( self , attribute_dict : dict ) -> Attribute : return Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) def parse_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if ( external_url := raw_data . get ( \"external_url\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"external_url\" , type = MetadataFieldType . TEXT , description = \"This is the URL that will appear below the asset's image on OpenSea \" \"and will allow users to leave OpenSea and view the item on your site.\" , value = external_url , ) ) if ( background_color := raw_data . get ( \"background_color\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"background_color\" , type = MetadataFieldType . TEXT , description = \"Background color of the item on OpenSea. Must be a six-character \" \"hexadecimal without a pre-pended #.\" , value = background_color , ) ) if ( animation_url := raw_data . get ( \"animation_url\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"animation_url\" , type = MetadataFieldType . TEXT , description = \"A URL to a multi-media attachment for the item.\" , value = animation_url , ) ) if ( youtube_url := raw_data . get ( \"youtube_url\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"youtube_url\" , type = MetadataFieldType . TEXT , description = \"A URL to a YouTube video.\" , value = youtube_url , ) ) return additional_fields Step 4: Registering a Parser After writing your custom metadata parser implementation, you'll want to register it to the ParserRegistry . The ParserRegistry tracks all parsers and is used by the metadata pipeline to know which parsers to run by default. @ParserRegistry . register class OpenseaParser ( SchemaParser ): ... Note: in order to have the parser be registered, you'll also need to import it in offchain/metadata/parsers/__init__.py . If you're developing locally, you still need to import the ParserRegistry to register your parser. The parser must be registered in order for it to be run by default in the MetadataPipeline . Step 5: Testing the Parser Lastly, you'll want to write tests to verify that your parser works as expected. At minimum, the should_parse_token() and parse_metadata() functions should be tested because the pipeline will call those directly. It's important to verify that the should_parse_token() function returns True if and only if a token is meant to be parsed by that parser. Given a token, parse_metadata() should normalize the raw data into the standardized metadata format. Since making network requests can be flaky, it's preferable to mock the data that would be returned by the server that hosts the metadata information. def test_opensea_parser_should_parse_token ( self , raw_crypto_coven_metadata ): fetcher = MetadataFetcher () ipfs_adapter = IPFSAdapter () fetcher . register_adapter ( ipfs_adapter , \"ipfs://\" ) parser = OpenseaParser ( fetcher = fetcher ) assert parser . should_parse_token ( token = self . token , raw_data = raw_crypto_coven_metadata ) == True def test_opensea_parser_parses_metadata ( self , raw_crypto_coven_metadata ): fetcher = MetadataFetcher () ipfs_adapter = IPFSAdapter () fetcher . register_adapter ( ipfs_adapter , \"ipfs://\" ) fetcher . fetch_mime_type_and_size = MagicMock ( return_value = ( \"application/json\" , \"3095\" )) parser = OpenseaParser ( fetcher = fetcher ) metadata = parser . parse_metadata ( token = self . token , raw_data = raw_crypto_coven_metadata ) assert metadata == Metadata ( token = self . token , raw_data = raw_crypto_coven_metadata , standard = None , attributes = [ Attribute ( trait_type = \"Background\" , value = \"Sepia\" , display_type = None ), Attribute ( trait_type = \"Skin Tone\" , value = \"Dawn\" , display_type = None ), Attribute ( trait_type = \"Body Shape\" , value = \"Lithe\" , display_type = None ), Attribute ( trait_type = \"Top\" , value = \"Sheer Top (Black)\" , display_type = None ), Attribute ( trait_type = \"Eyebrows\" , value = \"Medium Flat (Black)\" , display_type = None , ), Attribute ( trait_type = \"Eye Style\" , value = \"Nyx\" , display_type = None ), Attribute ( trait_type = \"Eye Color\" , value = \"Cloud\" , display_type = None ), Attribute ( trait_type = \"Mouth\" , value = \"Nyx (Mocha)\" , display_type = None ), Attribute ( trait_type = \"Hair (Front)\" , value = \"Nyx\" , display_type = None ), Attribute ( trait_type = \"Hair (Back)\" , value = \"Nyx Long\" , display_type = None ), Attribute ( trait_type = \"Hair Color\" , value = \"Steel\" , display_type = None ), Attribute ( trait_type = \"Hat\" , value = \"Witch (Black)\" , display_type = None ), Attribute ( trait_type = \"Necklace\" , value = \"Moon Necklace (Silver)\" , display_type = None , ), Attribute ( trait_type = \"Archetype of Power\" , value = \"Witch of Woe\" , display_type = None , ), Attribute ( trait_type = \"Sun Sign\" , value = \"Taurus\" , display_type = None ), Attribute ( trait_type = \"Moon Sign\" , value = \"Aquarius\" , display_type = None ), Attribute ( trait_type = \"Rising Sign\" , value = \"Capricorn\" , display_type = None ), Attribute ( trait_type = \"Will\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Wisdom\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Wonder\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Woe\" , value = \"10\" , display_type = \"number\" ), Attribute ( trait_type = \"Wit\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Wiles\" , value = \"9\" , display_type = \"number\" ), ], name = \"nyx\" , description = \"You are a WITCH of the highest order. You are borne of chaos that gives the night shape. Your magic spawns from primordial darkness. You are called oracle by those wise enough to listen. ALL THEOLOGY STEMS FROM THE TERROR OF THE FIRMAMENT!\" , mime_type = \"application/json\" , image = MediaDetails ( size = 3095 , sha256 = None , uri = \"https://cryptocoven.s3.amazonaws.com/nyx.png\" , mime_type = \"application/json\" , ), content = None , additional_fields = [ MetadataField ( field_name = \"external_url\" , type = MetadataFieldType . TEXT , description = \"This is the URL that will appear below the asset's image on OpenSea and will allow users to leave OpenSea and view the item on your site.\" , value = \"https://www.cryptocoven.xyz/witches/1\" , ), MetadataField ( field_name = \"background_color\" , type = MetadataFieldType . TEXT , description = \"Background color of the item on OpenSea. Must be a six-character hexadecimal without a pre-pended #.\" , value = \"\" , ), ], ) In addition to testing your parser, you'll need to verify that the parser has been registered and added to the pipeline correctly. The tests in tests/metadata/registries/test_parser_registry.py should break if the not modified to include your new parser class. OpenSea Schema Code View the full source code for the OpenSea schema parser here.","title":"Schema Parser"},{"location":"contributing/schema_parser/#contributing-a-schema-paser","text":"A guide on how to contribute a schema parser. We'll learn how to build a parser for NFTs that follow the OpenSea Metadata Standard and go over important considerations.","title":"Contributing a Schema Paser"},{"location":"contributing/schema_parser/#step-1-determine-the-type-of-parser","text":"Before implementing your parser, familiarize yourself with the BaseParser , CollectionParser , and SchemaParser base classes. A parser will be one of the following: CollectionParser : Runs based on a token's contract address. SchemaParser : Runs based on the shape of the token's metadata. Since, we're building a parser for NFTs that follow the OpenSea Metadata Standard , we'll use a SchemaParser . Schema parsers are used for metadata standards that will be used across many different NFT collections. class OpenseaParser ( SchemaParser ):","title":"Step 1: Determine the Type of Parser"},{"location":"contributing/schema_parser/#step-2-define-the-selection-criteria","text":"The next step is to define your parser's selection criteria. This tells the pipeline which tokens to run your parser on. For a schema parser you'll need to override the should_parse_token() method of BaseParser to implement custom selection logic based on the shape of the metadata. For instance, if the new metadata schema contains a unique field, checking for the existence of that field would qualify as selection criteria. In this case both background_color and youtube_url are criteria for checking if an NFT follows the OpenSea Metadata Standard . def should_parse_token ( self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" return ( raw_data is not None and isinstance ( raw_data , dict ) and ( raw_data . get ( \"background_color\" ) is not None or raw_data . get ( \"youtube_url\" ) is not None ) )","title":"Step 2: Define the Selection Criteria"},{"location":"contributing/schema_parser/#step-3-write-the-parsing-implementation","text":"If the token uri is not passed in as part of the input, the pipeline will attempt to fetch it from the tokenURI(uint256) function on the contract. Otherwise, it is expected that the parser will construct the token uri. Let's use the Song a Day collection as an example: Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x19b703f65aa7e1e775bd06c2aa0d0d08c80f1c45\" , token_id = 1351 , ) If we pass it into the parser, we'll get the following uri: ipfs://Qmb9X7yBk5iKzgnS5pfAfReFT8FVaLcf7cJGxxUFWzRMyk/1351 , which returns metadata for token #1351. Once you have the token uri, we can use the Fetcher to fetch the raw JSON data from the token uri. By default, the parser is initialized with a Fetcher instance with an IPFS adapter. raw_data = self . fetcher . fetch_content ( token . uri ) This should return the following data from the IPFS Gateway: { \"name\" : \"Obligatory Song About the iPhone 5\" , \"description\" : \"A new song, everyday, forever. Song A Day is an ever-growing collection of unique songs created by Jonathan Mann, starting January 1st, 2009. Each NFT is a 1:1 representation of that days song, and grants access to SongADAO, the orgninzation that controls all the rights and revenue to the songs. Own a piece of the collection to help govern the future of music. http://songaday.world\" , \"token_id\" : 1351 , \"image\" : \"ipfs://QmX2ZdS13khEYqpC8Jz4nm7Ub3He3g5Ws22z3QhunC2k58/1351\" , \"animation_url\" : \"ipfs://QmVHjFbGEqXfYuoPpJR4vmRacGM29KR5UenqbidJex8muB/1351\" , \"external_url\" : \"https://songaday.world/song/1351\" , \"youtube_url\" : \"https://www.youtube.com/watch?v=xBOaUo1GT0g\" , \"attributes\" : [ { \"trait_type\" : \"Date\" , \"value\" : \"2012-09-12\" }, { \"trait_type\" : \"Location\" , \"value\" : \"Brooklyn Studio\" }, { \"trait_type\" : \"Topic\" , \"value\" : \"Apple\" }, { \"trait_type\" : \"Instrument\" , \"value\" : \"Electric Guitar\" }, { \"trait_type\" : \"Mood\" , \"value\" : \"Bored\" }, { \"trait_type\" : \"Beard\" , \"value\" : \"Shadow\" }, { \"trait_type\" : \"Genre\" , \"value\" : \"Rock\" }, { \"trait_type\" : \"Style\" , \"value\" : \"Fun\" }, { \"trait_type\" : \"Length\" , \"value\" : \"0:47\" }, { \"trait_type\" : \"Key\" , \"value\" : \"E\" }, { \"trait_type\" : \"Tempo\" , \"value\" : \"90\" }, { \"trait_type\" : \"Song A Day\" , \"value\" : \"1351\" }, { \"trait_type\" : \"Year\" , \"value\" : \"2012\" }, { \"trait_type\" : \"Instrument\" , \"value\" : \"Bass\" }, { \"trait_type\" : \"Instrument\" , \"value\" : \"Drums\" }, { \"trait_type\" : \"Style\" , \"value\" : \"Catchy\" }, { \"trait_type\" : \"Proper Noun\" , \"value\" : \"iPhone\" } ] } The next step is to convert the metadata into the standardized metadata format . Each field in the new metadata format should either map a field in the standardized metadata format or be added as an MetadataField under the additional_fields property. In the case of Song a Day, the metadata format has the following fields: { \"name\" : \"NFT name\" , \"description\" : \"More info about the NFT\" , \"attributes\" : \"Custom traits\" , \"image\" : \"Image media asset\" , \"animation_url\" : \"Media asset for videos\" , \"external_url\" : \"External linking\" , \"youtube_url\" : \"Video on hosted on Youtube\" } Each of these fields can be mapped into the standard metadata format: Standard Metadata Field New Metadata Field token raw_data standard attributes attributes name name description description mime_type image image content animation_url additional_fields external_url, youtube_url Code used for parsing collections that use the OpenSea Metadata Standard : def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" mime , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) attributes = [ self . parse_attribute ( attribute ) for attribute in raw_data . get ( \"attributes\" , [])] image = None image_uri = raw_data . get ( \"image\" ) or raw_data . get ( \"image_data\" ) if image_uri : image_mime , image_size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image = MediaDetails ( size = image_size , uri = image_uri , mime_type = image_mime ) content = None content_uri = raw_data . get ( \"animation_url\" ) if content_uri : content_mime , content_size = self . fetcher . fetch_mime_type_and_size ( content_uri ) content = MediaDetails ( uri = content_uri , size = content_size , mime_type = content_mime ) if image and image . mime_type : mime = image . mime_type if content and content . mime_type : mime = content . mime_type return Metadata ( token = token , raw_data = raw_data , attributes = attributes , name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime , image = image , content = content , additional_fields = self . parse_additional_fields ( raw_data ), ) # Helper Functions def parse_attribute ( self , attribute_dict : dict ) -> Attribute : return Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) def parse_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if ( external_url := raw_data . get ( \"external_url\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"external_url\" , type = MetadataFieldType . TEXT , description = \"This is the URL that will appear below the asset's image on OpenSea \" \"and will allow users to leave OpenSea and view the item on your site.\" , value = external_url , ) ) if ( background_color := raw_data . get ( \"background_color\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"background_color\" , type = MetadataFieldType . TEXT , description = \"Background color of the item on OpenSea. Must be a six-character \" \"hexadecimal without a pre-pended #.\" , value = background_color , ) ) if ( animation_url := raw_data . get ( \"animation_url\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"animation_url\" , type = MetadataFieldType . TEXT , description = \"A URL to a multi-media attachment for the item.\" , value = animation_url , ) ) if ( youtube_url := raw_data . get ( \"youtube_url\" )) is not None : additional_fields . append ( MetadataField ( field_name = \"youtube_url\" , type = MetadataFieldType . TEXT , description = \"A URL to a YouTube video.\" , value = youtube_url , ) ) return additional_fields","title":"Step 3: Write the Parsing Implementation"},{"location":"contributing/schema_parser/#step-4-registering-a-parser","text":"After writing your custom metadata parser implementation, you'll want to register it to the ParserRegistry . The ParserRegistry tracks all parsers and is used by the metadata pipeline to know which parsers to run by default. @ParserRegistry . register class OpenseaParser ( SchemaParser ): ... Note: in order to have the parser be registered, you'll also need to import it in offchain/metadata/parsers/__init__.py . If you're developing locally, you still need to import the ParserRegistry to register your parser. The parser must be registered in order for it to be run by default in the MetadataPipeline .","title":"Step 4: Registering a Parser"},{"location":"contributing/schema_parser/#step-5-testing-the-parser","text":"Lastly, you'll want to write tests to verify that your parser works as expected. At minimum, the should_parse_token() and parse_metadata() functions should be tested because the pipeline will call those directly. It's important to verify that the should_parse_token() function returns True if and only if a token is meant to be parsed by that parser. Given a token, parse_metadata() should normalize the raw data into the standardized metadata format. Since making network requests can be flaky, it's preferable to mock the data that would be returned by the server that hosts the metadata information. def test_opensea_parser_should_parse_token ( self , raw_crypto_coven_metadata ): fetcher = MetadataFetcher () ipfs_adapter = IPFSAdapter () fetcher . register_adapter ( ipfs_adapter , \"ipfs://\" ) parser = OpenseaParser ( fetcher = fetcher ) assert parser . should_parse_token ( token = self . token , raw_data = raw_crypto_coven_metadata ) == True def test_opensea_parser_parses_metadata ( self , raw_crypto_coven_metadata ): fetcher = MetadataFetcher () ipfs_adapter = IPFSAdapter () fetcher . register_adapter ( ipfs_adapter , \"ipfs://\" ) fetcher . fetch_mime_type_and_size = MagicMock ( return_value = ( \"application/json\" , \"3095\" )) parser = OpenseaParser ( fetcher = fetcher ) metadata = parser . parse_metadata ( token = self . token , raw_data = raw_crypto_coven_metadata ) assert metadata == Metadata ( token = self . token , raw_data = raw_crypto_coven_metadata , standard = None , attributes = [ Attribute ( trait_type = \"Background\" , value = \"Sepia\" , display_type = None ), Attribute ( trait_type = \"Skin Tone\" , value = \"Dawn\" , display_type = None ), Attribute ( trait_type = \"Body Shape\" , value = \"Lithe\" , display_type = None ), Attribute ( trait_type = \"Top\" , value = \"Sheer Top (Black)\" , display_type = None ), Attribute ( trait_type = \"Eyebrows\" , value = \"Medium Flat (Black)\" , display_type = None , ), Attribute ( trait_type = \"Eye Style\" , value = \"Nyx\" , display_type = None ), Attribute ( trait_type = \"Eye Color\" , value = \"Cloud\" , display_type = None ), Attribute ( trait_type = \"Mouth\" , value = \"Nyx (Mocha)\" , display_type = None ), Attribute ( trait_type = \"Hair (Front)\" , value = \"Nyx\" , display_type = None ), Attribute ( trait_type = \"Hair (Back)\" , value = \"Nyx Long\" , display_type = None ), Attribute ( trait_type = \"Hair Color\" , value = \"Steel\" , display_type = None ), Attribute ( trait_type = \"Hat\" , value = \"Witch (Black)\" , display_type = None ), Attribute ( trait_type = \"Necklace\" , value = \"Moon Necklace (Silver)\" , display_type = None , ), Attribute ( trait_type = \"Archetype of Power\" , value = \"Witch of Woe\" , display_type = None , ), Attribute ( trait_type = \"Sun Sign\" , value = \"Taurus\" , display_type = None ), Attribute ( trait_type = \"Moon Sign\" , value = \"Aquarius\" , display_type = None ), Attribute ( trait_type = \"Rising Sign\" , value = \"Capricorn\" , display_type = None ), Attribute ( trait_type = \"Will\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Wisdom\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Wonder\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Woe\" , value = \"10\" , display_type = \"number\" ), Attribute ( trait_type = \"Wit\" , value = \"9\" , display_type = \"number\" ), Attribute ( trait_type = \"Wiles\" , value = \"9\" , display_type = \"number\" ), ], name = \"nyx\" , description = \"You are a WITCH of the highest order. You are borne of chaos that gives the night shape. Your magic spawns from primordial darkness. You are called oracle by those wise enough to listen. ALL THEOLOGY STEMS FROM THE TERROR OF THE FIRMAMENT!\" , mime_type = \"application/json\" , image = MediaDetails ( size = 3095 , sha256 = None , uri = \"https://cryptocoven.s3.amazonaws.com/nyx.png\" , mime_type = \"application/json\" , ), content = None , additional_fields = [ MetadataField ( field_name = \"external_url\" , type = MetadataFieldType . TEXT , description = \"This is the URL that will appear below the asset's image on OpenSea and will allow users to leave OpenSea and view the item on your site.\" , value = \"https://www.cryptocoven.xyz/witches/1\" , ), MetadataField ( field_name = \"background_color\" , type = MetadataFieldType . TEXT , description = \"Background color of the item on OpenSea. Must be a six-character hexadecimal without a pre-pended #.\" , value = \"\" , ), ], ) In addition to testing your parser, you'll need to verify that the parser has been registered and added to the pipeline correctly. The tests in tests/metadata/registries/test_parser_registry.py should break if the not modified to include your new parser class.","title":"Step 5: Testing the Parser"},{"location":"contributing/schema_parser/#opensea-schema-code","text":"View the full source code for the OpenSea schema parser here.","title":"OpenSea Schema Code"},{"location":"models/metadata/","text":"Metadata Metadata A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT raw_data dict raw metadata object fetched from token uri. standard MetadataStandard accepted metadata standard based on the format of the metadata. attributes list [ Attribute ] list of token metadata attributes. name str token name. description str token description. mime_type str metadata mime type, e.g. \"image/png\". image str nested image in the metadata. content str nested content, e.g. video, audio, etc., in the metadata. additional_fields list [ MetadataField ] any additional metadata fields that don't fit in the defined schema. Source code in offchain/metadata/models/metadata.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Metadata ( BaseModel ): \"\"\"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT raw_data (dict): raw metadata object fetched from token uri. standard (MetadataStandard): accepted metadata standard based on the format of the metadata. attributes (list[Attribute]): list of token metadata attributes. name (str, optional): token name. description (str, optional): token description. mime_type (str, optional): metadata mime type, e.g. \"image/png\". image (str, optional): nested image in the metadata. content (str, optional): nested content, e.g. video, audio, etc., in the metadata. additional_fields (list[MetadataField], optional): any additional metadata fields that don't fit in the defined schema. \"\"\" # noqa: E501 token : Token raw_data : dict # type: ignore[type-arg] attributes : list [ Attribute ] standard : Optional [ MetadataStandard ] = None name : Optional [ str ] = None description : Optional [ str ] = None mime_type : Optional [ str ] = None image : Optional [ MediaDetails ] = None content : Optional [ MediaDetails ] = None additional_fields : Optional [ list [ MetadataField ]] = None Attribute NFT metadata atttribute Attributes: Name Type Description trait_type str the attribute key. value str the attribute value. display_type str informs how the attribute is displayed. Source code in offchain/metadata/models/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 class Attribute ( BaseModel ): \"\"\"NFT metadata atttribute Attributes: trait_type (str, optional): the attribute key. value (str, optional): the attribute value. display_type (str, optional): informs how the attribute is displayed. \"\"\" trait_type : Optional [ str ] = None value : Optional [ str ] = None display_type : Optional [ str ] = None MediaDetails Metadata media information Attributes: Name Type Description size int size of the media. sha256 str the SHA256 hash of the media. uri str the uri at which the media was found. mime_type str the mime type of the media. Source code in offchain/metadata/models/metadata.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class MediaDetails ( BaseModel ): \"\"\"Metadata media information Attributes: size (int, optional): size of the media. sha256 (str, optional): the SHA256 hash of the media. uri (str, optional): the uri at which the media was found. mime_type (str, optional): the mime type of the media. \"\"\" size : Optional [ int ] = None sha256 : Optional [ str ] = None uri : Optional [ str ] = None mime_type : Optional [ str ] = None MetadataField Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: Name Type Description field_name str name of the metadata field. type MetadataFieldType metadata field type. description str a description of what this metadata field represents. value any the value of the metadata field. Source code in offchain/metadata/models/metadata.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MetadataField ( BaseModel ): \"\"\"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: field_name (str): name of the metadata field. type (MetadataFieldType): metadata field type. description (str, optional): a description of what this metadata field represents. value (any): the value of the metadata field. \"\"\" # noqa: E501 field_name : str type : MetadataFieldType description : str value : Any","title":"Metadata"},{"location":"models/metadata/#metadata","text":"","title":"Metadata"},{"location":"models/metadata/#metadata_1","text":"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT raw_data dict raw metadata object fetched from token uri. standard MetadataStandard accepted metadata standard based on the format of the metadata. attributes list [ Attribute ] list of token metadata attributes. name str token name. description str token description. mime_type str metadata mime type, e.g. \"image/png\". image str nested image in the metadata. content str nested content, e.g. video, audio, etc., in the metadata. additional_fields list [ MetadataField ] any additional metadata fields that don't fit in the defined schema. Source code in offchain/metadata/models/metadata.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Metadata ( BaseModel ): \"\"\"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT raw_data (dict): raw metadata object fetched from token uri. standard (MetadataStandard): accepted metadata standard based on the format of the metadata. attributes (list[Attribute]): list of token metadata attributes. name (str, optional): token name. description (str, optional): token description. mime_type (str, optional): metadata mime type, e.g. \"image/png\". image (str, optional): nested image in the metadata. content (str, optional): nested content, e.g. video, audio, etc., in the metadata. additional_fields (list[MetadataField], optional): any additional metadata fields that don't fit in the defined schema. \"\"\" # noqa: E501 token : Token raw_data : dict # type: ignore[type-arg] attributes : list [ Attribute ] standard : Optional [ MetadataStandard ] = None name : Optional [ str ] = None description : Optional [ str ] = None mime_type : Optional [ str ] = None image : Optional [ MediaDetails ] = None content : Optional [ MediaDetails ] = None additional_fields : Optional [ list [ MetadataField ]] = None","title":"Metadata"},{"location":"models/metadata/#attribute","text":"NFT metadata atttribute Attributes: Name Type Description trait_type str the attribute key. value str the attribute value. display_type str informs how the attribute is displayed. Source code in offchain/metadata/models/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 class Attribute ( BaseModel ): \"\"\"NFT metadata atttribute Attributes: trait_type (str, optional): the attribute key. value (str, optional): the attribute value. display_type (str, optional): informs how the attribute is displayed. \"\"\" trait_type : Optional [ str ] = None value : Optional [ str ] = None display_type : Optional [ str ] = None","title":"Attribute"},{"location":"models/metadata/#mediadetails","text":"Metadata media information Attributes: Name Type Description size int size of the media. sha256 str the SHA256 hash of the media. uri str the uri at which the media was found. mime_type str the mime type of the media. Source code in offchain/metadata/models/metadata.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class MediaDetails ( BaseModel ): \"\"\"Metadata media information Attributes: size (int, optional): size of the media. sha256 (str, optional): the SHA256 hash of the media. uri (str, optional): the uri at which the media was found. mime_type (str, optional): the mime type of the media. \"\"\" size : Optional [ int ] = None sha256 : Optional [ str ] = None uri : Optional [ str ] = None mime_type : Optional [ str ] = None","title":"MediaDetails"},{"location":"models/metadata/#metadatafield","text":"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: Name Type Description field_name str name of the metadata field. type MetadataFieldType metadata field type. description str a description of what this metadata field represents. value any the value of the metadata field. Source code in offchain/metadata/models/metadata.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MetadataField ( BaseModel ): \"\"\"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: field_name (str): name of the metadata field. type (MetadataFieldType): metadata field type. description (str, optional): a description of what this metadata field represents. value (any): the value of the metadata field. \"\"\" # noqa: E501 field_name : str type : MetadataFieldType description : str value : Any","title":"MetadataField"},{"location":"models/metadata_processing_error/","text":"MetadataProcessingError Interface for metadata processing errors and relevant contextual information. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT error_type str the class of caught exception. error_message str the error message of the caught exception. Source code in offchain/metadata/models/metadata_processing_error.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class MetadataProcessingError ( BaseModel ): \"\"\"Interface for metadata processing errors and relevant contextual information. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT error_type (str): the class of caught exception. error_message (str): the error message of the caught exception. \"\"\" # noqa: E501 token : Token error_type : str error_message : str @staticmethod def from_token_and_error ( token : Token , e : Exception ) -> \"MetadataProcessingError\" : return MetadataProcessingError ( token = token , error_type = e . __class__ . __name__ , error_message = str ( e ), )","title":"MetadataProcessingError"},{"location":"models/metadata_processing_error/#metadataprocessingerror","text":"Interface for metadata processing errors and relevant contextual information. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT error_type str the class of caught exception. error_message str the error message of the caught exception. Source code in offchain/metadata/models/metadata_processing_error.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class MetadataProcessingError ( BaseModel ): \"\"\"Interface for metadata processing errors and relevant contextual information. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT error_type (str): the class of caught exception. error_message (str): the error message of the caught exception. \"\"\" # noqa: E501 token : Token error_type : str error_message : str @staticmethod def from_token_and_error ( token : Token , e : Exception ) -> \"MetadataProcessingError\" : return MetadataProcessingError ( token = token , error_type = e . __class__ . __name__ , error_message = str ( e ), )","title":"MetadataProcessingError"},{"location":"models/token/","text":"Token Token interface with all information required to uniquely identify an NFT. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of token. token_id int unique identifier of token. uri str the uri where the metadata is stored. Source code in offchain/metadata/models/token.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Token ( BaseModel ): \"\"\"Token interface with all information required to uniquely identify an NFT. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of token. token_id (int): unique identifier of token. uri (str, optional): the uri where the metadata is stored. \"\"\" collection_address : str token_id : int chain_identifier : str = \"ETHEREUM-MAINNET\" uri : Optional [ str ] = None @validator ( \"chain_identifier\" ) def validate_chain_identifier ( cls , chain_identifier ): # type: ignore[no-untyped-def] # noqa: E501 if not re . match ( \"^[A-Z]*-[A-Z]*$\" , chain_identifier ): raise ValueError ( \"Expected chain identifier to be formatted as NETWORKNAME-CHAINNAME, e.g. ETHEREUM-MAINNET\" # noqa: E501 ) return chain_identifier","title":"Token"},{"location":"models/token/#token","text":"Token interface with all information required to uniquely identify an NFT. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of token. token_id int unique identifier of token. uri str the uri where the metadata is stored. Source code in offchain/metadata/models/token.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Token ( BaseModel ): \"\"\"Token interface with all information required to uniquely identify an NFT. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of token. token_id (int): unique identifier of token. uri (str, optional): the uri where the metadata is stored. \"\"\" collection_address : str token_id : int chain_identifier : str = \"ETHEREUM-MAINNET\" uri : Optional [ str ] = None @validator ( \"chain_identifier\" ) def validate_chain_identifier ( cls , chain_identifier ): # type: ignore[no-untyped-def] # noqa: E501 if not re . match ( \"^[A-Z]*-[A-Z]*$\" , chain_identifier ): raise ValueError ( \"Expected chain identifier to be formatted as NETWORKNAME-CHAINNAME, e.g. ETHEREUM-MAINNET\" # noqa: E501 ) return chain_identifier","title":"Token"},{"location":"pipeline/adapters/","text":"Adapters IPFS Provides an interface for Requests sessions to contact IPFS urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/ipfs.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 @AdapterRegistry . register class IPFSAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact IPFS urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" # noqa: E501 def __init__ ( # type: ignore[no-untyped-def] self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://gateway.pinata.cloud/ipfs/\" ] assert all ( [ g . endswith ( \"/\" ) for g in self . host_prefixes ] ), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url ) async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to IPFS host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client session Returns: httpx.Response: response from IPFS host. \"\"\" return await sess . get ( self . make_request_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) gen_send ( url , sess , * args , ** kwargs ) async Format and send async request to IPFS host. Parameters: Name Type Description Default url str url to send request to required sess AsyncClient async client session required Returns: Type Description Response httpx.Response: response from IPFS host. Source code in offchain/metadata/adapters/ipfs.py 99 100 101 102 103 104 105 106 107 108 109 async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to IPFS host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client session Returns: httpx.Response: response from IPFS host. \"\"\" return await sess . get ( self . make_request_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501 make_request_url ( request_url , gateway = None ) Parse and format incoming IPFS request url Parameters: Name Type Description Default request_url str incoming IPFS request url required gateway Optional [ str ] gateway to use when making a request None Returns: Name Type Description str str formatted IPFS url Source code in offchain/metadata/adapters/ipfs.py 85 86 87 88 89 90 91 92 93 94 95 96 97 def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url ) send ( request , * args , ** kwargs ) For IPFS hashes, query pinata cloud gateway Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from IPFS Gateway Source code in offchain/metadata/adapters/ipfs.py 111 112 113 114 115 116 117 118 119 120 121 122 123 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) ARWeave Provides an interface for Requests sessions to contact ARWeave urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/arweave.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @AdapterRegistry . register class ARWeaveAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact ARWeave urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" # noqa: E501 def __init__ ( # type: ignore[no-untyped-def] self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://arweave.net/\" ] assert all ( [ g . endswith ( \"/\" ) for g in self . host_prefixes ] ), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def parse_ar_url ( self , url : str ) -> str : \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" parsed = parse_url ( url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) new_url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : new_url += parsed . path url = new_url return url async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" return await sess . get ( self . parse_ar_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" request . url = self . parse_ar_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) gen_send ( url , sess , * args , ** kwargs ) async Format and send async request to ARWeave host. Parameters: Name Type Description Default url str url to send request to required sess AsyncClient async client required Returns: Type Description Response httpx.Response: response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 62 63 64 65 66 67 68 69 70 71 72 async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" return await sess . get ( self . parse_ar_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501 parse_ar_url ( url ) Format and send async request to ARWeave host. Parameters: Name Type Description Default url str url to send request to required sess AsyncClient async client required Returns: Type Description str httpx.Response: response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def parse_ar_url ( self , url : str ) -> str : \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" parsed = parse_url ( url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) new_url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : new_url += parsed . path url = new_url return url send ( request , * args , ** kwargs ) Format and send request to ARWeave host. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 74 75 76 77 78 79 80 81 82 83 84 85 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" request . url = self . parse_ar_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) Data URI Provides an interface for Requests sessions to handle data uris. Source code in offchain/metadata/adapters/data_uri.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @AdapterRegistry . register class DataURIAdapter ( BaseAdapter ): \"\"\"Provides an interface for Requests sessions to handle data uris.\"\"\" def __init__ ( self , * args , ** kwargs ): # type: ignore[no-untyped-def] super () . __init__ ( * args , ** kwargs ) # type: ignore[no-untyped-call] async def gen_send ( self , url : str , * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle async data uri request. Args: url (str): url Returns: httpx.Response: encoded data uri response. \"\"\" response = httpx . Response ( status_code = 200 , text = decode_data_url ( url ), # type: ignore[no-untyped-call] request = httpx . Request ( method = \"GET\" , url = url ), ) return response def send ( self , request : PreparedRequest , * args , ** kwargs ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url # type: ignore[assignment] newResponse . connection = self # type: ignore[attr-defined] try : response = urlopen ( request . url ) # type: ignore[arg-type] newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse def close ( self ): # type: ignore[no-untyped-def] self . response . close () gen_send ( url , * args , ** kwargs ) async Handle async data uri request. Parameters: Name Type Description Default url str url required Returns: Type Description Response httpx.Response: encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 async def gen_send ( self , url : str , * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle async data uri request. Args: url (str): url Returns: httpx.Response: encoded data uri response. \"\"\" response = httpx . Response ( status_code = 200 , text = decode_data_url ( url ), # type: ignore[no-untyped-call] request = httpx . Request ( method = \"GET\" , url = url ), ) return response send ( request , * args , ** kwargs ) Handle data uri request. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def send ( self , request : PreparedRequest , * args , ** kwargs ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url # type: ignore[assignment] newResponse . connection = self # type: ignore[attr-defined] try : response = urlopen ( request . url ) # type: ignore[arg-type] newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse","title":"Adapters"},{"location":"pipeline/adapters/#adapters","text":"","title":"Adapters"},{"location":"pipeline/adapters/#ipfs","text":"Provides an interface for Requests sessions to contact IPFS urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/ipfs.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 @AdapterRegistry . register class IPFSAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact IPFS urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" # noqa: E501 def __init__ ( # type: ignore[no-untyped-def] self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://gateway.pinata.cloud/ipfs/\" ] assert all ( [ g . endswith ( \"/\" ) for g in self . host_prefixes ] ), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url ) async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to IPFS host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client session Returns: httpx.Response: response from IPFS host. \"\"\" return await sess . get ( self . make_request_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"IPFS"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.gen_send","text":"Format and send async request to IPFS host. Parameters: Name Type Description Default url str url to send request to required sess AsyncClient async client session required Returns: Type Description Response httpx.Response: response from IPFS host. Source code in offchain/metadata/adapters/ipfs.py 99 100 101 102 103 104 105 106 107 108 109 async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to IPFS host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client session Returns: httpx.Response: response from IPFS host. \"\"\" return await sess . get ( self . make_request_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501","title":"gen_send()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.make_request_url","text":"Parse and format incoming IPFS request url Parameters: Name Type Description Default request_url str incoming IPFS request url required gateway Optional [ str ] gateway to use when making a request None Returns: Name Type Description str str formatted IPFS url Source code in offchain/metadata/adapters/ipfs.py 85 86 87 88 89 90 91 92 93 94 95 96 97 def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url )","title":"make_request_url()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.send","text":"For IPFS hashes, query pinata cloud gateway Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from IPFS Gateway Source code in offchain/metadata/adapters/ipfs.py 111 112 113 114 115 116 117 118 119 120 121 122 123 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"send()"},{"location":"pipeline/adapters/#arweave","text":"Provides an interface for Requests sessions to contact ARWeave urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/arweave.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @AdapterRegistry . register class ARWeaveAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact ARWeave urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" # noqa: E501 def __init__ ( # type: ignore[no-untyped-def] self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://arweave.net/\" ] assert all ( [ g . endswith ( \"/\" ) for g in self . host_prefixes ] ), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def parse_ar_url ( self , url : str ) -> str : \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" parsed = parse_url ( url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) new_url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : new_url += parsed . path url = new_url return url async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" return await sess . get ( self . parse_ar_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" request . url = self . parse_ar_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"ARWeave"},{"location":"pipeline/adapters/#offchain.metadata.adapters.arweave.ARWeaveAdapter.gen_send","text":"Format and send async request to ARWeave host. Parameters: Name Type Description Default url str url to send request to required sess AsyncClient async client required Returns: Type Description Response httpx.Response: response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 62 63 64 65 66 67 68 69 70 71 72 async def gen_send ( self , url : str , sess : httpx . AsyncClient (), * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def, valid-type] # noqa: E501 \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" return await sess . get ( self . parse_ar_url ( url ), timeout = self . timeout , follow_redirects = True ) # type: ignore[no-any-return] # noqa: E501","title":"gen_send()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.arweave.ARWeaveAdapter.parse_ar_url","text":"Format and send async request to ARWeave host. Parameters: Name Type Description Default url str url to send request to required sess AsyncClient async client required Returns: Type Description str httpx.Response: response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def parse_ar_url ( self , url : str ) -> str : \"\"\"Format and send async request to ARWeave host. Args: url (str): url to send request to sess (httpx.AsyncClient()): async client Returns: httpx.Response: response from ARWeave host. \"\"\" parsed = parse_url ( url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) new_url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : new_url += parsed . path url = new_url return url","title":"parse_ar_url()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.arweave.ARWeaveAdapter.send","text":"Format and send request to ARWeave host. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 74 75 76 77 78 79 80 81 82 83 84 85 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" request . url = self . parse_ar_url ( request . url ) # type: ignore[arg-type] kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"send()"},{"location":"pipeline/adapters/#data-uri","text":"Provides an interface for Requests sessions to handle data uris. Source code in offchain/metadata/adapters/data_uri.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @AdapterRegistry . register class DataURIAdapter ( BaseAdapter ): \"\"\"Provides an interface for Requests sessions to handle data uris.\"\"\" def __init__ ( self , * args , ** kwargs ): # type: ignore[no-untyped-def] super () . __init__ ( * args , ** kwargs ) # type: ignore[no-untyped-call] async def gen_send ( self , url : str , * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle async data uri request. Args: url (str): url Returns: httpx.Response: encoded data uri response. \"\"\" response = httpx . Response ( status_code = 200 , text = decode_data_url ( url ), # type: ignore[no-untyped-call] request = httpx . Request ( method = \"GET\" , url = url ), ) return response def send ( self , request : PreparedRequest , * args , ** kwargs ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url # type: ignore[assignment] newResponse . connection = self # type: ignore[attr-defined] try : response = urlopen ( request . url ) # type: ignore[arg-type] newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse def close ( self ): # type: ignore[no-untyped-def] self . response . close ()","title":"Data URI"},{"location":"pipeline/adapters/#offchain.metadata.adapters.data_uri.DataURIAdapter.gen_send","text":"Handle async data uri request. Parameters: Name Type Description Default url str url required Returns: Type Description Response httpx.Response: encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 async def gen_send ( self , url : str , * args , ** kwargs ) -> httpx . Response : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle async data uri request. Args: url (str): url Returns: httpx.Response: encoded data uri response. \"\"\" response = httpx . Response ( status_code = 200 , text = decode_data_url ( url ), # type: ignore[no-untyped-call] request = httpx . Request ( method = \"GET\" , url = url ), ) return response","title":"gen_send()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.data_uri.DataURIAdapter.send","text":"Handle data uri request. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def send ( self , request : PreparedRequest , * args , ** kwargs ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url # type: ignore[assignment] newResponse . connection = self # type: ignore[attr-defined] try : response = urlopen ( request . url ) # type: ignore[arg-type] newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse","title":"send()"},{"location":"pipeline/fetchers/","text":"Fetchers MetadataFetcher Fetcher class that makes network requests for metadata-related information. Attributes: Name Type Description timeout int request timeout in seconds. max_retries int maximum number of request retries. sess Session a requests Session object. Source code in offchain/metadata/fetchers/metadata_fetcher.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 @FetcherRegistry . register class MetadataFetcher ( BaseFetcher ): \"\"\"Fetcher class that makes network requests for metadata-related information. Attributes: timeout (int): request timeout in seconds. max_retries (int): maximum number of request retries. sess (requests.Session): a requests Session object. \"\"\" def __init__ ( self , timeout : int = 30 , max_retries : int = 0 , async_adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . timeout = timeout self . max_retries = max_retries self . sess = requests . Session () self . async_sess = httpx . AsyncClient () self . async_adapter_configs = async_adapter_configs def register_adapter ( self , adapter : Adapter , url_prefix : str ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) def set_max_retries ( self , max_retries : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries def set_timeout ( self , timeout : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout def _head ( self , uri : str ): # type: ignore[no-untyped-def] return self . sess . head ( uri , timeout = self . timeout , allow_redirects = True ) def _get ( self , uri : str ): # type: ignore[no-untyped-def] return self . sess . get ( uri , timeout = self . timeout , allow_redirects = True ) async def _gen ( self , uri : str ) -> httpx . Response : from offchain.metadata.pipelines.metadata_pipeline import ( DEFAULT_ADAPTER_CONFIGS , ) configs = DEFAULT_ADAPTER_CONFIGS if self . async_adapter_configs : configs = self . async_adapter_configs for adapter_config in configs : if any ( uri . startswith ( prefix ) for prefix in adapter_config . mount_prefixes ): adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ) return await adapter . gen_send ( url = uri , timeout = self . timeout , sess = self . async_sess ) return await self . async_sess . get ( uri , timeout = self . timeout , follow_redirects = True ) def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise async def gen_fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : # try skip head request # res = await self._gen_head(uri) # # For any error status, try a get # if 300 <= res.status_code < 600: res = await self . _gen ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise def fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text # type: ignore[no-any-return] except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) async def gen_fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] # noqa: E501 \"\"\"Async fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = await self . _gen ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) fetch_content ( uri ) Fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text # type: ignore[no-any-return] except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) fetch_mime_type_and_size ( uri ) Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise gen_fetch_content ( uri ) async Async fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 async def gen_fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] # noqa: E501 \"\"\"Async fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = await self . _gen ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) gen_fetch_mime_type_and_size ( uri ) async Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 async def gen_fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : # try skip head request # res = await self._gen_head(uri) # # For any error status, try a get # if 300 <= res.status_code < 600: res = await self . _gen ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise register_adapter ( adapter , url_prefix ) Register an adapter to a url prefix. Parameters: Name Type Description Default adapter Adapter an Adapter instance to register. required url_prefix str the url prefix to which the adapter should be registered. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 35 36 37 38 39 40 41 42 def register_adapter ( self , adapter : Adapter , url_prefix : str ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) set_max_retries ( max_retries ) Setter function for max retries Parameters: Name Type Description Default max_retries int new maximum number of request retries. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 44 45 46 47 48 49 50 def set_max_retries ( self , max_retries : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries set_timeout ( timeout ) Setter function for timeout Parameters: Name Type Description Default timeout int new request timeout in seconds. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 52 53 54 55 56 57 58 def set_timeout ( self , timeout : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout","title":"Fetchers"},{"location":"pipeline/fetchers/#fetchers","text":"","title":"Fetchers"},{"location":"pipeline/fetchers/#metadatafetcher","text":"Fetcher class that makes network requests for metadata-related information. Attributes: Name Type Description timeout int request timeout in seconds. max_retries int maximum number of request retries. sess Session a requests Session object. Source code in offchain/metadata/fetchers/metadata_fetcher.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 @FetcherRegistry . register class MetadataFetcher ( BaseFetcher ): \"\"\"Fetcher class that makes network requests for metadata-related information. Attributes: timeout (int): request timeout in seconds. max_retries (int): maximum number of request retries. sess (requests.Session): a requests Session object. \"\"\" def __init__ ( self , timeout : int = 30 , max_retries : int = 0 , async_adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . timeout = timeout self . max_retries = max_retries self . sess = requests . Session () self . async_sess = httpx . AsyncClient () self . async_adapter_configs = async_adapter_configs def register_adapter ( self , adapter : Adapter , url_prefix : str ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) def set_max_retries ( self , max_retries : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries def set_timeout ( self , timeout : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout def _head ( self , uri : str ): # type: ignore[no-untyped-def] return self . sess . head ( uri , timeout = self . timeout , allow_redirects = True ) def _get ( self , uri : str ): # type: ignore[no-untyped-def] return self . sess . get ( uri , timeout = self . timeout , allow_redirects = True ) async def _gen ( self , uri : str ) -> httpx . Response : from offchain.metadata.pipelines.metadata_pipeline import ( DEFAULT_ADAPTER_CONFIGS , ) configs = DEFAULT_ADAPTER_CONFIGS if self . async_adapter_configs : configs = self . async_adapter_configs for adapter_config in configs : if any ( uri . startswith ( prefix ) for prefix in adapter_config . mount_prefixes ): adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ) return await adapter . gen_send ( url = uri , timeout = self . timeout , sess = self . async_sess ) return await self . async_sess . get ( uri , timeout = self . timeout , follow_redirects = True ) def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise async def gen_fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : # try skip head request # res = await self._gen_head(uri) # # For any error status, try a get # if 300 <= res.status_code < 600: res = await self . _gen ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise def fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text # type: ignore[no-any-return] except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) async def gen_fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] # noqa: E501 \"\"\"Async fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = await self . _gen ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"MetadataFetcher"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.fetch_content","text":"Fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text # type: ignore[no-any-return] except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"fetch_content()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.fetch_mime_type_and_size","text":"Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise","title":"fetch_mime_type_and_size()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.gen_fetch_content","text":"Async fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 async def gen_fetch_content ( self , uri : str ) -> Union [ dict , str ]: # type: ignore[type-arg] # noqa: E501 \"\"\"Async fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = await self . _gen ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () # type: ignore[no-any-return] else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"gen_fetch_content()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.gen_fetch_mime_type_and_size","text":"Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 async def gen_fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : # try skip head request # res = await self._gen_head(uri) # # For any error status, try a get # if 300 <= res.status_code < 600: res = await self . _gen ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise","title":"gen_fetch_mime_type_and_size()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.register_adapter","text":"Register an adapter to a url prefix. Parameters: Name Type Description Default adapter Adapter an Adapter instance to register. required url_prefix str the url prefix to which the adapter should be registered. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 35 36 37 38 39 40 41 42 def register_adapter ( self , adapter : Adapter , url_prefix : str ): # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter )","title":"register_adapter()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.set_max_retries","text":"Setter function for max retries Parameters: Name Type Description Default max_retries int new maximum number of request retries. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 44 45 46 47 48 49 50 def set_max_retries ( self , max_retries : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries","title":"set_max_retries()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.set_timeout","text":"Setter function for timeout Parameters: Name Type Description Default timeout int new request timeout in seconds. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 52 53 54 55 56 57 58 def set_timeout ( self , timeout : int ): # type: ignore[no-untyped-def] \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout","title":"set_timeout()"},{"location":"pipeline/parsers/","text":"Parsers BaseParser Base protocol for Parser classes Attributes: Name Type Description _METADATA_STANDARD MetadataStandard a class variable defining the metadata standard a parser supports. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making requests. Source code in offchain/metadata/parsers/base_parser.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class BaseParser ( Protocol ): \"\"\"Base protocol for Parser classes Attributes: _METADATA_STANDARD (MetadataStandard): a class variable defining the metadata standard a parser supports. fetcher (BaseFetcher): a fetcher instance responsible for fetching content, mime type, and size by making requests. \"\"\" # noqa: E501 _METADATA_STANDARD : MetadataStandard fetcher : BaseFetcher def __init__ ( self , fetcher : BaseFetcher ) -> None : pass def parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 pass def should_parse_token ( # type: ignore[no-untyped-def] self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs # type: ignore[type-arg] # noqa: E501 ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass @abstractmethod async def _gen_parse_metadata_impl ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ): raise NotImplementedError ( \"Not implemented\" ) async def gen_parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 try : return await self . _gen_parse_metadata_impl ( token , raw_data , * args , ** kwargs ) # type: ignore[no-any-return] # noqa: E501 except NotImplementedError : logger . warn ( f \" { self . __class__ . __name__ } doesn't implement gen_parse_metadata, fallback to legacy parse_metadata\" # noqa: E501 ) if pytest . is_running : raise return self . parse_metadata ( token , raw_data , * args , ** kwargs ) gen_parse_metadata ( token , raw_data , * args , ** kwargs ) async Given a token and raw data returned from the token uri, return a normalized Metadata object. Parameters: Name Type Description Default token Token token to process metadata for. required raw_data dict raw data returned from token uri. required Returns: Type Description Optional [ Metadata ] Optional[Metadata]: normalized metadata object, if successfully parsed. Source code in offchain/metadata/parsers/base_parser.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 async def gen_parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 try : return await self . _gen_parse_metadata_impl ( token , raw_data , * args , ** kwargs ) # type: ignore[no-any-return] # noqa: E501 except NotImplementedError : logger . warn ( f \" { self . __class__ . __name__ } doesn't implement gen_parse_metadata, fallback to legacy parse_metadata\" # noqa: E501 ) if pytest . is_running : raise return self . parse_metadata ( token , raw_data , * args , ** kwargs ) parse_metadata ( token , raw_data , * args , ** kwargs ) Given a token and raw data returned from the token uri, return a normalized Metadata object. Parameters: Name Type Description Default token Token token to process metadata for. required raw_data dict raw data returned from token uri. required Returns: Type Description Optional [ Metadata ] Optional[Metadata]: normalized metadata object, if successfully parsed. Source code in offchain/metadata/parsers/base_parser.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 pass should_parse_token ( token , raw_data , * args , ** kwargs ) Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required raw_data dict raw data returned from token uri. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/base_parser.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def should_parse_token ( # type: ignore[no-untyped-def] self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs # type: ignore[type-arg] # noqa: E501 ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass CollectionParser Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" # noqa: E501 _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( # type: ignore[no-untyped-def] self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs # noqa: E501 ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address . lower () in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] # noqa: E501 should_parse_token ( token , * args , ** kwargs ) Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 38 39 40 41 42 43 44 45 46 47 48 49 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address . lower () in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] # noqa: E501 SchemaParser Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" # noqa: E501 def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : # type: ignore[no-untyped-def] # noqa: E501 self . fetcher = fetcher or MetadataFetcher ()","title":"Parsers"},{"location":"pipeline/parsers/#parsers","text":"","title":"Parsers"},{"location":"pipeline/parsers/#baseparser","text":"Base protocol for Parser classes Attributes: Name Type Description _METADATA_STANDARD MetadataStandard a class variable defining the metadata standard a parser supports. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making requests. Source code in offchain/metadata/parsers/base_parser.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class BaseParser ( Protocol ): \"\"\"Base protocol for Parser classes Attributes: _METADATA_STANDARD (MetadataStandard): a class variable defining the metadata standard a parser supports. fetcher (BaseFetcher): a fetcher instance responsible for fetching content, mime type, and size by making requests. \"\"\" # noqa: E501 _METADATA_STANDARD : MetadataStandard fetcher : BaseFetcher def __init__ ( self , fetcher : BaseFetcher ) -> None : pass def parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 pass def should_parse_token ( # type: ignore[no-untyped-def] self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs # type: ignore[type-arg] # noqa: E501 ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass @abstractmethod async def _gen_parse_metadata_impl ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ): raise NotImplementedError ( \"Not implemented\" ) async def gen_parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 try : return await self . _gen_parse_metadata_impl ( token , raw_data , * args , ** kwargs ) # type: ignore[no-any-return] # noqa: E501 except NotImplementedError : logger . warn ( f \" { self . __class__ . __name__ } doesn't implement gen_parse_metadata, fallback to legacy parse_metadata\" # noqa: E501 ) if pytest . is_running : raise return self . parse_metadata ( token , raw_data , * args , ** kwargs )","title":"BaseParser"},{"location":"pipeline/parsers/#offchain.metadata.parsers.BaseParser.gen_parse_metadata","text":"Given a token and raw data returned from the token uri, return a normalized Metadata object. Parameters: Name Type Description Default token Token token to process metadata for. required raw_data dict raw data returned from token uri. required Returns: Type Description Optional [ Metadata ] Optional[Metadata]: normalized metadata object, if successfully parsed. Source code in offchain/metadata/parsers/base_parser.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 async def gen_parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 try : return await self . _gen_parse_metadata_impl ( token , raw_data , * args , ** kwargs ) # type: ignore[no-any-return] # noqa: E501 except NotImplementedError : logger . warn ( f \" { self . __class__ . __name__ } doesn't implement gen_parse_metadata, fallback to legacy parse_metadata\" # noqa: E501 ) if pytest . is_running : raise return self . parse_metadata ( token , raw_data , * args , ** kwargs )","title":"gen_parse_metadata()"},{"location":"pipeline/parsers/#offchain.metadata.parsers.BaseParser.parse_metadata","text":"Given a token and raw data returned from the token uri, return a normalized Metadata object. Parameters: Name Type Description Default token Token token to process metadata for. required raw_data dict raw data returned from token uri. required Returns: Type Description Optional [ Metadata ] Optional[Metadata]: normalized metadata object, if successfully parsed. Source code in offchain/metadata/parsers/base_parser.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def parse_metadata ( # type: ignore[no-untyped-def] self , token : Token , raw_data : dict , * args , ** kwargs # type: ignore[type-arg] ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" # noqa: E501 pass","title":"parse_metadata()"},{"location":"pipeline/parsers/#offchain.metadata.parsers.BaseParser.should_parse_token","text":"Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required raw_data dict raw data returned from token uri. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/base_parser.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def should_parse_token ( # type: ignore[no-untyped-def] self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs # type: ignore[type-arg] # noqa: E501 ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass","title":"should_parse_token()"},{"location":"pipeline/parsers/#collectionparser","text":"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" # noqa: E501 _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( # type: ignore[no-untyped-def] self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs # noqa: E501 ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address . lower () in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] # noqa: E501","title":"CollectionParser"},{"location":"pipeline/parsers/#offchain.metadata.parsers.collection.collection_parser.CollectionParser.should_parse_token","text":"Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 38 39 40 41 42 43 44 45 46 47 48 49 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : # type: ignore[no-untyped-def] # noqa: E501 \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address . lower () in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] # noqa: E501","title":"should_parse_token()"},{"location":"pipeline/parsers/#schemaparser","text":"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" # noqa: E501 def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : # type: ignore[no-untyped-def] # noqa: E501 self . fetcher = fetcher or MetadataFetcher ()","title":"SchemaParser"},{"location":"pipeline/pipeline/","text":"Pipeline MetadataPipeline Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: Name Type Description contract_caller ContractCaller a contract caller instance for fetching data from contracts. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers list [ BaseParser ] a list of parser instances for parsing token metadata. adapter_configs list [ BaseParser ] (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. Source code in offchain/metadata/pipelines/metadata_pipeline.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 class MetadataPipeline ( BasePipeline ): \"\"\"Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. fetcher (BaseFetcher, optional): a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers (list[BaseParser], optional): a list of parser instances for parsing token metadata. adapter_configs: (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. \"\"\" # noqa: E501 def __init__ ( self , contract_caller : Optional [ ContractCaller ] = None , fetcher : Optional [ BaseFetcher ] = None , parsers : Optional [ list [ BaseParser ]] = None , adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher ( async_adapter_configs = adapter_configs ) if adapter_configs is None : adapter_configs = DEFAULT_ADAPTER_CONFIGS for adapter_config in adapter_configs : self . mount_adapter ( adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ), url_prefixes = adapter_config . mount_prefixes , ) if parsers is None : parsers = [ parser_cls ( fetcher = self . fetcher , contract_caller = self . contract_caller ) for parser_cls in DEFAULT_PARSERS ] self . parsers = parsers def mount_adapter ( # type: ignore[no-untyped-def] self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" # noqa: E501 for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None async def gen_fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = await self . contract_caller . rpc . async_reader . gen_call_single_function_single_address_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) # type: ignore[arg-type] # noqa: E501 if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ] async def gen_fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors : list [ Union [ Metadata , MetadataProcessingError ] ] = [] if not token . uri : return MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( \"Token has not uri\" ) ) raw_data = None try : raw_data = await self . fetcher . gen_fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) async def gen_parse_metadata ( parser : BaseParser , ) -> Optional [ Union [ Metadata , MetadataProcessingError ]]: if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 return None try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) return metadata_or_error nullable_possible_metadatas_or_errors : list [ Optional [ Union [ Metadata , MetadataProcessingError ]] ] = await asyncio . gather ( * ( gen_parse_metadata ( parser ) for parser in self . parsers ) ) possible_metadatas_or_errors += filter ( None , nullable_possible_metadatas_or_errors ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ] def run ( # type: ignore[no-untyped-def, override] self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens ) ) return metadatas_or_errors async def async_run ( # type: ignore[no-untyped-def] self , tokens : list [ Token ], select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Async Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] tasks = [ self . gen_fetch_token_metadata ( token , select_metadata_fn ) for token in tokens ] metadatas_or_errors = await asyncio . gather ( * tasks ) return metadatas_or_errors async_run ( tokens , select_metadata_fn = None , * args , ** kwargs ) async Async Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 async def async_run ( # type: ignore[no-untyped-def] self , tokens : list [ Token ], select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Async Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] tasks = [ self . gen_fetch_token_metadata ( token , select_metadata_fn ) for token in tokens ] metadatas_or_errors = await asyncio . gather ( * tasks ) return metadatas_or_errors fetch_token_metadata ( token , metadata_selector_fn = None ) Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) # type: ignore[arg-type] # noqa: E501 if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ] fetch_token_uri ( token , function_signature = 'tokenURI(uint256)' ) Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None gen_fetch_token_metadata ( token , metadata_selector_fn = None ) async Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 async def gen_fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors : list [ Union [ Metadata , MetadataProcessingError ] ] = [] if not token . uri : return MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( \"Token has not uri\" ) ) raw_data = None try : raw_data = await self . fetcher . gen_fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) async def gen_parse_metadata ( parser : BaseParser , ) -> Optional [ Union [ Metadata , MetadataProcessingError ]]: if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 return None try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) return metadata_or_error nullable_possible_metadatas_or_errors : list [ Optional [ Union [ Metadata , MetadataProcessingError ]] ] = await asyncio . gather ( * ( gen_parse_metadata ( parser ) for parser in self . parsers ) ) possible_metadatas_or_errors += filter ( None , nullable_possible_metadatas_or_errors ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ] gen_fetch_token_uri ( token , function_signature = 'tokenURI(uint256)' ) async Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 async def gen_fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = await self . contract_caller . rpc . async_reader . gen_call_single_function_single_address_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None mount_adapter ( adapter , url_prefixes ) Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Parameters: Name Type Description Default adapter Adapter Adapter instance required url_prefixes list [ str ] list of url prefixes to which to mount the adapter. required Source code in offchain/metadata/pipelines/metadata_pipeline.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def mount_adapter ( # type: ignore[no-untyped-def] self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" # noqa: E501 for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) run ( tokens , parallelize = True , select_metadata_fn = None , * args , ** kwargs ) Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required parallelize bool whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. True select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def run ( # type: ignore[no-untyped-def, override] self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens ) ) return metadatas_or_errors","title":"Pipeline"},{"location":"pipeline/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"pipeline/pipeline/#metadatapipeline","text":"Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: Name Type Description contract_caller ContractCaller a contract caller instance for fetching data from contracts. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers list [ BaseParser ] a list of parser instances for parsing token metadata. adapter_configs list [ BaseParser ] (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. Source code in offchain/metadata/pipelines/metadata_pipeline.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 class MetadataPipeline ( BasePipeline ): \"\"\"Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. fetcher (BaseFetcher, optional): a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers (list[BaseParser], optional): a list of parser instances for parsing token metadata. adapter_configs: (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. \"\"\" # noqa: E501 def __init__ ( self , contract_caller : Optional [ ContractCaller ] = None , fetcher : Optional [ BaseFetcher ] = None , parsers : Optional [ list [ BaseParser ]] = None , adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher ( async_adapter_configs = adapter_configs ) if adapter_configs is None : adapter_configs = DEFAULT_ADAPTER_CONFIGS for adapter_config in adapter_configs : self . mount_adapter ( adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ), url_prefixes = adapter_config . mount_prefixes , ) if parsers is None : parsers = [ parser_cls ( fetcher = self . fetcher , contract_caller = self . contract_caller ) for parser_cls in DEFAULT_PARSERS ] self . parsers = parsers def mount_adapter ( # type: ignore[no-untyped-def] self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" # noqa: E501 for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None async def gen_fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = await self . contract_caller . rpc . async_reader . gen_call_single_function_single_address_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) # type: ignore[arg-type] # noqa: E501 if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ] async def gen_fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors : list [ Union [ Metadata , MetadataProcessingError ] ] = [] if not token . uri : return MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( \"Token has not uri\" ) ) raw_data = None try : raw_data = await self . fetcher . gen_fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) async def gen_parse_metadata ( parser : BaseParser , ) -> Optional [ Union [ Metadata , MetadataProcessingError ]]: if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 return None try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) return metadata_or_error nullable_possible_metadatas_or_errors : list [ Optional [ Union [ Metadata , MetadataProcessingError ]] ] = await asyncio . gather ( * ( gen_parse_metadata ( parser ) for parser in self . parsers ) ) possible_metadatas_or_errors += filter ( None , nullable_possible_metadatas_or_errors ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ] def run ( # type: ignore[no-untyped-def, override] self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens ) ) return metadatas_or_errors async def async_run ( # type: ignore[no-untyped-def] self , tokens : list [ Token ], select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Async Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] tasks = [ self . gen_fetch_token_metadata ( token , select_metadata_fn ) for token in tokens ] metadatas_or_errors = await asyncio . gather ( * tasks ) return metadatas_or_errors","title":"MetadataPipeline"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.async_run","text":"Async Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 async def async_run ( # type: ignore[no-untyped-def] self , tokens : list [ Token ], select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Async Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] tasks = [ self . gen_fetch_token_metadata ( token , select_metadata_fn ) for token in tokens ] metadatas_or_errors = await asyncio . gather ( * tasks ) return metadatas_or_errors","title":"async_run()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.fetch_token_metadata","text":"Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) # type: ignore[arg-type] # noqa: E501 if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ]","title":"fetch_token_metadata()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.fetch_token_uri","text":"Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None","title":"fetch_token_uri()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.gen_fetch_token_metadata","text":"Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 async def gen_fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , # type: ignore[type-arg] ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors : list [ Union [ Metadata , MetadataProcessingError ] ] = [] if not token . uri : return MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( \"Token has not uri\" ) ) raw_data = None try : raw_data = await self . fetcher . gen_fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message ) ) ) async def gen_parse_metadata ( parser : BaseParser , ) -> Optional [ Union [ Metadata , MetadataProcessingError ]]: if not parser . should_parse_token ( token = token , raw_data = raw_data ): # type: ignore[arg-type] # noqa: E501 return None try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data # type: ignore[arg-type] ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( # type: ignore[assignment] # noqa: E501 token = token , e = e ) return metadata_or_error nullable_possible_metadatas_or_errors : list [ Optional [ Union [ Metadata , MetadataProcessingError ]] ] = await asyncio . gather ( * ( gen_parse_metadata ( parser ) for parser in self . parsers ) ) possible_metadatas_or_errors += filter ( None , nullable_possible_metadatas_or_errors ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" # noqa: E501 ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) # type: ignore[no-any-return] # noqa: E501 return possible_metadatas_or_errors [ 0 ]","title":"gen_fetch_token_metadata()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.gen_fetch_token_uri","text":"Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 async def gen_fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" # noqa: E501 res = await self . contract_caller . rpc . async_reader . gen_call_single_function_single_address_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None","title":"gen_fetch_token_uri()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.mount_adapter","text":"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Parameters: Name Type Description Default adapter Adapter Adapter instance required url_prefixes list [ str ] list of url prefixes to which to mount the adapter. required Source code in offchain/metadata/pipelines/metadata_pipeline.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def mount_adapter ( # type: ignore[no-untyped-def] self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" # noqa: E501 for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix )","title":"mount_adapter()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.run","text":"Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required parallelize bool whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. True select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def run ( # type: ignore[no-untyped-def, override] self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , # type: ignore[type-arg] * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" # noqa: E501 if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens ) ) return metadatas_or_errors","title":"run()"},{"location":"usage/customize/","text":"Customizing the Pipeline There are countless ways to customize the pipeline. The default MetadataPipeline can be constructed with any permutation of Fetchers , Adapters , Parsers , and ContractCallers . And you can even define your own custom Pipeline by extending the BasePipeline class. In this guide, we'll cover a few ways you can customize the MetadataPipeline to best suit your needs. Custom RPC Provider By default, the pipeline uses https://cloudflare-eth.com as the Ethereum JSON RPC url. This is a free Ethereum RPC provider, which means that it is very easy to exceed the rate-limit. If you have a custom RPC provider url you'd like to use, you can specify it like this: from offchain import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller ) Custom Parsers By default, the pipeline runs with all collection, schema, and catch-all parsers. That said, you may find that you're only interested in using a subset of the parsers. Let's say you're only interested in parsing metadata for a specific collection. If this is the case, you can pass in a list of specific parser instances to run. For example, the following configuration runs the pipeline using only the ENS collection parser. from offchain import MetadataPipeline from offchain.metadata import ENSParser ens_parser = ENSParser () pipeline = MetadataPipeline ( parsers = [ ens_parser ]) View the full list of available parsers here . Custom Adapters By default, the pipeline is run with all available adapters. Each adapter has a default host prefix and is configured with the following args: {\"pool_connections\": 100, \"pool_maxsize\": 1000, \"max_retries\": 0} You can also customize the pipeline to only use a subset of the adapters. For instance, if you wanted to build a metadata indexer that only indexes onchain metadata, you may opt to only use the IPFS, ARWeave, and DataURI adapters. There are two ways to configure custom adapters for the pipeline: Specifying Adapter Configs from offchain.metadata.adapters import ARWeaveAdapter , DataURIAdapter , HTTPAdapter , IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import AdapterConfig , MetadataPipeline adapter_configs = [ AdapterConfig ( adapter_cls = ARWeaveAdapter , mount_prefixes = [ \"ar://\" ], host_prefixes = [ \"https://arweave.net/\" ], kwargs = { \"pool_connections\" : 100 , \"pool_maxsize\" : 1000 , \"max_retries\" : 0 }, ), AdapterConfig ( adapter_cls = DataURIAdapter , mount_prefixes = [ \"data:\" ]), ] pipeline = MetadataPipeline ( adapter_configs = adapter_configs ) View the full list of available adapters here . Mounting Custom Adapters from offchain.metadata.adapters import IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline pipeline = MetadataPipeline () pipeline . mount_adapter ( adapter = IPFSAdapter ( host_prefixes = [ MY_CUSTOM_IPFS_HOST ], pool_connections = 100 , pool_maxsize = 1000 , max_retries = 0 , ), url_prefixes = [ \"ipfs://\" , \"https://gateway.pinata.cloud/\" , \"https://ipfs.io/\" , ], )","title":"Customizing the Pipeline"},{"location":"usage/customize/#customizing-the-pipeline","text":"There are countless ways to customize the pipeline. The default MetadataPipeline can be constructed with any permutation of Fetchers , Adapters , Parsers , and ContractCallers . And you can even define your own custom Pipeline by extending the BasePipeline class. In this guide, we'll cover a few ways you can customize the MetadataPipeline to best suit your needs.","title":"Customizing the Pipeline"},{"location":"usage/customize/#custom-rpc-provider","text":"By default, the pipeline uses https://cloudflare-eth.com as the Ethereum JSON RPC url. This is a free Ethereum RPC provider, which means that it is very easy to exceed the rate-limit. If you have a custom RPC provider url you'd like to use, you can specify it like this: from offchain import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller )","title":"Custom RPC Provider"},{"location":"usage/customize/#custom-parsers","text":"By default, the pipeline runs with all collection, schema, and catch-all parsers. That said, you may find that you're only interested in using a subset of the parsers. Let's say you're only interested in parsing metadata for a specific collection. If this is the case, you can pass in a list of specific parser instances to run. For example, the following configuration runs the pipeline using only the ENS collection parser. from offchain import MetadataPipeline from offchain.metadata import ENSParser ens_parser = ENSParser () pipeline = MetadataPipeline ( parsers = [ ens_parser ]) View the full list of available parsers here .","title":"Custom Parsers"},{"location":"usage/customize/#custom-adapters","text":"By default, the pipeline is run with all available adapters. Each adapter has a default host prefix and is configured with the following args: {\"pool_connections\": 100, \"pool_maxsize\": 1000, \"max_retries\": 0} You can also customize the pipeline to only use a subset of the adapters. For instance, if you wanted to build a metadata indexer that only indexes onchain metadata, you may opt to only use the IPFS, ARWeave, and DataURI adapters. There are two ways to configure custom adapters for the pipeline:","title":"Custom Adapters"},{"location":"usage/customize/#specifying-adapter-configs","text":"from offchain.metadata.adapters import ARWeaveAdapter , DataURIAdapter , HTTPAdapter , IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import AdapterConfig , MetadataPipeline adapter_configs = [ AdapterConfig ( adapter_cls = ARWeaveAdapter , mount_prefixes = [ \"ar://\" ], host_prefixes = [ \"https://arweave.net/\" ], kwargs = { \"pool_connections\" : 100 , \"pool_maxsize\" : 1000 , \"max_retries\" : 0 }, ), AdapterConfig ( adapter_cls = DataURIAdapter , mount_prefixes = [ \"data:\" ]), ] pipeline = MetadataPipeline ( adapter_configs = adapter_configs ) View the full list of available adapters here .","title":"Specifying Adapter Configs"},{"location":"usage/customize/#mounting-custom-adapters","text":"from offchain.metadata.adapters import IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline pipeline = MetadataPipeline () pipeline . mount_adapter ( adapter = IPFSAdapter ( host_prefixes = [ MY_CUSTOM_IPFS_HOST ], pool_connections = 100 , pool_maxsize = 1000 , max_retries = 0 , ), url_prefixes = [ \"ipfs://\" , \"https://gateway.pinata.cloud/\" , \"https://ipfs.io/\" , ], )","title":"Mounting Custom Adapters"},{"location":"usage/overview/","text":"Usage Please review the Core Concepts page before reading the rest of the documentation. Basic Usage Fetching metadata for a single token from offchain import get_token_metadata metadata = get_token_metadata ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) # Data for the token at index 0 metadata . name # -> 'antares the improbable' metadata . description # -> 'You are a WITCH who bathes in the tears of...' metadata . standard # -> OPENSEA_STANDARD metadata . attributes # -> [Attribute(trait_type='Skin Tone', ...] metadata . image # -> MediaDetails(size=2139693, sha256=None, uri='https://cryptocoven.s3.amazonaws.com/2048b255aa1d02045eef13cdd7100479.png', mime_type='image/png') metadata . additional_fields # -> [MetadataField(...), ...] Fetching metadata for multiple tokens from offchain import MetadataPipeline , Token pipeline = MetadataPipeline () token_1 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) token_2 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9560 ) metadatas = pipeline . run ([ token_1 , token_2 ]) Input The Token interface is how the metadata pipeline uniquely identifies an NFT. A Token is composed of four properties: collection_address : The token's contract address. token_id : The unique identifier for a token within a collection. chain_identifier : The network and chain for the token. Defaults to \"ETHEREUM-MAINNET\" if nothing is passed in. uri : The url where the metadata information lives. Defaults to fetching from the contract directly if nothing is passed in. Example of token 9559 from CryptoCoven on Ethereum Mainnet: from offchain import Token Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , #Required token_id = 9559 , #Required chain_identifier = \"ETHEREUM-MAINNET\" , # Optional, defaults to Ethereum Mainnet uri : \"ipfs://QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm/9559.json\" # Optional, defaults to requesting the URI from the contract directly ) Output The MetadataPipeline is run on a list of Token objects and outputs a list of equal length. Each item in the output list maps to the Token at the same index in the input list. If the pipeline successfully fetches metadata for a token, the token should map to a Metadata object. The Metadata interface is a standardized representation of NFT metadata. Conversely, if the pipeline fails to fetch metadata for a token, it should map to a MetadataProcessingError object. The MetadataProcessingError interface defines contextual information for how and why processing metadata for a specific token failed.","title":"Overview"},{"location":"usage/overview/#usage","text":"Please review the Core Concepts page before reading the rest of the documentation.","title":"Usage"},{"location":"usage/overview/#basic-usage","text":"","title":"Basic Usage"},{"location":"usage/overview/#fetching-metadata-for-a-single-token","text":"from offchain import get_token_metadata metadata = get_token_metadata ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) # Data for the token at index 0 metadata . name # -> 'antares the improbable' metadata . description # -> 'You are a WITCH who bathes in the tears of...' metadata . standard # -> OPENSEA_STANDARD metadata . attributes # -> [Attribute(trait_type='Skin Tone', ...] metadata . image # -> MediaDetails(size=2139693, sha256=None, uri='https://cryptocoven.s3.amazonaws.com/2048b255aa1d02045eef13cdd7100479.png', mime_type='image/png') metadata . additional_fields # -> [MetadataField(...), ...]","title":"Fetching metadata for a single token"},{"location":"usage/overview/#fetching-metadata-for-multiple-tokens","text":"from offchain import MetadataPipeline , Token pipeline = MetadataPipeline () token_1 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) token_2 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9560 ) metadatas = pipeline . run ([ token_1 , token_2 ])","title":"Fetching metadata for multiple tokens"},{"location":"usage/overview/#input","text":"The Token interface is how the metadata pipeline uniquely identifies an NFT. A Token is composed of four properties: collection_address : The token's contract address. token_id : The unique identifier for a token within a collection. chain_identifier : The network and chain for the token. Defaults to \"ETHEREUM-MAINNET\" if nothing is passed in. uri : The url where the metadata information lives. Defaults to fetching from the contract directly if nothing is passed in. Example of token 9559 from CryptoCoven on Ethereum Mainnet: from offchain import Token Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , #Required token_id = 9559 , #Required chain_identifier = \"ETHEREUM-MAINNET\" , # Optional, defaults to Ethereum Mainnet uri : \"ipfs://QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm/9559.json\" # Optional, defaults to requesting the URI from the contract directly )","title":"Input"},{"location":"usage/overview/#output","text":"The MetadataPipeline is run on a list of Token objects and outputs a list of equal length. Each item in the output list maps to the Token at the same index in the input list. If the pipeline successfully fetches metadata for a token, the token should map to a Metadata object. The Metadata interface is a standardized representation of NFT metadata. Conversely, if the pipeline fails to fetch metadata for a token, it should map to a MetadataProcessingError object. The MetadataProcessingError interface defines contextual information for how and why processing metadata for a specific token failed.","title":"Output"}]}