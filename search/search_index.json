{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started Documentation for version: v0.1.0 Overview offchain is an open-source Python package for processing both onchain and offchain NFT metadata. The purpose of this project is to enable anyone to define and standardize their own custom NFT metadata schema. offchain powers the Zora Indexer, and any contributions to metadata parsing implementations will be reflected in the Zora API. If you'd like to contribute your own metadata parsing implementation, please see the Contributing page. Installation with pip: pip install offchain with poetry: poetry add offchain with conda: conda install offchain -c conda-forge from repository: pip install git+https://github.com/ourzora/offchain.git","title":"Getting started"},{"location":"#getting-started","text":"Documentation for version: v0.1.0","title":"Getting started"},{"location":"#overview","text":"offchain is an open-source Python package for processing both onchain and offchain NFT metadata. The purpose of this project is to enable anyone to define and standardize their own custom NFT metadata schema. offchain powers the Zora Indexer, and any contributions to metadata parsing implementations will be reflected in the Zora API. If you'd like to contribute your own metadata parsing implementation, please see the Contributing page.","title":"Overview"},{"location":"#installation","text":"with pip: pip install offchain with poetry: poetry add offchain with conda: conda install offchain -c conda-forge from repository: pip install git+https://github.com/ourzora/offchain.git","title":"Installation"},{"location":"changelog/","text":"Changelog v0.1.0 define all interfaces and pipeline components add documentation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v010","text":"define all interfaces and pipeline components add documentation","title":"v0.1.0"},{"location":"contributing/","text":"Guidelines for contributing Contributing a collection parser If you have a specific NFT collection for which you'd like to write a custom parsing implementation, you can contribute a collection parser. Collection parsers must specify the collection address(es) for which they parse metadata. You can get started by extending the CollectionParser base class and following the patterns laid out by other collection parsers, such as ENSParser or NounsParser . CollectionParser Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] should_parse_token ( token , * args , ** kwargs ) Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 34 35 36 37 38 39 40 41 42 43 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] Contributing a schema parser A schema parser should only be defined if the metadata schema spans across multiple collections the parsed schema is sufficiently different from that returned by a CatchallParser and there is a way to uniquely identify tokens that have this metadata schema, without accidentally catching other tokens. You can contribute a schema parser by extending the SchemaParser base class and following the patterns laid out by other schema parsers, such as OpenseaParser . SchemaParser Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : self . fetcher = fetcher or MetadataFetcher () Contributing an adapter We currently support HTTP, Data URI, IPFS, and ARWeave url formats. If you have another url format you'd like to support, you can extend the BaseAdapter class or HTTPAdapter class and write a custom interface for Requests sessions to handle your url format. BaseAdapter Base Adapter inheriting from requests BaseAdapter Source code in offchain/metadata/adapters/base_adapter.py 10 11 12 13 14 class BaseAdapter ( RequestsBaseAdapter ): \"\"\"Base Adapter inheriting from requests BaseAdapter\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ () HTTPAdapter HTTP Adapter inheriting from requests HTTPAdapter Source code in offchain/metadata/adapters/base_adapter.py 17 18 19 20 21 22 23 24 25 26 27 28 29 class HTTPAdapter ( RequestsHTTPAdapter ): \"\"\"HTTP Adapter inheriting from requests HTTPAdapter\"\"\" def __init__ ( self , pool_connections : int = ... , pool_maxsize : int = ... , max_retries : Union [ Retry , int , None ] = ... , pool_block : bool = ... , * args , ** kwargs ) -> None : super () . __init__ ( pool_connections , pool_maxsize , max_retries , pool_block )","title":"Contributing"},{"location":"contributing/#guidelines-for-contributing","text":"","title":"Guidelines for contributing"},{"location":"contributing/#contributing-a-collection-parser","text":"If you have a specific NFT collection for which you'd like to write a custom parsing implementation, you can contribute a collection parser. Collection parsers must specify the collection address(es) for which they parse metadata. You can get started by extending the CollectionParser base class and following the patterns laid out by other collection parsers, such as ENSParser or NounsParser .","title":"Contributing a collection parser"},{"location":"contributing/#offchain.metadata.parsers.metadata.collection_parser.CollectionParser","text":"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ]","title":"CollectionParser"},{"location":"contributing/#offchain.metadata.parsers.metadata.collection_parser.CollectionParser.should_parse_token","text":"Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 34 35 36 37 38 39 40 41 42 43 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ]","title":"should_parse_token()"},{"location":"contributing/#contributing-a-schema-parser","text":"A schema parser should only be defined if the metadata schema spans across multiple collections the parsed schema is sufficiently different from that returned by a CatchallParser and there is a way to uniquely identify tokens that have this metadata schema, without accidentally catching other tokens. You can contribute a schema parser by extending the SchemaParser base class and following the patterns laid out by other schema parsers, such as OpenseaParser .","title":"Contributing a schema parser"},{"location":"contributing/#offchain.metadata.parsers.metadata.schema_parser.SchemaParser","text":"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : self . fetcher = fetcher or MetadataFetcher ()","title":"SchemaParser"},{"location":"contributing/#contributing-an-adapter","text":"We currently support HTTP, Data URI, IPFS, and ARWeave url formats. If you have another url format you'd like to support, you can extend the BaseAdapter class or HTTPAdapter class and write a custom interface for Requests sessions to handle your url format.","title":"Contributing an adapter"},{"location":"contributing/#baseadapter","text":"Base Adapter inheriting from requests BaseAdapter Source code in offchain/metadata/adapters/base_adapter.py 10 11 12 13 14 class BaseAdapter ( RequestsBaseAdapter ): \"\"\"Base Adapter inheriting from requests BaseAdapter\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ()","title":"BaseAdapter"},{"location":"contributing/#httpadapter","text":"HTTP Adapter inheriting from requests HTTPAdapter Source code in offchain/metadata/adapters/base_adapter.py 17 18 19 20 21 22 23 24 25 26 27 28 29 class HTTPAdapter ( RequestsHTTPAdapter ): \"\"\"HTTP Adapter inheriting from requests HTTPAdapter\"\"\" def __init__ ( self , pool_connections : int = ... , pool_maxsize : int = ... , max_retries : Union [ Retry , int , None ] = ... , pool_block : bool = ... , * args , ** kwargs ) -> None : super () . __init__ ( pool_connections , pool_maxsize , max_retries , pool_block )","title":"HTTPAdapter"},{"location":"usage/","text":"Usage Basic usage from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 , uri = \"ipfs://QmSr3vdMuP2fSxWD7S26KzzBWcAN1eNhm4hk1qaR3x3vmj/9559.json\" ) metadatas = pipeline . run ([ token ]) How it works The MetadataPipeline is run on a list of Token objects and returns a list of Metadata or MetadataProcessingError objects, each of which maps to the Token at the same index in the input list. (see Interfaces for more information). The MetadataPipeline is initialized with a ContractCaller , a Fetcher , a list of Adapters , and a list of Parsers . If no uri is passed in for a token, the pipeline will use the ContractCaller to attempt to fetch it from a tokenURI(uint256) view function on the contract. The pipeline use the Fetcher and Adapters to attempt to fetch metadata in form of raw JSON from the uri. The pipeline runs each parser in the order they were passed in. By default, the ordering is CollectionParsers , SchemaParsers , and then CatchallParsers . The pipeline will return the result of the first parser that is able to successfully parse the token, unless a metadata_selector_fn is specified. If no parser is able to successfully parse a token, the pipeline will return a MetadataProcessingError for that token. Using a custom RPC provider url By default, the pipeline uses https://cloudflare-eth.com as the Ethereum JSON RPC url. You can specify your own rpc provider url like this: from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller ) Using custom parsers By default, the pipeline runs with all collection, schema, and catch-all parsers. You can pass in a list of specific parser instances to run. For instance, the following configuration runs the pipeline using only the ENS collection parser. from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.parsers.collection.ens import ENSParser ens_parser = ENSParser () pipeline = MetadataPipeline ( parsers = [ ens_parser ]) Using custom adapters By default, the pipeline is run with all available adapters. Each adapter has a default host prefix and is configured with the following args: {\"pool_connections\": 100, \"pool_maxsize\": 1000, \"max_retries\": 0} There are two ways to configure custom adapters for the pipeline: Specifying adapter configs from offchain.metadata.adapters import ARWeaveAdapter , DataURIAdapter , HTTPAdapter , IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import AdapterConfig , MetadataPipeline adapter_configs = [ AdapterConfig ( adapter_cls = ARWeaveAdapter , mount_prefixes = [ \"ar://\" ], host_prefixes = [ \"https://arweave.net/\" ], kwargs = { \"pool_connections\" : 100 , \"pool_maxsize\" : 1000 , \"max_retries\" : 0 }, ), AdapterConfig ( adapter_cls = DataURIAdapter , mount_prefixes = [ \"data:\" ]), ] pipeline = MetadataPipeline ( adapter_configs = adapter_configs ) Mounting custom adapters from offchain.metadata.adapters import IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline pipeline = MetadataPipeline () pipeline . mount_adapter ( adapter = IPFSAdapter ( host_prefixes = [ MY_CUSTOM_IPFS_HOST ], pool_connections = 100 , pool_maxsize = 1000 , max_retries = 0 , ), url_prefixes = [ \"ipfs://\" , \"https://gateway.pinata.cloud/\" , \"https://ipfs.io/\" , ], )","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#basic-usage","text":"from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 , uri = \"ipfs://QmSr3vdMuP2fSxWD7S26KzzBWcAN1eNhm4hk1qaR3x3vmj/9559.json\" ) metadatas = pipeline . run ([ token ])","title":"Basic usage"},{"location":"usage/#how-it-works","text":"The MetadataPipeline is run on a list of Token objects and returns a list of Metadata or MetadataProcessingError objects, each of which maps to the Token at the same index in the input list. (see Interfaces for more information). The MetadataPipeline is initialized with a ContractCaller , a Fetcher , a list of Adapters , and a list of Parsers . If no uri is passed in for a token, the pipeline will use the ContractCaller to attempt to fetch it from a tokenURI(uint256) view function on the contract. The pipeline use the Fetcher and Adapters to attempt to fetch metadata in form of raw JSON from the uri. The pipeline runs each parser in the order they were passed in. By default, the ordering is CollectionParsers , SchemaParsers , and then CatchallParsers . The pipeline will return the result of the first parser that is able to successfully parse the token, unless a metadata_selector_fn is specified. If no parser is able to successfully parse a token, the pipeline will return a MetadataProcessingError for that token.","title":"How it works"},{"location":"usage/#using-a-custom-rpc-provider-url","text":"By default, the pipeline uses https://cloudflare-eth.com as the Ethereum JSON RPC url. You can specify your own rpc provider url like this: from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller )","title":"Using a custom RPC provider url"},{"location":"usage/#using-custom-parsers","text":"By default, the pipeline runs with all collection, schema, and catch-all parsers. You can pass in a list of specific parser instances to run. For instance, the following configuration runs the pipeline using only the ENS collection parser. from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.parsers.collection.ens import ENSParser ens_parser = ENSParser () pipeline = MetadataPipeline ( parsers = [ ens_parser ])","title":"Using custom parsers"},{"location":"usage/#using-custom-adapters","text":"By default, the pipeline is run with all available adapters. Each adapter has a default host prefix and is configured with the following args: {\"pool_connections\": 100, \"pool_maxsize\": 1000, \"max_retries\": 0} There are two ways to configure custom adapters for the pipeline:","title":"Using custom adapters"},{"location":"usage/#specifying-adapter-configs","text":"from offchain.metadata.adapters import ARWeaveAdapter , DataURIAdapter , HTTPAdapter , IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import AdapterConfig , MetadataPipeline adapter_configs = [ AdapterConfig ( adapter_cls = ARWeaveAdapter , mount_prefixes = [ \"ar://\" ], host_prefixes = [ \"https://arweave.net/\" ], kwargs = { \"pool_connections\" : 100 , \"pool_maxsize\" : 1000 , \"max_retries\" : 0 }, ), AdapterConfig ( adapter_cls = DataURIAdapter , mount_prefixes = [ \"data:\" ]), ] pipeline = MetadataPipeline ( adapter_configs = adapter_configs )","title":"Specifying adapter configs"},{"location":"usage/#mounting-custom-adapters","text":"from offchain.metadata.adapters import IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline pipeline = MetadataPipeline () pipeline . mount_adapter ( adapter = IPFSAdapter ( host_prefixes = [ MY_CUSTOM_IPFS_HOST ], pool_connections = 100 , pool_maxsize = 1000 , max_retries = 0 , ), url_prefixes = [ \"ipfs://\" , \"https://gateway.pinata.cloud/\" , \"https://ipfs.io/\" , ], )","title":"Mounting custom adapters"},{"location":"models/metadata/","text":"Metadata Metadata A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: Name Type Description token Token (Token): a Token interface with all information required to uniquely identify an NFT raw_data dict raw metadata object fetched from token uri. standard MetadataStandard accepted metadata standard based on the format of the metadata. attributes list [ Attribute ] list of token metadata attributes. name str token name. description str token description. mime_type str metadata mime type, e.g. \"image/png\". image str nested image in the metadata. content str nested content, e.g. video, audio, etc., in the metadata. additional_fields list [ MetadataField ] any additional metadata fields that don't fit in the defined schema. Source code in offchain/metadata/models/metadata.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Metadata ( BaseModel ): \"\"\"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: token: (Token): a Token interface with all information required to uniquely identify an NFT raw_data (dict): raw metadata object fetched from token uri. standard (MetadataStandard): accepted metadata standard based on the format of the metadata. attributes (list[Attribute]): list of token metadata attributes. name (str, optional): token name. description (str, optional): token description. mime_type (str, optional): metadata mime type, e.g. \"image/png\". image (str, optional): nested image in the metadata. content (str, optional): nested content, e.g. video, audio, etc., in the metadata. additional_fields (list[MetadataField], optional): any additional metadata fields that don't fit in the defined schema. \"\"\" token : Token raw_data : dict attributes : list [ Attribute ] standard : Optional [ MetadataStandard ] = None name : Optional [ str ] = None description : Optional [ str ] = None mime_type : Optional [ str ] = None image : Optional [ MediaDetails ] = None content : Optional [ MediaDetails ] = None additional_fields : Optional [ list [ MetadataField ]] = None Attribute NFT metadata atttribute Attributes: Name Type Description trait_type str the attribute key. value str the attribute value. display_type str informs how the attribute is displayed. Source code in offchain/metadata/models/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 class Attribute ( BaseModel ): \"\"\"NFT metadata atttribute Attributes: trait_type (str, optional): the attribute key. value (str, optional): the attribute value. display_type (str, optional): informs how the attribute is displayed. \"\"\" trait_type : Optional [ str ] = None value : Optional [ str ] = None display_type : Optional [ str ] = None MediaDetails Metadata media information Attributes: Name Type Description size int size of the media. sha256 str the SHA256 hash of the media. uri str the uri at which the media was found. mime_type str the mime type of the media. Source code in offchain/metadata/models/metadata.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class MediaDetails ( BaseModel ): \"\"\"Metadata media information Attributes: size (int, optional): size of the media. sha256 (str, optional): the SHA256 hash of the media. uri (str, optional): the uri at which the media was found. mime_type (str, optional): the mime type of the media. \"\"\" size : Optional [ int ] = None sha256 : Optional [ str ] = None uri : Optional [ str ] = None mime_type : Optional [ str ] = None MetadataField Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: Name Type Description field_name str name of the metadata field. type MetadataFieldType (MetadataFieldType): metadata field type. description str (str, optional): a description of what this metadata field represents. value Any (any): the value of the metadata field. Source code in offchain/metadata/models/metadata.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MetadataField ( BaseModel ): \"\"\"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: field_name (str): name of the metadata field. type: (MetadataFieldType): metadata field type. description: (str, optional): a description of what this metadata field represents. value: (any): the value of the metadata field. \"\"\" field_name : str type : MetadataFieldType description : str value : Any","title":"Metadata"},{"location":"models/metadata/#metadata","text":"","title":"Metadata"},{"location":"models/metadata/#metadata_1","text":"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: Name Type Description token Token (Token): a Token interface with all information required to uniquely identify an NFT raw_data dict raw metadata object fetched from token uri. standard MetadataStandard accepted metadata standard based on the format of the metadata. attributes list [ Attribute ] list of token metadata attributes. name str token name. description str token description. mime_type str metadata mime type, e.g. \"image/png\". image str nested image in the metadata. content str nested content, e.g. video, audio, etc., in the metadata. additional_fields list [ MetadataField ] any additional metadata fields that don't fit in the defined schema. Source code in offchain/metadata/models/metadata.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Metadata ( BaseModel ): \"\"\"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: token: (Token): a Token interface with all information required to uniquely identify an NFT raw_data (dict): raw metadata object fetched from token uri. standard (MetadataStandard): accepted metadata standard based on the format of the metadata. attributes (list[Attribute]): list of token metadata attributes. name (str, optional): token name. description (str, optional): token description. mime_type (str, optional): metadata mime type, e.g. \"image/png\". image (str, optional): nested image in the metadata. content (str, optional): nested content, e.g. video, audio, etc., in the metadata. additional_fields (list[MetadataField], optional): any additional metadata fields that don't fit in the defined schema. \"\"\" token : Token raw_data : dict attributes : list [ Attribute ] standard : Optional [ MetadataStandard ] = None name : Optional [ str ] = None description : Optional [ str ] = None mime_type : Optional [ str ] = None image : Optional [ MediaDetails ] = None content : Optional [ MediaDetails ] = None additional_fields : Optional [ list [ MetadataField ]] = None","title":"Metadata"},{"location":"models/metadata/#attribute","text":"NFT metadata atttribute Attributes: Name Type Description trait_type str the attribute key. value str the attribute value. display_type str informs how the attribute is displayed. Source code in offchain/metadata/models/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 class Attribute ( BaseModel ): \"\"\"NFT metadata atttribute Attributes: trait_type (str, optional): the attribute key. value (str, optional): the attribute value. display_type (str, optional): informs how the attribute is displayed. \"\"\" trait_type : Optional [ str ] = None value : Optional [ str ] = None display_type : Optional [ str ] = None","title":"Attribute"},{"location":"models/metadata/#mediadetails","text":"Metadata media information Attributes: Name Type Description size int size of the media. sha256 str the SHA256 hash of the media. uri str the uri at which the media was found. mime_type str the mime type of the media. Source code in offchain/metadata/models/metadata.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class MediaDetails ( BaseModel ): \"\"\"Metadata media information Attributes: size (int, optional): size of the media. sha256 (str, optional): the SHA256 hash of the media. uri (str, optional): the uri at which the media was found. mime_type (str, optional): the mime type of the media. \"\"\" size : Optional [ int ] = None sha256 : Optional [ str ] = None uri : Optional [ str ] = None mime_type : Optional [ str ] = None","title":"MediaDetails"},{"location":"models/metadata/#metadatafield","text":"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: Name Type Description field_name str name of the metadata field. type MetadataFieldType (MetadataFieldType): metadata field type. description str (str, optional): a description of what this metadata field represents. value Any (any): the value of the metadata field. Source code in offchain/metadata/models/metadata.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MetadataField ( BaseModel ): \"\"\"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: field_name (str): name of the metadata field. type: (MetadataFieldType): metadata field type. description: (str, optional): a description of what this metadata field represents. value: (any): the value of the metadata field. \"\"\" field_name : str type : MetadataFieldType description : str value : Any","title":"MetadataField"},{"location":"models/metadata_processing_error/","text":"MetadataProcessingError Interface for metadata processing errors and relevant contextual information. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of the token. token_id int token id of the token. uri str metadata uri of the token. error_type str the class of caught exception. error_message str the error message of the caught exception. Source code in offchain/metadata/models/metadata_processing_error.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class MetadataProcessingError ( BaseModel ): \"\"\"Interface for metadata processing errors and relevant contextual information. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of the token. token_id (int): token id of the token. uri (str): metadata uri of the token. error_type (str): the class of caught exception. error_message (str): the error message of the caught exception. \"\"\" chain_identifier : str collection_address : str token_id : int uri : Optional [ str ] error_type : str error_message : str @staticmethod def from_token_and_error ( token : Token , e : Exception ) -> \"MetadataProcessingError\" : return MetadataProcessingError ( chain_identifier = token . chain_identifier , collection_address = token . collection_address , token_id = token . token_id , uri = token . uri , error_type = e . __class__ . __name__ , error_message = str ( e ), )","title":"MetadataProcessingError"},{"location":"models/metadata_processing_error/#metadataprocessingerror","text":"Interface for metadata processing errors and relevant contextual information. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of the token. token_id int token id of the token. uri str metadata uri of the token. error_type str the class of caught exception. error_message str the error message of the caught exception. Source code in offchain/metadata/models/metadata_processing_error.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class MetadataProcessingError ( BaseModel ): \"\"\"Interface for metadata processing errors and relevant contextual information. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of the token. token_id (int): token id of the token. uri (str): metadata uri of the token. error_type (str): the class of caught exception. error_message (str): the error message of the caught exception. \"\"\" chain_identifier : str collection_address : str token_id : int uri : Optional [ str ] error_type : str error_message : str @staticmethod def from_token_and_error ( token : Token , e : Exception ) -> \"MetadataProcessingError\" : return MetadataProcessingError ( chain_identifier = token . chain_identifier , collection_address = token . collection_address , token_id = token . token_id , uri = token . uri , error_type = e . __class__ . __name__ , error_message = str ( e ), )","title":"MetadataProcessingError"},{"location":"models/token/","text":"Token Token interface with all information required to uniquely identify an NFT. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of token. token_id int unique identifier of token. uri str the uri where the metadata is stored. Source code in offchain/metadata/models/token.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Token ( BaseModel ): \"\"\"Token interface with all information required to uniquely identify an NFT. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of token. token_id (int): unique identifier of token. uri (str, optional): the uri where the metadata is stored. \"\"\" chain_identifier : str collection_address : str token_id : int uri : Optional [ str ] = None","title":"Token"},{"location":"models/token/#token","text":"Token interface with all information required to uniquely identify an NFT. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of token. token_id int unique identifier of token. uri str the uri where the metadata is stored. Source code in offchain/metadata/models/token.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Token ( BaseModel ): \"\"\"Token interface with all information required to uniquely identify an NFT. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of token. token_id (int): unique identifier of token. uri (str, optional): the uri where the metadata is stored. \"\"\" chain_identifier : str collection_address : str token_id : int uri : Optional [ str ] = None","title":"Token"},{"location":"pipeline/adapters/","text":"Adapters IPFS Provides an interface for Requests sessions to contact IPFS urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/ipfs.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 @AdapterRegistry . register class IPFSAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact IPFS urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://gateway.pinata.cloud/ipfs/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def make_request_url ( self , request_url : str ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url Returns: str: formatted IPFS url \"\"\" parsed_url = parse_url ( request_url ) gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" # Handle \"ipfs://\" prefixed urls if parsed_url . scheme == \"ipfs\" : # Don't duplicate since gateways already have \"ipfs/\" if parsed_url . host != \"ipfs\" : host = parsed_url . host # Remove duplicate slashes if url . endswith ( \"/\" ) and host . startswith ( \"/\" ): host = host [ 1 :] url += host if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] url += path # Handle \"https://\" prefixed urls that have \"/ipfs/\" in the path elif parsed_url . scheme == \"https\" and \"ipfs\" in parsed_url . path : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] if path . startswith ( \"ipfs/\" ): path = path [ 5 :] url += path return url def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) make_request_url ( request_url ) Parse and format incoming IPFS request url Parameters: Name Type Description Default request_url str incoming IPFS request url required Returns: Name Type Description str str formatted IPFS url Source code in offchain/metadata/adapters/ipfs.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def make_request_url ( self , request_url : str ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url Returns: str: formatted IPFS url \"\"\" parsed_url = parse_url ( request_url ) gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" # Handle \"ipfs://\" prefixed urls if parsed_url . scheme == \"ipfs\" : # Don't duplicate since gateways already have \"ipfs/\" if parsed_url . host != \"ipfs\" : host = parsed_url . host # Remove duplicate slashes if url . endswith ( \"/\" ) and host . startswith ( \"/\" ): host = host [ 1 :] url += host if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] url += path # Handle \"https://\" prefixed urls that have \"/ipfs/\" in the path elif parsed_url . scheme == \"https\" and \"ipfs\" in parsed_url . path : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] if path . startswith ( \"ipfs/\" ): path = path [ 5 :] url += path return url send ( request , * args , ** kwargs ) For IPFS hashes, query pinata cloud gateway Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from IPFS Gateway Source code in offchain/metadata/adapters/ipfs.py 81 82 83 84 85 86 87 88 89 90 91 92 93 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) ARWeave Provides an interface for Requests sessions to contact ARWeave urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/arweave.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @AdapterRegistry . register class ARWeaveAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact ARWeave urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://arweave.net/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) send ( request , * args , ** kwargs ) Format and send request to ARWeave host. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) Data URI Provides an interface for Requests sessions to handle data uris. Source code in offchain/metadata/adapters/data_uri.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @AdapterRegistry . register class DataURIAdapter ( BaseAdapter ): \"\"\"Provides an interface for Requests sessions to handle data uris.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse def close ( self ): self . response . close () send ( request , * args , ** kwargs ) Handle data uri request. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse","title":"Adapters"},{"location":"pipeline/adapters/#adapters","text":"","title":"Adapters"},{"location":"pipeline/adapters/#ipfs","text":"Provides an interface for Requests sessions to contact IPFS urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/ipfs.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 @AdapterRegistry . register class IPFSAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact IPFS urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://gateway.pinata.cloud/ipfs/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def make_request_url ( self , request_url : str ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url Returns: str: formatted IPFS url \"\"\" parsed_url = parse_url ( request_url ) gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" # Handle \"ipfs://\" prefixed urls if parsed_url . scheme == \"ipfs\" : # Don't duplicate since gateways already have \"ipfs/\" if parsed_url . host != \"ipfs\" : host = parsed_url . host # Remove duplicate slashes if url . endswith ( \"/\" ) and host . startswith ( \"/\" ): host = host [ 1 :] url += host if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] url += path # Handle \"https://\" prefixed urls that have \"/ipfs/\" in the path elif parsed_url . scheme == \"https\" and \"ipfs\" in parsed_url . path : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] if path . startswith ( \"ipfs/\" ): path = path [ 5 :] url += path return url def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"IPFS"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.make_request_url","text":"Parse and format incoming IPFS request url Parameters: Name Type Description Default request_url str incoming IPFS request url required Returns: Name Type Description str str formatted IPFS url Source code in offchain/metadata/adapters/ipfs.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def make_request_url ( self , request_url : str ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url Returns: str: formatted IPFS url \"\"\" parsed_url = parse_url ( request_url ) gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" # Handle \"ipfs://\" prefixed urls if parsed_url . scheme == \"ipfs\" : # Don't duplicate since gateways already have \"ipfs/\" if parsed_url . host != \"ipfs\" : host = parsed_url . host # Remove duplicate slashes if url . endswith ( \"/\" ) and host . startswith ( \"/\" ): host = host [ 1 :] url += host if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] url += path # Handle \"https://\" prefixed urls that have \"/ipfs/\" in the path elif parsed_url . scheme == \"https\" and \"ipfs\" in parsed_url . path : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway } \" if parsed_url . path is not None : path = parsed_url . path # Remove duplicate slashes if url . endswith ( \"/\" ) and path . startswith ( \"/\" ): path = path [ 1 :] if path . startswith ( \"ipfs/\" ): path = path [ 5 :] url += path return url","title":"make_request_url()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.send","text":"For IPFS hashes, query pinata cloud gateway Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from IPFS Gateway Source code in offchain/metadata/adapters/ipfs.py 81 82 83 84 85 86 87 88 89 90 91 92 93 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"send()"},{"location":"pipeline/adapters/#arweave","text":"Provides an interface for Requests sessions to contact ARWeave urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/arweave.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @AdapterRegistry . register class ARWeaveAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact ARWeave urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://arweave.net/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"ARWeave"},{"location":"pipeline/adapters/#offchain.metadata.adapters.arweave.ARWeaveAdapter.send","text":"Format and send request to ARWeave host. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"send()"},{"location":"pipeline/adapters/#data-uri","text":"Provides an interface for Requests sessions to handle data uris. Source code in offchain/metadata/adapters/data_uri.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @AdapterRegistry . register class DataURIAdapter ( BaseAdapter ): \"\"\"Provides an interface for Requests sessions to handle data uris.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse def close ( self ): self . response . close ()","title":"Data URI"},{"location":"pipeline/adapters/#offchain.metadata.adapters.data_uri.DataURIAdapter.send","text":"Handle data uri request. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse","title":"send()"},{"location":"pipeline/fetchers/","text":"Fetchers MetadataFetcher Metadata fetcher class Attributes: Name Type Description timeout int request timeout in seconds. max_retries int maximum number of request retries. sess requests . Session a requests Session object. Source code in offchain/metadata/fetchers/metadata_fetcher.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @FetcherRegistry . register class MetadataFetcher ( BaseFetcher ): \"\"\"Metadata fetcher class Attributes: timeout (int): request timeout in seconds. max_retries (int): maximum number of request retries. sess (requests.Session): a requests Session object. \"\"\" def __init__ ( self , timeout : int = 30 , max_retries : int = 0 , ) -> None : self . timeout = timeout self . max_retries = max_retries self . sess = requests . Session () def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout def _head ( self , uri : str ): return self . sess . head ( uri , timeout = self . timeout , allow_redirects = True ) def _get ( self , uri : str ): return self . sess . get ( uri , timeout = self . timeout , allow_redirects = True ) def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) fetch_content ( uri ) Fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) fetch_mime_type_and_size ( uri ) Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise register_adapter ( adapter , url_prefix ) Register an adapter to a url prefix. Parameters: Name Type Description Default adapter Adapter an Adapter instance to register. required url_prefix str the url prefix to which the adapter should be registered. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 30 31 32 33 34 35 36 37 def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) set_max_retries ( max_retries ) Setter function for max retries Parameters: Name Type Description Default max_retries int new maximum number of request retries. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 39 40 41 42 43 44 45 def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries set_timeout ( timeout ) Setter function for timeout Parameters: Name Type Description Default timeout int new request timeout in seconds. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 47 48 49 50 51 52 53 def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout","title":"Fetchers"},{"location":"pipeline/fetchers/#fetchers","text":"","title":"Fetchers"},{"location":"pipeline/fetchers/#metadatafetcher","text":"Metadata fetcher class Attributes: Name Type Description timeout int request timeout in seconds. max_retries int maximum number of request retries. sess requests . Session a requests Session object. Source code in offchain/metadata/fetchers/metadata_fetcher.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @FetcherRegistry . register class MetadataFetcher ( BaseFetcher ): \"\"\"Metadata fetcher class Attributes: timeout (int): request timeout in seconds. max_retries (int): maximum number of request retries. sess (requests.Session): a requests Session object. \"\"\" def __init__ ( self , timeout : int = 30 , max_retries : int = 0 , ) -> None : self . timeout = timeout self . max_retries = max_retries self . sess = requests . Session () def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout def _head ( self , uri : str ): return self . sess . head ( uri , timeout = self . timeout , allow_redirects = True ) def _get ( self , uri : str ): return self . sess . get ( uri , timeout = self . timeout , allow_redirects = True ) def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"MetadataFetcher"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.fetch_content","text":"Fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"fetch_content()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.fetch_mime_type_and_size","text":"Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise","title":"fetch_mime_type_and_size()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.register_adapter","text":"Register an adapter to a url prefix. Parameters: Name Type Description Default adapter Adapter an Adapter instance to register. required url_prefix str the url prefix to which the adapter should be registered. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 30 31 32 33 34 35 36 37 def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter )","title":"register_adapter()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.set_max_retries","text":"Setter function for max retries Parameters: Name Type Description Default max_retries int new maximum number of request retries. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 39 40 41 42 43 44 45 def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries","title":"set_max_retries()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.set_timeout","text":"Setter function for timeout Parameters: Name Type Description Default timeout int new request timeout in seconds. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 47 48 49 50 51 52 53 def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout","title":"set_timeout()"},{"location":"pipeline/parsers/","text":"Parsers CollectionParser Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] should_parse_token ( token , * args , ** kwargs ) Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 34 35 36 37 38 39 40 41 42 43 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] SchemaParser Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : self . fetcher = fetcher or MetadataFetcher ()","title":"Parsers"},{"location":"pipeline/parsers/#parsers","text":"","title":"Parsers"},{"location":"pipeline/parsers/#collectionparser","text":"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ]","title":"CollectionParser"},{"location":"pipeline/parsers/#offchain.metadata.parsers.metadata.collection_parser.CollectionParser.should_parse_token","text":"Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 34 35 36 37 38 39 40 41 42 43 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ]","title":"should_parse_token()"},{"location":"pipeline/parsers/#schemaparser","text":"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : self . fetcher = fetcher or MetadataFetcher ()","title":"SchemaParser"},{"location":"pipeline/pipeline/","text":"Pipeline MetadataPipeline Pipeline for processing NFT metadata By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: Name Type Description contract_caller ContractCaller a contract caller instance for fetching data from contracts. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers list [ BaseParser ] a list of parser instances for parsing token metadata. adapter_configs list [ BaseParser ] (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. Source code in offchain/metadata/pipelines/metadata_pipeline.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 class MetadataPipeline ( BasePipeline ): \"\"\"Pipeline for processing NFT metadata By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. fetcher (BaseFetcher, optional): a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers (list[BaseParser], optional): a list of parser instances for parsing token metadata. adapter_configs: (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. \"\"\" def __init__ ( self , contract_caller : Optional [ ContractCaller ] = None , fetcher : Optional [ BaseFetcher ] = None , parsers : Optional [ list [ BaseParser ]] = None , adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () if adapter_configs is None : adapter_configs = DEFAULT_ADAPTER_CONFIGS for adapter_config in adapter_configs : self . mount_adapter ( adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ), url_prefixes = adapter_config . mount_prefixes , ) if parsers is None : parsers = [ parser_cls ( fetcher = self . fetcher , contract_caller = self . contract_caller ) for parser_cls in DEFAULT_PARSERS ] self . parsers = parsers def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ] def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors fetch_token_metadata ( token , metadata_selector_fn = None ) Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ] fetch_token_uri ( token , function_signature = 'tokenURI(uint256)' ) Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None mount_adapter ( adapter , url_prefixes ) Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Parameters: Name Type Description Default adapter Adapter Adapter instance required url_prefixes list [ str ] list of url prefixes to which to mount the adapter. required Source code in offchain/metadata/pipelines/metadata_pipeline.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) run ( tokens , parallelize = True , select_metadata_fn = None , * args , ** kwargs ) Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required parallelize bool whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. True select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors","title":"Pipeline"},{"location":"pipeline/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"pipeline/pipeline/#metadatapipeline","text":"Pipeline for processing NFT metadata By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: Name Type Description contract_caller ContractCaller a contract caller instance for fetching data from contracts. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers list [ BaseParser ] a list of parser instances for parsing token metadata. adapter_configs list [ BaseParser ] (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. Source code in offchain/metadata/pipelines/metadata_pipeline.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 class MetadataPipeline ( BasePipeline ): \"\"\"Pipeline for processing NFT metadata By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. fetcher (BaseFetcher, optional): a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers (list[BaseParser], optional): a list of parser instances for parsing token metadata. adapter_configs: (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. \"\"\" def __init__ ( self , contract_caller : Optional [ ContractCaller ] = None , fetcher : Optional [ BaseFetcher ] = None , parsers : Optional [ list [ BaseParser ]] = None , adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () if adapter_configs is None : adapter_configs = DEFAULT_ADAPTER_CONFIGS for adapter_config in adapter_configs : self . mount_adapter ( adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ), url_prefixes = adapter_config . mount_prefixes , ) if parsers is None : parsers = [ parser_cls ( fetcher = self . fetcher , contract_caller = self . contract_caller ) for parser_cls in DEFAULT_PARSERS ] self . parsers = parsers def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ] def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors","title":"MetadataPipeline"},{"location":"pipeline/pipeline/#offchain.metadata.metadata.metadata_pipeline.MetadataPipeline.fetch_token_metadata","text":"Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ]","title":"fetch_token_metadata()"},{"location":"pipeline/pipeline/#offchain.metadata.metadata.metadata_pipeline.MetadataPipeline.fetch_token_uri","text":"Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None","title":"fetch_token_uri()"},{"location":"pipeline/pipeline/#offchain.metadata.metadata.metadata_pipeline.MetadataPipeline.mount_adapter","text":"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Parameters: Name Type Description Default adapter Adapter Adapter instance required url_prefixes list [ str ] list of url prefixes to which to mount the adapter. required Source code in offchain/metadata/pipelines/metadata_pipeline.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix )","title":"mount_adapter()"},{"location":"pipeline/pipeline/#offchain.metadata.metadata.metadata_pipeline.MetadataPipeline.run","text":"Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required parallelize bool whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. True select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors","title":"run()"}]}