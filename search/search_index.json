{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started Documentation for version: v0.1.1 Overview offchain is an open-source Python package for processing both onchain and offchain NFT metadata. The purpose of this project is to enable anyone to define and standardize their own custom NFT metadata schema. offchain powers the Zora Indexer, and any contributions to metadata parsing implementations will be reflected in the Zora API. If you'd like to contribute your own metadata parsing implementation, please see the Contributing page. Installation with pip: pip install offchain with poetry: poetry add offchain from repository: pip install git+https://github.com/ourzora/offchain.git","title":"Getting started"},{"location":"#getting-started","text":"Documentation for version: v0.1.1","title":"Getting started"},{"location":"#overview","text":"offchain is an open-source Python package for processing both onchain and offchain NFT metadata. The purpose of this project is to enable anyone to define and standardize their own custom NFT metadata schema. offchain powers the Zora Indexer, and any contributions to metadata parsing implementations will be reflected in the Zora API. If you'd like to contribute your own metadata parsing implementation, please see the Contributing page.","title":"Overview"},{"location":"#installation","text":"with pip: pip install offchain with poetry: poetry add offchain from repository: pip install git+https://github.com/ourzora/offchain.git","title":"Installation"},{"location":"changelog/","text":"Changelog v0.1.1 add a get_token_metadata() function for simple use cases add basic validation logic for chain_identifier field on Token expose get_token_metadata , Metadata , MetadataFetcher , MetadataPipeline , MetadataProcessingError , and Token as root-level imports v0.1.0 define all interfaces and pipeline components add documentation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v011","text":"add a get_token_metadata() function for simple use cases add basic validation logic for chain_identifier field on Token expose get_token_metadata , Metadata , MetadataFetcher , MetadataPipeline , MetadataProcessingError , and Token as root-level imports","title":"v0.1.1"},{"location":"changelog/#v010","text":"define all interfaces and pipeline components add documentation","title":"v0.1.0"},{"location":"contributing/guidelines/","text":"Guidelines Contributing a collection parser If you have a specific NFT collection you'd like to support, you can contribute a collection parser. A collection parser must specify the collection address(es) that it should be run on. Contributing a schema parser If you have a new metadata format you'd like to support, you can add a schema parser. A new schema parser should only be defined if the metadata format spans across multiple collections, and new collections will be created that use this format. the parsed schema is sufficiently different from that returned by a CatchallParser . there is a way to uniquely identify tokens that have this metadata format, without including other metadata formats. Contributing an adapter We currently support HTTP, Data URI, IPFS, and ARWeave url formats. If you have another url format you'd like to support, you can write a custom adapter to handle your url format.","title":"Guidelines"},{"location":"contributing/guidelines/#guidelines","text":"","title":"Guidelines"},{"location":"contributing/guidelines/#contributing-a-collection-parser","text":"If you have a specific NFT collection you'd like to support, you can contribute a collection parser. A collection parser must specify the collection address(es) that it should be run on.","title":"Contributing a collection parser"},{"location":"contributing/guidelines/#contributing-a-schema-parser","text":"If you have a new metadata format you'd like to support, you can add a schema parser. A new schema parser should only be defined if the metadata format spans across multiple collections, and new collections will be created that use this format. the parsed schema is sufficiently different from that returned by a CatchallParser . there is a way to uniquely identify tokens that have this metadata format, without including other metadata formats.","title":"Contributing a schema parser"},{"location":"contributing/guidelines/#contributing-an-adapter","text":"We currently support HTTP, Data URI, IPFS, and ARWeave url formats. If you have another url format you'd like to support, you can write a custom adapter to handle your url format.","title":"Contributing an adapter"},{"location":"contributing/metadata_tutorial/","text":"Adding a new metadata format To add support for a new metadata format, you'll need to add a new parser to the offchain repo. In this guide, we'll build a parser for the ENS collection from scratch and go over important considerations for building your own parser. Step 1: Determine the type of parser The first consideration is determining which type of parser to build. Before implementing your parser, familiarize yourself with the BaseParser , CollectionParser , and SchemaParser base classes. Your parser will be one of the following: CollectionParser : determines if it should run on a token by looking at the token's collection address SchemaParser : determines if it should run on a token by looking at the shape of the token's metadata As a general rule of thumb, you should only define a new schema parser if the answer to all of the following questions is Yes : Is there a way to uniquely identify tokens that have this new metadata format? Will there be new NFT collections that use this new metadata format? Does the default parser in the pipeline parse the this new metadata format incorrectly? Since, we're building a parser for the ENS collection, we'll be building a CollectionParser . class ENSParser ( CollectionParser ): pass Step 2: Defining the selection criteria The next step is to define your parser's selection criteria. This tells the pipeline which tokens to run your parser on. If you're building a schema parser, you'll need to override the should_parse_token() method of BaseParser to implement custom selection logic based on the shape of the metadata. For instance, if the new metadata schema contains a unique field, checking for the existence of that field would qualify as selection criteria: def should_parse_token ( self , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : return raw_data is not None and raw_data . get ( \"unique_field\" ) is not None If you're building a collection parser, the selection criteria is simply defined by a _COLLECTION_ADDRESSES class variable, which tells the parser which collection address(es) to run on. The ENS collection parser will only run on tokens with the ENS collection contract address ( 0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85 ). class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] Step 3: Writing the metadata parsing implementation Step 3a: Constructing the token uri The token uri is needed to tell the parser where to fetch the metadata from. If the token uri is not passed in as part of the input, the pipeline will attempt to fetch it from a tokenURI(uint256) view function on the contract. Otherwise, it is expected that the parser will construct the token uri. Note: it is not uncommon for token uris to be base64 encoded data is stored entirely on chain. This is the case for collections like Nouns or Zorbs. ENS hosts their own metadata service and token uris are constructed in the following format: https://metadata.ens.domains/<chain_name>/<collection_address>/<token_id>/ class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) Let's use this ENS NFT as an example: Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) If we pass it into the parser, we'll get the following uri: https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396 , which returns metadata information from the ENS metadata service. Step 3b: Fetching metadata from the token uri Once you have the token uri, we can use the Fetcher to fetch the raw JSON data from the token uri. By default, the parser is initialized with a Fetcher instance with an HTTP adapter. raw_data = self . fetcher . fetch_content ( token . uri ) This should return the following data from the ENS metadata service: { \"is_normalized\" : true , \"name\" : \"steev.eth\" , \"description\" : \"steev.eth, an ENS name.\" , \"attributes\" : [ { \"trait_type\" : \"Created Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Segment Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Character Set\" , \"display_type\" : \"string\" , \"value\" : \"letter\" }, { \"trait_type\" : \"Registration Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Expiration Date\" , \"display_type\" : \"date\" , \"value\" : 1822465450000 } ], \"name_length\" : 5 , \"segment_length\" : 5 , \"url\" : \"https://app.ens.domains/name/steev.eth\" , \"version\" : 0 , \"background_image\" : \"https://metadata.ens.domains/mainnet/avatar/steev.eth\" , \"image\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" , \"image_url\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" } Step 3c: Standardize the new metadata format The next step is to convert the metadata into the standardized metadata format . Each field in the new metadata format should either map a field in the standardized metadata format or be added as an MetadataField under the additional_fields property. In the case of ENS, the metadata format has the following fields: { \"name\" : \"ENS name\" , \"description\" : \"Short ENS name description\" , \"attributes\" : \"Custom traits about ENS\" , \"name_length\" : \"Character length of ens name\" , \"url\" : \"ENS App URL of the name\" , \"version\" : \"ENS NFT version\" , \"background_image\" : \"Origin URL of avatar image\" , \"image_url\" : \"URL of ENS NFT image\" } Each of these fields can be mapped into the standard metadata format: Standard Metadata Field New Metadata Field token raw_data standard attributes attributes name name description description mime_type image image_url content background_image additional_fields name_length, url, version And this is how it would look programatically: class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), ) Step 4: Registering your parser After writing your custom metadata parser implementation, you'll want to register it to the ParserRegistry . The ParserRegistry tracks all parsers and is used by the metadata pipeline to know which parsers to run by default. @ParserRegistry . register class ENSParser ( CollectionParser ): ... Note: in order to have the parser be registered, you'll also need to import it in offchain/metadata/parsers/__init__.py . If you're developing locally, you still need to import the ParserRegistry to register your parser. The parser must be registered in order for it to be run by default in the MetadataPipeline . In the example below, we register the ENSParser class locally and run get_token_metadata() , which leverages the MetadataPipeline . from offchain import get_token_metadata from offchain.metadata import CollectionParser from offchain.metadata.registries.parser_registry import ParserRegistry @ParserRegistry . register class ENSParser ( CollectionParser ): ... get_token_metadata ( collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = \"ETHEREUM-MAINNET\" ) Step 5: Testing your parser Step 5a: Writing unit tests You'll want to write tests to verify that your parser works as expected. At minimum, the should_parse_token() and parse_metadata() functions should be tested because the pipeline will call those directly. It's important to verify that the should_parse_token() function returns True if and only if a token is meant to be parsed by that parser. Given a token, parse_metadata() should normalize the raw data into the standardized metadata format. Since making network requests can be flaky, it's preferable to mock the data that would be returned by the server that hosts the metadata information. def test_ens_parser_should_parse_token ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . should_parse_token ( token = token ) == True def test_ens_parser_parses_metadata ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () fetcher . fetch_mime_type_and_size = MagicMock ( return_value = ( \"application/json\" , 41145 )) fetcher . fetch_content = MagicMock ( return_value = mocked_raw_data ) parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . parse_metadata ( token = token , raw_data = None ) == expected_metadata In addition to testing your parser, you'll need to verify that the parser has been registered and added to the pipeline correctly. The tests in tests/metadata/registries/test_parser_registry.py should break if the not modified to include your new parser class. Step 5b: Testing manually It's always good practice to test manually as well. We can set up our pipeline using the example NFT from earlier: from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) metadata = pipeline . run ([ token ])[ 0 ] This should give us the following standardized metadata: Metadata ( token = Token ( collection_address = '0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85' token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = 'ETHEREUM-MAINNET' , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396/' ), raw_data = { 'is_normalized' : True , 'name' : 'steev.eth' , 'description' : 'steev.eth, an ENS name.' , 'attributes' : [ { 'trait_type' : 'Created Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Segment Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Character Set' , 'display_type' : 'string' , 'value' : 'letter' }, { 'trait_type' : 'Registration Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Expiration Date' , 'display_type' : 'date' , 'value' : 1822465450000 } ], 'name_length' : 5 , 'segment_length' : 5 , 'url' : 'https://app.ens.domains/name/steev.eth' , 'version' : 0 , 'background_image' : 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , 'image' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , 'image_url' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' }, attributes = [ Attribute ( trait_type = 'Created Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Segment Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Character Set' , value = 'letter' , display_type = 'string' ), Attribute ( trait_type = 'Registration Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Expiration Date' , value = '1822465450000' , display_type = 'date' ) ], standard = COLLECTION_STANDARD , name = 'steev.eth' , description = 'steev.eth, an ENS name.' , mime_type = 'application/json' , image = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , mime_type = 'image/svg+xml' ), content = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , mime_type = None ), additional_fields = [ MetadataField ( field_name = 'name_length' , type = TEXT , description = 'Character length of ens name' , value = 5 ), MetadataField ( field_name = 'url' , type = TEXT , description = 'ENS App URL of the name' , value = 'https://app.ens.domains/name/steev.eth' ) ] ) ENS collection parser source code Source code in offchain/metadata/parsers/collection/ens.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @ParserRegistry . register class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ CollectionAddress . ENS ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), )","title":"Adding a new metadata format"},{"location":"contributing/metadata_tutorial/#adding-a-new-metadata-format","text":"To add support for a new metadata format, you'll need to add a new parser to the offchain repo. In this guide, we'll build a parser for the ENS collection from scratch and go over important considerations for building your own parser.","title":"Adding a new metadata format"},{"location":"contributing/metadata_tutorial/#step-1-determine-the-type-of-parser","text":"The first consideration is determining which type of parser to build. Before implementing your parser, familiarize yourself with the BaseParser , CollectionParser , and SchemaParser base classes. Your parser will be one of the following: CollectionParser : determines if it should run on a token by looking at the token's collection address SchemaParser : determines if it should run on a token by looking at the shape of the token's metadata As a general rule of thumb, you should only define a new schema parser if the answer to all of the following questions is Yes : Is there a way to uniquely identify tokens that have this new metadata format? Will there be new NFT collections that use this new metadata format? Does the default parser in the pipeline parse the this new metadata format incorrectly? Since, we're building a parser for the ENS collection, we'll be building a CollectionParser . class ENSParser ( CollectionParser ): pass","title":"Step 1: Determine the type of parser"},{"location":"contributing/metadata_tutorial/#step-2-defining-the-selection-criteria","text":"The next step is to define your parser's selection criteria. This tells the pipeline which tokens to run your parser on. If you're building a schema parser, you'll need to override the should_parse_token() method of BaseParser to implement custom selection logic based on the shape of the metadata. For instance, if the new metadata schema contains a unique field, checking for the existence of that field would qualify as selection criteria: def should_parse_token ( self , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : return raw_data is not None and raw_data . get ( \"unique_field\" ) is not None If you're building a collection parser, the selection criteria is simply defined by a _COLLECTION_ADDRESSES class variable, which tells the parser which collection address(es) to run on. The ENS collection parser will only run on tokens with the ENS collection contract address ( 0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85 ). class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ]","title":"Step 2: Defining the selection criteria"},{"location":"contributing/metadata_tutorial/#step-3-writing-the-metadata-parsing-implementation","text":"","title":"Step 3: Writing the metadata parsing implementation"},{"location":"contributing/metadata_tutorial/#step-3a-constructing-the-token-uri","text":"The token uri is needed to tell the parser where to fetch the metadata from. If the token uri is not passed in as part of the input, the pipeline will attempt to fetch it from a tokenURI(uint256) view function on the contract. Otherwise, it is expected that the parser will construct the token uri. Note: it is not uncommon for token uris to be base64 encoded data is stored entirely on chain. This is the case for collections like Nouns or Zorbs. ENS hosts their own metadata service and token uris are constructed in the following format: https://metadata.ens.domains/<chain_name>/<collection_address>/<token_id>/ class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) Let's use this ENS NFT as an example: Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) If we pass it into the parser, we'll get the following uri: https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396 , which returns metadata information from the ENS metadata service.","title":"Step 3a: Constructing the token uri"},{"location":"contributing/metadata_tutorial/#step-3b-fetching-metadata-from-the-token-uri","text":"Once you have the token uri, we can use the Fetcher to fetch the raw JSON data from the token uri. By default, the parser is initialized with a Fetcher instance with an HTTP adapter. raw_data = self . fetcher . fetch_content ( token . uri ) This should return the following data from the ENS metadata service: { \"is_normalized\" : true , \"name\" : \"steev.eth\" , \"description\" : \"steev.eth, an ENS name.\" , \"attributes\" : [ { \"trait_type\" : \"Created Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Segment Length\" , \"display_type\" : \"number\" , \"value\" : 5 }, { \"trait_type\" : \"Character Set\" , \"display_type\" : \"string\" , \"value\" : \"letter\" }, { \"trait_type\" : \"Registration Date\" , \"display_type\" : \"date\" , \"value\" : 1633123738000 }, { \"trait_type\" : \"Expiration Date\" , \"display_type\" : \"date\" , \"value\" : 1822465450000 } ], \"name_length\" : 5 , \"segment_length\" : 5 , \"url\" : \"https://app.ens.domains/name/steev.eth\" , \"version\" : 0 , \"background_image\" : \"https://metadata.ens.domains/mainnet/avatar/steev.eth\" , \"image\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" , \"image_url\" : \"https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image\" }","title":"Step 3b: Fetching metadata from the token uri"},{"location":"contributing/metadata_tutorial/#step-3c-standardize-the-new-metadata-format","text":"The next step is to convert the metadata into the standardized metadata format . Each field in the new metadata format should either map a field in the standardized metadata format or be added as an MetadataField under the additional_fields property. In the case of ENS, the metadata format has the following fields: { \"name\" : \"ENS name\" , \"description\" : \"Short ENS name description\" , \"attributes\" : \"Custom traits about ENS\" , \"name_length\" : \"Character length of ens name\" , \"url\" : \"ENS App URL of the name\" , \"version\" : \"ENS NFT version\" , \"background_image\" : \"Origin URL of avatar image\" , \"image_url\" : \"URL of ENS NFT image\" } Each of these fields can be mapped into the standard metadata format: Standard Metadata Field New Metadata Field token raw_data standard attributes attributes name name description description mime_type image image_url content background_image additional_fields name_length, url, version And this is how it would look programatically: class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ \"0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85\" ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), )","title":"Step 3c: Standardize the new metadata format"},{"location":"contributing/metadata_tutorial/#step-4-registering-your-parser","text":"After writing your custom metadata parser implementation, you'll want to register it to the ParserRegistry . The ParserRegistry tracks all parsers and is used by the metadata pipeline to know which parsers to run by default. @ParserRegistry . register class ENSParser ( CollectionParser ): ... Note: in order to have the parser be registered, you'll also need to import it in offchain/metadata/parsers/__init__.py . If you're developing locally, you still need to import the ParserRegistry to register your parser. The parser must be registered in order for it to be run by default in the MetadataPipeline . In the example below, we register the ENSParser class locally and run get_token_metadata() , which leverages the MetadataPipeline . from offchain import get_token_metadata from offchain.metadata import CollectionParser from offchain.metadata.registries.parser_registry import ParserRegistry @ParserRegistry . register class ENSParser ( CollectionParser ): ... get_token_metadata ( collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = \"ETHEREUM-MAINNET\" )","title":"Step 4: Registering your parser"},{"location":"contributing/metadata_tutorial/#step-5-testing-your-parser","text":"","title":"Step 5: Testing your parser"},{"location":"contributing/metadata_tutorial/#step-5a-writing-unit-tests","text":"You'll want to write tests to verify that your parser works as expected. At minimum, the should_parse_token() and parse_metadata() functions should be tested because the pipeline will call those directly. It's important to verify that the should_parse_token() function returns True if and only if a token is meant to be parsed by that parser. Given a token, parse_metadata() should normalize the raw data into the standardized metadata format. Since making network requests can be flaky, it's preferable to mock the data that would be returned by the server that hosts the metadata information. def test_ens_parser_should_parse_token ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . should_parse_token ( token = token ) == True def test_ens_parser_parses_metadata ( self ): fetcher = MetadataFetcher () contract_caller = ContractCaller () fetcher . fetch_mime_type_and_size = MagicMock ( return_value = ( \"application/json\" , 41145 )) fetcher . fetch_content = MagicMock ( return_value = mocked_raw_data ) parser = ENSParser ( fetcher = fetcher , contract_caller = contract_caller ) assert parser . parse_metadata ( token = token , raw_data = None ) == expected_metadata In addition to testing your parser, you'll need to verify that the parser has been registered and added to the pipeline correctly. The tests in tests/metadata/registries/test_parser_registry.py should break if the not modified to include your new parser class.","title":"Step 5a: Writing unit tests"},{"location":"contributing/metadata_tutorial/#step-5b-testing-manually","text":"It's always good practice to test manually as well. We can set up our pipeline using the example NFT from earlier: from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline from offchain.metadata.models.token import Token pipeline = MetadataPipeline () token = Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85\" , token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , ) metadata = pipeline . run ([ token ])[ 0 ] This should give us the following standardized metadata: Metadata ( token = Token ( collection_address = '0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85' token_id = 10110056301157368922112380646085332716736091604887080310048917803187113883396 , chain_identifier = 'ETHEREUM-MAINNET' , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/10110056301157368922112380646085332716736091604887080310048917803187113883396/' ), raw_data = { 'is_normalized' : True , 'name' : 'steev.eth' , 'description' : 'steev.eth, an ENS name.' , 'attributes' : [ { 'trait_type' : 'Created Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Segment Length' , 'display_type' : 'number' , 'value' : 5 }, { 'trait_type' : 'Character Set' , 'display_type' : 'string' , 'value' : 'letter' }, { 'trait_type' : 'Registration Date' , 'display_type' : 'date' , 'value' : 1633123738000 }, { 'trait_type' : 'Expiration Date' , 'display_type' : 'date' , 'value' : 1822465450000 } ], 'name_length' : 5 , 'segment_length' : 5 , 'url' : 'https://app.ens.domains/name/steev.eth' , 'version' : 0 , 'background_image' : 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , 'image' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , 'image_url' : 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' }, attributes = [ Attribute ( trait_type = 'Created Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Segment Length' , value = '5' , display_type = 'number' ), Attribute ( trait_type = 'Character Set' , value = 'letter' , display_type = 'string' ), Attribute ( trait_type = 'Registration Date' , value = '1633123738000' , display_type = 'date' ), Attribute ( trait_type = 'Expiration Date' , value = '1822465450000' , display_type = 'date' ) ], standard = COLLECTION_STANDARD , name = 'steev.eth' , description = 'steev.eth, an ENS name.' , mime_type = 'application/json' , image = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/0x57f1887a8bf19b14fc0df6fd9b2acc9af147ea85/0x165a16ce2915e51295772b6a67bfc8ceee2c1c7caa85591fba107af4ee24f704/image' , mime_type = 'image/svg+xml' ), content = MediaDetails ( size = 0 , sha256 = None , uri = 'https://metadata.ens.domains/mainnet/avatar/steev.eth' , mime_type = None ), additional_fields = [ MetadataField ( field_name = 'name_length' , type = TEXT , description = 'Character length of ens name' , value = 5 ), MetadataField ( field_name = 'url' , type = TEXT , description = 'ENS App URL of the name' , value = 'https://app.ens.domains/name/steev.eth' ) ] )","title":"Step 5b: Testing manually"},{"location":"contributing/metadata_tutorial/#ens-collection-parser-source-code","text":"Source code in offchain/metadata/parsers/collection/ens.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @ParserRegistry . register class ENSParser ( CollectionParser ): _COLLECTION_ADDRESSES : list [ str ] = [ CollectionAddress . ENS ] @staticmethod def make_ens_chain_name ( chain_identifier : str ): try : return chain_identifier . split ( \"-\" )[ 1 ] . lower () except Exception : logger . error ( f \"Received unexpected chain identifier: { chain_identifier } \" ) return \"mainnet\" def get_additional_fields ( self , raw_data : dict ) -> list [ MetadataField ]: additional_fields = [] if name_length := raw_data . get ( \"name_length\" ): additional_fields . append ( MetadataField ( field_name = \"name_length\" , type = MetadataFieldType . TEXT , description = \"Character length of ens name\" , value = name_length , ) ) if version := raw_data . get ( \"version\" ): additional_fields . append ( MetadataField ( field_name = \"version\" , type = MetadataFieldType . TEXT , description = \"ENS NFT version\" , value = version , ) ) if url := raw_data . get ( \"url\" ): additional_fields . append ( MetadataField ( field_name = \"url\" , type = MetadataFieldType . TEXT , description = \"ENS App URL of the name\" , value = url , ) ) return additional_fields def parse_attributes ( self , raw_data : dict ) -> Optional [ list [ Attribute ]]: attributes = raw_data . get ( \"attributes\" ) if not attributes or not isinstance ( attributes , list ): return return [ Attribute ( trait_type = attribute_dict . get ( \"trait_type\" ), value = attribute_dict . get ( \"value\" ), display_type = attribute_dict . get ( \"display_type\" ), ) for attribute_dict in attributes ] def get_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: image_uri = raw_data . get ( \"image_url\" ) or raw_data . get ( \"image\" ) if image_uri : image = MediaDetails ( uri = image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def get_background_image ( self , raw_data : dict ) -> Optional [ MediaDetails ]: bg_image_uri = raw_data . get ( \"background_image\" ) if bg_image_uri : image = MediaDetails ( uri = bg_image_uri , size = None , sha256 = None , mime_type = None ) try : content_type , size = self . fetcher . fetch_mime_type_and_size ( bg_image_uri ) image . mime_type = content_type image . size = size return image except Exception : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: ens_chain_name = self . make_ens_chain_name ( token . chain_identifier ) token . uri = ( f \"https://metadata.ens.domains/ { ens_chain_name } / { token . collection_address . lower () } / { token . token_id } /\" ) raw_data = self . fetcher . fetch_content ( token . uri ) mime_type , _ = self . fetcher . fetch_mime_type_and_size ( token . uri ) return Metadata ( token = token , raw_data = raw_data , attributes = self . parse_attributes ( raw_data ), name = raw_data . get ( \"name\" ), description = raw_data . get ( \"description\" ), mime_type = mime_type , image = self . get_image ( raw_data = raw_data ), content = self . get_background_image ( raw_data = raw_data ), additional_fields = self . get_additional_fields ( raw_data = raw_data ), )","title":"ENS collection parser source code"},{"location":"models/metadata/","text":"Metadata Metadata A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT raw_data dict raw metadata object fetched from token uri. standard MetadataStandard accepted metadata standard based on the format of the metadata. attributes list [ Attribute ] list of token metadata attributes. name str token name. description str token description. mime_type str metadata mime type, e.g. \"image/png\". image str nested image in the metadata. content str nested content, e.g. video, audio, etc., in the metadata. additional_fields list [ MetadataField ] any additional metadata fields that don't fit in the defined schema. Source code in offchain/metadata/models/metadata.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Metadata ( BaseModel ): \"\"\"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT raw_data (dict): raw metadata object fetched from token uri. standard (MetadataStandard): accepted metadata standard based on the format of the metadata. attributes (list[Attribute]): list of token metadata attributes. name (str, optional): token name. description (str, optional): token description. mime_type (str, optional): metadata mime type, e.g. \"image/png\". image (str, optional): nested image in the metadata. content (str, optional): nested content, e.g. video, audio, etc., in the metadata. additional_fields (list[MetadataField], optional): any additional metadata fields that don't fit in the defined schema. \"\"\" token : Token raw_data : dict attributes : list [ Attribute ] standard : Optional [ MetadataStandard ] = None name : Optional [ str ] = None description : Optional [ str ] = None mime_type : Optional [ str ] = None image : Optional [ MediaDetails ] = None content : Optional [ MediaDetails ] = None additional_fields : Optional [ list [ MetadataField ]] = None Attribute NFT metadata atttribute Attributes: Name Type Description trait_type str the attribute key. value str the attribute value. display_type str informs how the attribute is displayed. Source code in offchain/metadata/models/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 class Attribute ( BaseModel ): \"\"\"NFT metadata atttribute Attributes: trait_type (str, optional): the attribute key. value (str, optional): the attribute value. display_type (str, optional): informs how the attribute is displayed. \"\"\" trait_type : Optional [ str ] = None value : Optional [ str ] = None display_type : Optional [ str ] = None MediaDetails Metadata media information Attributes: Name Type Description size int size of the media. sha256 str the SHA256 hash of the media. uri str the uri at which the media was found. mime_type str the mime type of the media. Source code in offchain/metadata/models/metadata.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class MediaDetails ( BaseModel ): \"\"\"Metadata media information Attributes: size (int, optional): size of the media. sha256 (str, optional): the SHA256 hash of the media. uri (str, optional): the uri at which the media was found. mime_type (str, optional): the mime type of the media. \"\"\" size : Optional [ int ] = None sha256 : Optional [ str ] = None uri : Optional [ str ] = None mime_type : Optional [ str ] = None MetadataField Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: Name Type Description field_name str name of the metadata field. type MetadataFieldType metadata field type. description str a description of what this metadata field represents. value any the value of the metadata field. Source code in offchain/metadata/models/metadata.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MetadataField ( BaseModel ): \"\"\"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: field_name (str): name of the metadata field. type (MetadataFieldType): metadata field type. description (str, optional): a description of what this metadata field represents. value (any): the value of the metadata field. \"\"\" field_name : str type : MetadataFieldType description : str value : Any","title":"Metadata"},{"location":"models/metadata/#metadata","text":"","title":"Metadata"},{"location":"models/metadata/#metadata_1","text":"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT raw_data dict raw metadata object fetched from token uri. standard MetadataStandard accepted metadata standard based on the format of the metadata. attributes list [ Attribute ] list of token metadata attributes. name str token name. description str token description. mime_type str metadata mime type, e.g. \"image/png\". image str nested image in the metadata. content str nested content, e.g. video, audio, etc., in the metadata. additional_fields list [ MetadataField ] any additional metadata fields that don't fit in the defined schema. Source code in offchain/metadata/models/metadata.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Metadata ( BaseModel ): \"\"\"A standard metadata interface This aims to be a relatively comprehensive definition of NFT metadata, but not all metadata will fit cleanly into this shape. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT raw_data (dict): raw metadata object fetched from token uri. standard (MetadataStandard): accepted metadata standard based on the format of the metadata. attributes (list[Attribute]): list of token metadata attributes. name (str, optional): token name. description (str, optional): token description. mime_type (str, optional): metadata mime type, e.g. \"image/png\". image (str, optional): nested image in the metadata. content (str, optional): nested content, e.g. video, audio, etc., in the metadata. additional_fields (list[MetadataField], optional): any additional metadata fields that don't fit in the defined schema. \"\"\" token : Token raw_data : dict attributes : list [ Attribute ] standard : Optional [ MetadataStandard ] = None name : Optional [ str ] = None description : Optional [ str ] = None mime_type : Optional [ str ] = None image : Optional [ MediaDetails ] = None content : Optional [ MediaDetails ] = None additional_fields : Optional [ list [ MetadataField ]] = None","title":"Metadata"},{"location":"models/metadata/#attribute","text":"NFT metadata atttribute Attributes: Name Type Description trait_type str the attribute key. value str the attribute value. display_type str informs how the attribute is displayed. Source code in offchain/metadata/models/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 class Attribute ( BaseModel ): \"\"\"NFT metadata atttribute Attributes: trait_type (str, optional): the attribute key. value (str, optional): the attribute value. display_type (str, optional): informs how the attribute is displayed. \"\"\" trait_type : Optional [ str ] = None value : Optional [ str ] = None display_type : Optional [ str ] = None","title":"Attribute"},{"location":"models/metadata/#mediadetails","text":"Metadata media information Attributes: Name Type Description size int size of the media. sha256 str the SHA256 hash of the media. uri str the uri at which the media was found. mime_type str the mime type of the media. Source code in offchain/metadata/models/metadata.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class MediaDetails ( BaseModel ): \"\"\"Metadata media information Attributes: size (int, optional): size of the media. sha256 (str, optional): the SHA256 hash of the media. uri (str, optional): the uri at which the media was found. mime_type (str, optional): the mime type of the media. \"\"\" size : Optional [ int ] = None sha256 : Optional [ str ] = None uri : Optional [ str ] = None mime_type : Optional [ str ] = None","title":"MediaDetails"},{"location":"models/metadata/#metadatafield","text":"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: Name Type Description field_name str name of the metadata field. type MetadataFieldType metadata field type. description str a description of what this metadata field represents. value any the value of the metadata field. Source code in offchain/metadata/models/metadata.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MetadataField ( BaseModel ): \"\"\"Additional metadata field that does not fit within the defined Metadata entity Some metadata standards will have standardized additional metadata fields that are not captured in our metadata schema. For example, OpenSea allows for an 'external_url' field. Attributes: field_name (str): name of the metadata field. type (MetadataFieldType): metadata field type. description (str, optional): a description of what this metadata field represents. value (any): the value of the metadata field. \"\"\" field_name : str type : MetadataFieldType description : str value : Any","title":"MetadataField"},{"location":"models/metadata_processing_error/","text":"MetadataProcessingError Interface for metadata processing errors and relevant contextual information. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT error_type str the class of caught exception. error_message str the error message of the caught exception. Source code in offchain/metadata/models/metadata_processing_error.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class MetadataProcessingError ( BaseModel ): \"\"\"Interface for metadata processing errors and relevant contextual information. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT error_type (str): the class of caught exception. error_message (str): the error message of the caught exception. \"\"\" token = Token error_type : str error_message : str @staticmethod def from_token_and_error ( token : Token , e : Exception ) -> \"MetadataProcessingError\" : return MetadataProcessingError ( token = token , error_type = e . __class__ . __name__ , error_message = str ( e ), )","title":"MetadataProcessingError"},{"location":"models/metadata_processing_error/#metadataprocessingerror","text":"Interface for metadata processing errors and relevant contextual information. Attributes: Name Type Description token Token a Token interface with all information required to uniquely identify an NFT error_type str the class of caught exception. error_message str the error message of the caught exception. Source code in offchain/metadata/models/metadata_processing_error.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class MetadataProcessingError ( BaseModel ): \"\"\"Interface for metadata processing errors and relevant contextual information. Attributes: token (Token): a Token interface with all information required to uniquely identify an NFT error_type (str): the class of caught exception. error_message (str): the error message of the caught exception. \"\"\" token = Token error_type : str error_message : str @staticmethod def from_token_and_error ( token : Token , e : Exception ) -> \"MetadataProcessingError\" : return MetadataProcessingError ( token = token , error_type = e . __class__ . __name__ , error_message = str ( e ), )","title":"MetadataProcessingError"},{"location":"models/token/","text":"Token Token interface with all information required to uniquely identify an NFT. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of token. token_id int unique identifier of token. uri str the uri where the metadata is stored. Source code in offchain/metadata/models/token.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Token ( BaseModel ): \"\"\"Token interface with all information required to uniquely identify an NFT. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of token. token_id (int): unique identifier of token. uri (str, optional): the uri where the metadata is stored. \"\"\" collection_address : str token_id : int chain_identifier : str = \"ETHEREUM-MAINNET\" uri : Optional [ str ] = None @validator ( \"chain_identifier\" ) def validate_chain_identifier ( cls , chain_identifier ): if not re . match ( \"^[A-Z]*-[A-Z]*$\" , chain_identifier ): raise ValueError ( \"Expected chain identifier to be formatted as NETWORKNAME-CHAINNAME, e.g. ETHEREUM-MAINNET\" ) return chain_identifier","title":"Token"},{"location":"models/token/#token","text":"Token interface with all information required to uniquely identify an NFT. Attributes: Name Type Description chain_identifier str identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address str collection address of token. token_id int unique identifier of token. uri str the uri where the metadata is stored. Source code in offchain/metadata/models/token.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Token ( BaseModel ): \"\"\"Token interface with all information required to uniquely identify an NFT. Attributes: chain_identifier (str): identifier for network and chain of token, formatted as NETWORK_NAME-CHAIN_NAME (e.g. \"ETHEREUM-MAINNET\"). collection_address (str): collection address of token. token_id (int): unique identifier of token. uri (str, optional): the uri where the metadata is stored. \"\"\" collection_address : str token_id : int chain_identifier : str = \"ETHEREUM-MAINNET\" uri : Optional [ str ] = None @validator ( \"chain_identifier\" ) def validate_chain_identifier ( cls , chain_identifier ): if not re . match ( \"^[A-Z]*-[A-Z]*$\" , chain_identifier ): raise ValueError ( \"Expected chain identifier to be formatted as NETWORKNAME-CHAINNAME, e.g. ETHEREUM-MAINNET\" ) return chain_identifier","title":"Token"},{"location":"pipeline/adapters/","text":"Adapters IPFS Provides an interface for Requests sessions to contact IPFS urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/ipfs.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @AdapterRegistry . register class IPFSAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact IPFS urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://gateway.pinata.cloud/ipfs/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url ) def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) make_request_url ( request_url , gateway = None ) Parse and format incoming IPFS request url Parameters: Name Type Description Default request_url str incoming IPFS request url required gateway Optional [ str ] gateway to use when making a request None Returns: Name Type Description str str formatted IPFS url Source code in offchain/metadata/adapters/ipfs.py 82 83 84 85 86 87 88 89 90 91 92 93 94 def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url ) send ( request , * args , ** kwargs ) For IPFS hashes, query pinata cloud gateway Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from IPFS Gateway Source code in offchain/metadata/adapters/ipfs.py 96 97 98 99 100 101 102 103 104 105 106 107 108 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) ARWeave Provides an interface for Requests sessions to contact ARWeave urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/arweave.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @AdapterRegistry . register class ARWeaveAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact ARWeave urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://arweave.net/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) send ( request , * args , ** kwargs ) Format and send request to ARWeave host. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs ) Data URI Provides an interface for Requests sessions to handle data uris. Source code in offchain/metadata/adapters/data_uri.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @AdapterRegistry . register class DataURIAdapter ( BaseAdapter ): \"\"\"Provides an interface for Requests sessions to handle data uris.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse def close ( self ): self . response . close () send ( request , * args , ** kwargs ) Handle data uri request. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse","title":"Adapters"},{"location":"pipeline/adapters/#adapters","text":"","title":"Adapters"},{"location":"pipeline/adapters/#ipfs","text":"Provides an interface for Requests sessions to contact IPFS urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/ipfs.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @AdapterRegistry . register class IPFSAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact IPFS urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://gateway.pinata.cloud/ipfs/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url ) def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"IPFS"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.make_request_url","text":"Parse and format incoming IPFS request url Parameters: Name Type Description Default request_url str incoming IPFS request url required gateway Optional [ str ] gateway to use when making a request None Returns: Name Type Description str str formatted IPFS url Source code in offchain/metadata/adapters/ipfs.py 82 83 84 85 86 87 88 89 90 91 92 93 94 def make_request_url ( self , request_url : str , gateway : Optional [ str ] = None ) -> str : \"\"\"Parse and format incoming IPFS request url Args: request_url (str): incoming IPFS request url gateway (Optional[str]): gateway to use when making a request Returns: str: formatted IPFS url \"\"\" gateway = gateway or random . choice ( self . host_prefixes ) return build_request_url ( gateway = gateway , request_url = request_url )","title":"make_request_url()"},{"location":"pipeline/adapters/#offchain.metadata.adapters.ipfs.IPFSAdapter.send","text":"For IPFS hashes, query pinata cloud gateway Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from IPFS Gateway Source code in offchain/metadata/adapters/ipfs.py 96 97 98 99 100 101 102 103 104 105 106 107 108 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"For IPFS hashes, query pinata cloud gateway Args: request (PreparedRequest): incoming request Returns: Response: response from IPFS Gateway \"\"\" request . url = self . make_request_url ( request . url ) kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"send()"},{"location":"pipeline/adapters/#arweave","text":"Provides an interface for Requests sessions to contact ARWeave urls. Parameters: Name Type Description Default host_prefixes list [ str ] list of possible host url prefixes to choose from None key str optional key to send with request None secret str optional secret to send with request None timeout int request timeout in seconds. Defaults to 10 seconds. 10 Source code in offchain/metadata/adapters/arweave.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @AdapterRegistry . register class ARWeaveAdapter ( HTTPAdapter ): \"\"\"Provides an interface for Requests sessions to contact ARWeave urls. Args: host_prefixes (list[str], optional): list of possible host url prefixes to choose from key (str, optional): optional key to send with request secret (str, optional): optional secret to send with request timeout (int): request timeout in seconds. Defaults to 10 seconds. \"\"\" def __init__ ( self , host_prefixes : Optional [ list [ str ]] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , timeout : int = 10 , * args , ** kwargs , ): self . host_prefixes = host_prefixes or [ \"https://arweave.net/\" ] assert all ([ g . endswith ( \"/\" ) for g in self . host_prefixes ]), \"gateways should have trailing slashes\" self . key = key self . secret = secret self . timeout = timeout super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"ARWeave"},{"location":"pipeline/adapters/#offchain.metadata.adapters.arweave.ARWeaveAdapter.send","text":"Format and send request to ARWeave host. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response Response response from ARWeave host. Source code in offchain/metadata/adapters/arweave.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def send ( self , request : PreparedRequest , * args , ** kwargs ) -> Response : \"\"\"Format and send request to ARWeave host. Args: request (PreparedRequest): incoming request Returns: Response: response from ARWeave host. \"\"\" parsed = parse_url ( request . url ) if parsed . scheme == \"ar\" : gateway = random . choice ( self . host_prefixes ) url = f \" { gateway }{ parsed . host } \" if parsed . path is not None : url += parsed . path request . url = url kwargs [ \"timeout\" ] = self . timeout return super () . send ( request , * args , ** kwargs )","title":"send()"},{"location":"pipeline/adapters/#data-uri","text":"Provides an interface for Requests sessions to handle data uris. Source code in offchain/metadata/adapters/data_uri.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @AdapterRegistry . register class DataURIAdapter ( BaseAdapter ): \"\"\"Provides an interface for Requests sessions to handle data uris.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse def close ( self ): self . response . close ()","title":"Data URI"},{"location":"pipeline/adapters/#offchain.metadata.adapters.data_uri.DataURIAdapter.send","text":"Handle data uri request. Parameters: Name Type Description Default request PreparedRequest incoming request required Returns: Name Type Description Response encoded data uri response. Source code in offchain/metadata/adapters/data_uri.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def send ( self , request : PreparedRequest , * args , ** kwargs ): \"\"\"Handle data uri request. Args: request (PreparedRequest): incoming request Returns: Response: encoded data uri response. \"\"\" newResponse = Response () newResponse . request = request newResponse . url = request . url newResponse . connection = self try : response = urlopen ( request . url ) newResponse . status_code = 200 newResponse . headers = response . headers newResponse . raw = response newResponse . encoding = \"utf-8\" self . response = response finally : return newResponse","title":"send()"},{"location":"pipeline/fetchers/","text":"Fetchers MetadataFetcher Fetcher class that makes network requests for metadata-related information. Attributes: Name Type Description timeout int request timeout in seconds. max_retries int maximum number of request retries. sess requests . Session a requests Session object. Source code in offchain/metadata/fetchers/metadata_fetcher.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @FetcherRegistry . register class MetadataFetcher ( BaseFetcher ): \"\"\"Fetcher class that makes network requests for metadata-related information. Attributes: timeout (int): request timeout in seconds. max_retries (int): maximum number of request retries. sess (requests.Session): a requests Session object. \"\"\" def __init__ ( self , timeout : int = 30 , max_retries : int = 0 , ) -> None : self . timeout = timeout self . max_retries = max_retries self . sess = requests . Session () def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout def _head ( self , uri : str ): return self . sess . head ( uri , timeout = self . timeout , allow_redirects = True ) def _get ( self , uri : str ): return self . sess . get ( uri , timeout = self . timeout , allow_redirects = True ) def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) fetch_content ( uri ) Fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" ) fetch_mime_type_and_size ( uri ) Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise register_adapter ( adapter , url_prefix ) Register an adapter to a url prefix. Parameters: Name Type Description Default adapter Adapter an Adapter instance to register. required url_prefix str the url prefix to which the adapter should be registered. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 30 31 32 33 34 35 36 37 def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) set_max_retries ( max_retries ) Setter function for max retries Parameters: Name Type Description Default max_retries int new maximum number of request retries. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 39 40 41 42 43 44 45 def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries set_timeout ( timeout ) Setter function for timeout Parameters: Name Type Description Default timeout int new request timeout in seconds. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 47 48 49 50 51 52 53 def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout","title":"Fetchers"},{"location":"pipeline/fetchers/#fetchers","text":"","title":"Fetchers"},{"location":"pipeline/fetchers/#metadatafetcher","text":"Fetcher class that makes network requests for metadata-related information. Attributes: Name Type Description timeout int request timeout in seconds. max_retries int maximum number of request retries. sess requests . Session a requests Session object. Source code in offchain/metadata/fetchers/metadata_fetcher.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @FetcherRegistry . register class MetadataFetcher ( BaseFetcher ): \"\"\"Fetcher class that makes network requests for metadata-related information. Attributes: timeout (int): request timeout in seconds. max_retries (int): maximum number of request retries. sess (requests.Session): a requests Session object. \"\"\" def __init__ ( self , timeout : int = 30 , max_retries : int = 0 , ) -> None : self . timeout = timeout self . max_retries = max_retries self . sess = requests . Session () def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter ) def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout def _head ( self , uri : str ): return self . sess . head ( uri , timeout = self . timeout , allow_redirects = True ) def _get ( self , uri : str ): return self . sess . get ( uri , timeout = self . timeout , allow_redirects = True ) def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"MetadataFetcher"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.fetch_content","text":"Fetch the content at a given uri Parameters: Name Type Description Default uri str uri from which to fetch content. required Returns: Type Description Union [ dict , str ] Union[dict, str]: content fetched from uri Source code in offchain/metadata/fetchers/metadata_fetcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fetch_content ( self , uri : str ) -> Union [ dict , str ]: \"\"\"Fetch the content at a given uri Args: uri (str): uri from which to fetch content. Returns: Union[dict, str]: content fetched from uri \"\"\" try : res = self . _get ( uri ) res . raise_for_status () if res . text . startswith ( \"{\" ): return res . json () else : return res . text except Exception as e : raise Exception ( f \"Don't know how to fetch metadata for { uri =} . { str ( e ) } \" )","title":"fetch_content()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.fetch_mime_type_and_size","text":"Fetch the mime type and size of the content at a given uri. Parameters: Name Type Description Default uri str uri from which to fetch content mime type and size. required Returns: Type Description Tuple [ str , int ] tuple[str, int]: mime type and size Source code in offchain/metadata/fetchers/metadata_fetcher.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def fetch_mime_type_and_size ( self , uri : str ) -> Tuple [ str , int ]: \"\"\"Fetch the mime type and size of the content at a given uri. Args: uri (str): uri from which to fetch content mime type and size. Returns: tuple[str, int]: mime type and size \"\"\" try : res = self . _head ( uri ) # For any error status, try a get if 300 <= res . status_code < 600 : res = self . _get ( uri ) res . raise_for_status () headers = res . headers size = headers . get ( \"content-length\" , 0 ) content_type = headers . get ( \"content-type\" ) or headers . get ( \"Content-Type\" ) if content_type is not None : content_type , _ = cgi . parse_header ( content_type ) return content_type , size except Exception as e : logger . error ( f \"Failed to fetch content-type and size from uri { uri } . Error: { e } \" ) raise","title":"fetch_mime_type_and_size()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.register_adapter","text":"Register an adapter to a url prefix. Parameters: Name Type Description Default adapter Adapter an Adapter instance to register. required url_prefix str the url prefix to which the adapter should be registered. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 30 31 32 33 34 35 36 37 def register_adapter ( self , adapter : Adapter , url_prefix : str ): \"\"\"Register an adapter to a url prefix. Args: adapter (Adapter): an Adapter instance to register. url_prefix (str): the url prefix to which the adapter should be registered. \"\"\" self . sess . mount ( url_prefix , adapter )","title":"register_adapter()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.set_max_retries","text":"Setter function for max retries Parameters: Name Type Description Default max_retries int new maximum number of request retries. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 39 40 41 42 43 44 45 def set_max_retries ( self , max_retries : int ): \"\"\"Setter function for max retries Args: max_retries (int): new maximum number of request retries. \"\"\" self . max_retries = max_retries","title":"set_max_retries()"},{"location":"pipeline/fetchers/#offchain.metadata.fetchers.metadata_fetcher.MetadataFetcher.set_timeout","text":"Setter function for timeout Parameters: Name Type Description Default timeout int new request timeout in seconds. required Source code in offchain/metadata/fetchers/metadata_fetcher.py 47 48 49 50 51 52 53 def set_timeout ( self , timeout : int ): \"\"\"Setter function for timeout Args: timeout (int): new request timeout in seconds. \"\"\" self . timeout = timeout","title":"set_timeout()"},{"location":"pipeline/parsers/","text":"Parsers BaseParser Base protocol for Parser classes Attributes: Name Type Description _METADATA_STANDARD MetadataStandard a class variable defining the metadata standard a parser supports. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making requests. Source code in offchain/metadata/parsers/base_parser.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BaseParser ( Protocol ): \"\"\"Base protocol for Parser classes Attributes: _METADATA_STANDARD (MetadataStandard): a class variable defining the metadata standard a parser supports. fetcher (BaseFetcher): a fetcher instance responsible for fetching content, mime type, and size by making requests. \"\"\" _METADATA_STANDARD : MetadataStandard fetcher : BaseFetcher def __init__ ( self , fetcher : BaseFetcher ) -> None : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" pass def should_parse_token ( self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass parse_metadata ( token , raw_data , * args , ** kwargs ) Given a token and raw data returned from the token uri, return a normalized Metadata object. Parameters: Name Type Description Default token Token token to process metadata for. required raw_data dict raw data returned from token uri. required Returns: Type Description Optional [ Metadata ] Optional[Metadata]: normalized metadata object, if successfully parsed. Source code in offchain/metadata/parsers/base_parser.py 24 25 26 27 28 29 30 31 32 33 34 def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" pass should_parse_token ( token , raw_data , * args , ** kwargs ) Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required raw_data dict raw data returned from token uri. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/base_parser.py 36 37 38 39 40 41 42 43 44 45 46 def should_parse_token ( self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass CollectionParser Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] should_parse_token ( token , * args , ** kwargs ) Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 34 35 36 37 38 39 40 41 42 43 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ] SchemaParser Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : self . fetcher = fetcher or MetadataFetcher ()","title":"Parsers"},{"location":"pipeline/parsers/#parsers","text":"","title":"Parsers"},{"location":"pipeline/parsers/#baseparser","text":"Base protocol for Parser classes Attributes: Name Type Description _METADATA_STANDARD MetadataStandard a class variable defining the metadata standard a parser supports. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making requests. Source code in offchain/metadata/parsers/base_parser.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BaseParser ( Protocol ): \"\"\"Base protocol for Parser classes Attributes: _METADATA_STANDARD (MetadataStandard): a class variable defining the metadata standard a parser supports. fetcher (BaseFetcher): a fetcher instance responsible for fetching content, mime type, and size by making requests. \"\"\" _METADATA_STANDARD : MetadataStandard fetcher : BaseFetcher def __init__ ( self , fetcher : BaseFetcher ) -> None : pass def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" pass def should_parse_token ( self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass","title":"BaseParser"},{"location":"pipeline/parsers/#offchain.metadata.parsers.base_parser.BaseParser.parse_metadata","text":"Given a token and raw data returned from the token uri, return a normalized Metadata object. Parameters: Name Type Description Default token Token token to process metadata for. required raw_data dict raw data returned from token uri. required Returns: Type Description Optional [ Metadata ] Optional[Metadata]: normalized metadata object, if successfully parsed. Source code in offchain/metadata/parsers/base_parser.py 24 25 26 27 28 29 30 31 32 33 34 def parse_metadata ( self , token : Token , raw_data : dict , * args , ** kwargs ) -> Optional [ Metadata ]: \"\"\"Given a token and raw data returned from the token uri, return a normalized Metadata object. Args: token (Token): token to process metadata for. raw_data (dict): raw data returned from token uri. Returns: Optional[Metadata]: normalized metadata object, if successfully parsed. \"\"\" pass","title":"parse_metadata()"},{"location":"pipeline/parsers/#offchain.metadata.parsers.base_parser.BaseParser.should_parse_token","text":"Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required raw_data dict raw data returned from token uri. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/base_parser.py 36 37 38 39 40 41 42 43 44 45 46 def should_parse_token ( self , token : Token , raw_data : Optional [ dict ], * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. raw_data (dict): raw data returned from token uri. Returns: bool: whether or not the collection parser handles this token. \"\"\" pass","title":"should_parse_token()"},{"location":"pipeline/parsers/#collectionparser","text":"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description _COLLECTION_ADDRESSES list [ str ] list of collection addresses that a parser class handles. _METADATA_STANDARD MetadataStandard (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher BaseFetcher a fetcher instance for making network requests. contract_caller ContractCaller a contract caller instance for fetching data from contracts. Source code in offchain/metadata/parsers/collection/collection_parser.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class CollectionParser ( BaseParser ): \"\"\"Base class for collection parsers All parsers that handle collection-based metadata parsing will need to inherit from this base class. Attributes: _COLLECTION_ADDRESSES (list[str]): list of collection addresses that a parser class handles. _METADATA_STANDARD: (MetadataStandard): metadata standard of all metadata returned by this class of parser. Defaults to MetadataStandard.COLLECTION_STANDARD. fetcher (BaseFetcher, optional): a fetcher instance for making network requests. contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. \"\"\" _COLLECTION_ADDRESSES : list [ str ] _METADATA_STANDARD : MetadataStandard = MetadataStandard . COLLECTION_STANDARD def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , contract_caller : Optional [ ContractCaller ] = None , * args , ** kwargs ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ]","title":"CollectionParser"},{"location":"pipeline/parsers/#offchain.metadata.parsers.collection.collection_parser.CollectionParser.should_parse_token","text":"Return whether or not a collection parser should parse a given token. Parameters: Name Type Description Default token Token the token whose metadata needs to be parsed. required Returns: Name Type Description bool bool whether or not the collection parser handles this token. Source code in offchain/metadata/parsers/collection/collection_parser.py 34 35 36 37 38 39 40 41 42 43 def should_parse_token ( self , token : Token , * args , ** kwargs ) -> bool : \"\"\"Return whether or not a collection parser should parse a given token. Args: token (Token): the token whose metadata needs to be parsed. Returns: bool: whether or not the collection parser handles this token. \"\"\" return token . collection_address in [ address . lower () for address in self . _COLLECTION_ADDRESSES ]","title":"should_parse_token()"},{"location":"pipeline/parsers/#schemaparser","text":"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: Name Type Description fetcher BaseFetcher a fetcher instance for making network requests Source code in offchain/metadata/parsers/schema/schema_parser.py 8 9 10 11 12 13 14 15 16 17 18 class SchemaParser ( BaseParser ): \"\"\"Base class for schema parsers All parsers that handle schema-based metadata parsing will need to inherit from this base class. Attributes: fetcher (BaseFetcher, optional): a fetcher instance for making network requests \"\"\" def __init__ ( self , fetcher : Optional [ BaseFetcher ] = None , * args , ** kwargs ) -> None : self . fetcher = fetcher or MetadataFetcher ()","title":"SchemaParser"},{"location":"pipeline/pipeline/","text":"Pipeline MetadataPipeline Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: Name Type Description contract_caller ContractCaller a contract caller instance for fetching data from contracts. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers list [ BaseParser ] a list of parser instances for parsing token metadata. adapter_configs list [ BaseParser ] (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. Source code in offchain/metadata/pipelines/metadata_pipeline.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class MetadataPipeline ( BasePipeline ): \"\"\"Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. fetcher (BaseFetcher, optional): a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers (list[BaseParser], optional): a list of parser instances for parsing token metadata. adapter_configs: (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. \"\"\" def __init__ ( self , contract_caller : Optional [ ContractCaller ] = None , fetcher : Optional [ BaseFetcher ] = None , parsers : Optional [ list [ BaseParser ]] = None , adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () if adapter_configs is None : adapter_configs = DEFAULT_ADAPTER_CONFIGS for adapter_config in adapter_configs : self . mount_adapter ( adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ), url_prefixes = adapter_config . mount_prefixes , ) if parsers is None : parsers = [ parser_cls ( fetcher = self . fetcher , contract_caller = self . contract_caller ) for parser_cls in DEFAULT_PARSERS ] self . parsers = parsers def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ] def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors fetch_token_metadata ( token , metadata_selector_fn = None ) Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ] fetch_token_uri ( token , function_signature = 'tokenURI(uint256)' ) Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None mount_adapter ( adapter , url_prefixes ) Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Parameters: Name Type Description Default adapter Adapter Adapter instance required url_prefixes list [ str ] list of url prefixes to which to mount the adapter. required Source code in offchain/metadata/pipelines/metadata_pipeline.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) run ( tokens , parallelize = True , select_metadata_fn = None , * args , ** kwargs ) Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required parallelize bool whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. True select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors","title":"Pipeline"},{"location":"pipeline/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"pipeline/pipeline/#metadatapipeline","text":"Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: Name Type Description contract_caller ContractCaller a contract caller instance for fetching data from contracts. fetcher BaseFetcher a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers list [ BaseParser ] a list of parser instances for parsing token metadata. adapter_configs list [ BaseParser ] (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. Source code in offchain/metadata/pipelines/metadata_pipeline.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class MetadataPipeline ( BasePipeline ): \"\"\"Pipeline for processing NFT metadata. By default, the parsers are run in order and we will early return when of them returns a valid metadata object. Attributes: contract_caller (ContractCaller, optional): a contract caller instance for fetching data from contracts. fetcher (BaseFetcher, optional): a fetcher instance responsible for fetching content, mime type, and size by making network requests. parsers (list[BaseParser], optional): a list of parser instances for parsing token metadata. adapter_configs: (list[AdapterConfig], optional): a list of adapter configs used to register adapters to specified url prefixes. \"\"\" def __init__ ( self , contract_caller : Optional [ ContractCaller ] = None , fetcher : Optional [ BaseFetcher ] = None , parsers : Optional [ list [ BaseParser ]] = None , adapter_configs : Optional [ list [ AdapterConfig ]] = None , ) -> None : self . contract_caller = contract_caller or ContractCaller () self . fetcher = fetcher or MetadataFetcher () if adapter_configs is None : adapter_configs = DEFAULT_ADAPTER_CONFIGS for adapter_config in adapter_configs : self . mount_adapter ( adapter = adapter_config . adapter_cls ( host_prefixes = adapter_config . host_prefixes , ** adapter_config . kwargs ), url_prefixes = adapter_config . mount_prefixes , ) if parsers is None : parsers = [ parser_cls ( fetcher = self . fetcher , contract_caller = self . contract_caller ) for parser_cls in DEFAULT_PARSERS ] self . parsers = parsers def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix ) def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ] def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors","title":"MetadataPipeline"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.fetch_token_metadata","text":"Fetch metadata for a single token Parameters: Name Type Description Default token Token token for which to fetch metadata. required metadata_selector_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. None Returns: Type Description Union [ Metadata , MetadataProcessingError ] Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. Source code in offchain/metadata/pipelines/metadata_pipeline.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def fetch_token_metadata ( self , token : Token , metadata_selector_fn : Optional [ Callable ] = None , ) -> Union [ Metadata , MetadataProcessingError ]: \"\"\"Fetch metadata for a single token Args: token (Token): token for which to fetch metadata. metadata_selector_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Returns: Union[Metadata, MetadataProcessingError]: returns either a Metadata or a MetadataProcessingError if unable to parse. \"\"\" possible_metadatas_or_errors = [] # If no token uri is passed in, try to fetch the token uri from the contract if token . uri is None : try : token . uri = self . fetch_token_uri ( token ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to fetch token uri. { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) raw_data = None # Try to fetch the raw data from the token uri if token . uri is not None : try : raw_data = self . fetcher . fetch_content ( token . uri ) except Exception as e : error_message = f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) Failed to parse token uri: { token . uri } . { str ( e ) } \" # noqa: E501 logger . error ( error_message ) possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( error_message )) ) for parser in self . parsers : if not parser . should_parse_token ( token = token , raw_data = raw_data ): continue try : metadata_or_error = parser . parse_metadata ( token = token , raw_data = raw_data ) if isinstance ( metadata_or_error , Metadata ): metadata_or_error . standard = parser . _METADATA_STANDARD if metadata_selector_fn is None : return metadata_or_error except Exception as e : metadata_or_error = MetadataProcessingError . from_token_and_error ( token = token , e = e ) possible_metadatas_or_errors . append ( metadata_or_error ) if len ( possible_metadatas_or_errors ) == 0 : possible_metadatas_or_errors . append ( MetadataProcessingError . from_token_and_error ( token = token , e = Exception ( f \"( { token . chain_identifier } - { token . collection_address } - { token . token_id } ) No parsers found.\" ), ) ) if metadata_selector_fn : return metadata_selector_fn ( possible_metadatas_or_errors ) return possible_metadatas_or_errors [ 0 ]","title":"fetch_token_metadata()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.fetch_token_uri","text":"Given a token, fetch the token uri from the contract using a specified function signature. Parameters: Name Type Description Default token Token token whose uri we want to fetch. required function_signature str token uri contract function signature. Defaults to \"tokenURI(uint256)\". 'tokenURI(uint256)' Returns: Type Description Optional [ str ] Optional[str]: the token uri, if found. Source code in offchain/metadata/pipelines/metadata_pipeline.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def fetch_token_uri ( self , token : Token , function_signature : str = \"tokenURI(uint256)\" ) -> Optional [ str ]: \"\"\"Given a token, fetch the token uri from the contract using a specified function signature. Args: token (Token): token whose uri we want to fetch. function_signature (str, optional): token uri contract function signature. Defaults to \"tokenURI(uint256)\". Returns: Optional[str]: the token uri, if found. \"\"\" res = self . contract_caller . single_address_single_fn_many_args ( address = token . collection_address , function_sig = function_signature , return_type = [ \"string\" ], args = [[ token . token_id ]], ) return res [ 0 ] if res and len ( res ) > 0 else None","title":"fetch_token_uri()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.mount_adapter","text":"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Parameters: Name Type Description Default adapter Adapter Adapter instance required url_prefixes list [ str ] list of url prefixes to which to mount the adapter. required Source code in offchain/metadata/pipelines/metadata_pipeline.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def mount_adapter ( self , adapter : Adapter , url_prefixes : list [ str ], ): \"\"\"Given an adapter and list of url prefixes, register the adapter to each of the prefixes. Example Usage: mount_adapter(IPFSAdapter, [\"ipfs://\", \"https://gateway.pinata.cloud/\"]) Args: adapter (Adapter): Adapter instance url_prefixes (list[str]): list of url prefixes to which to mount the adapter. \"\"\" for prefix in url_prefixes : self . fetcher . register_adapter ( adapter , prefix )","title":"mount_adapter()"},{"location":"pipeline/pipeline/#offchain.metadata.pipelines.metadata_pipeline.MetadataPipeline.run","text":"Run metadata pipeline on a list of tokens. Parameters: Name Type Description Default tokens list [ Token ] tokens for which to process metadata. required parallelize bool whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. True select_metadata_fn Optional [ Callable ] optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. None Returns: Type Description list [ Union [ Metadata , MetadataProcessingError ]] list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. Source code in offchain/metadata/pipelines/metadata_pipeline.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run ( self , tokens : list [ Token ], parallelize : bool = True , select_metadata_fn : Optional [ Callable ] = None , * args , ** kwargs , ) -> list [ Union [ Metadata , MetadataProcessingError ]]: \"\"\"Run metadata pipeline on a list of tokens. Args: tokens (list[Token]): tokens for which to process metadata. parallelize (bool, optional): whether or not metadata should be processed in parallel. Defaults to True. Turn off parallelization to reduce risk of getting rate-limited. select_metadata_fn (Optional[Callable], optional): optionally specify a function to select a metadata object from a list of metadata. Defaults to None. Defaults to None. Returns: list[Union[Metadata, MetadataProcessingError]]: returns a list of Metadatas or MetadataProcessingErrors that map 1:1 to the tokens passed in. \"\"\" if len ( tokens ) == 0 : return [] if parallelize : metadatas_or_errors = batched_parmap ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens , 15 ) else : metadatas_or_errors = list ( map ( lambda t : self . fetch_token_metadata ( t , select_metadata_fn ), tokens )) return metadatas_or_errors","title":"run()"},{"location":"usage/customize/","text":"Customizing the pipeline There are countless ways to customize the pipeline. The default MetadataPipeline can be constructed with any permutation of Fetchers , Adapters , Parsers , and ContractCallers . And you can even define your own custom Pipeline by extending the BasePipeline class. In this guide, we'll enumerate few ways you can customize the MetadataPipeline to best suit your needs. Using a custom RPC provider url By default, the pipeline uses https://cloudflare-eth.com as the Ethereum JSON RPC url. This is a free Ethereum RPC provider, which means that it is very easy to exceed the rate-limit. If you have a custom RPC provider url you'd like to use, you can specify it like this: from offchain import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller ) Using custom parsers By default, the pipeline runs with all collection, schema, and catch-all parsers. That said, you may find that you're only interested in using a subset of the parsers. Let's say you're only interested in parsing metadata for a specific collection. If this is the case, you can pass in a list of specific parser instances to run. For example, the following configuration runs the pipeline using only the ENS collection parser. from offchain import MetadataPipeline from offchain.metadata import ENSParser ens_parser = ENSParser () pipeline = MetadataPipeline ( parsers = [ ens_parser ]) Using custom adapters By default, the pipeline is run with all available adapters. Each adapter has a default host prefix and is configured with the following args: {\"pool_connections\": 100, \"pool_maxsize\": 1000, \"max_retries\": 0} You can also customize the pipeline to only use a subset of the adapters. For instance, if you wanted to build a metadata indexer that only indexes onchain metadata, you may opt to only use the IPFS, ARWeave, and DataURI adapters. There are two ways to configure custom adapters for the pipeline: Specifying adapter configs from offchain.metadata.adapters import ARWeaveAdapter , DataURIAdapter , HTTPAdapter , IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import AdapterConfig , MetadataPipeline adapter_configs = [ AdapterConfig ( adapter_cls = ARWeaveAdapter , mount_prefixes = [ \"ar://\" ], host_prefixes = [ \"https://arweave.net/\" ], kwargs = { \"pool_connections\" : 100 , \"pool_maxsize\" : 1000 , \"max_retries\" : 0 }, ), AdapterConfig ( adapter_cls = DataURIAdapter , mount_prefixes = [ \"data:\" ]), ] pipeline = MetadataPipeline ( adapter_configs = adapter_configs ) Mounting custom adapters from offchain.metadata.adapters import IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline pipeline = MetadataPipeline () pipeline . mount_adapter ( adapter = IPFSAdapter ( host_prefixes = [ MY_CUSTOM_IPFS_HOST ], pool_connections = 100 , pool_maxsize = 1000 , max_retries = 0 , ), url_prefixes = [ \"ipfs://\" , \"https://gateway.pinata.cloud/\" , \"https://ipfs.io/\" , ], )","title":"Customizing the pipeline"},{"location":"usage/customize/#customizing-the-pipeline","text":"There are countless ways to customize the pipeline. The default MetadataPipeline can be constructed with any permutation of Fetchers , Adapters , Parsers , and ContractCallers . And you can even define your own custom Pipeline by extending the BasePipeline class. In this guide, we'll enumerate few ways you can customize the MetadataPipeline to best suit your needs.","title":"Customizing the pipeline"},{"location":"usage/customize/#using-a-custom-rpc-provider-url","text":"By default, the pipeline uses https://cloudflare-eth.com as the Ethereum JSON RPC url. This is a free Ethereum RPC provider, which means that it is very easy to exceed the rate-limit. If you have a custom RPC provider url you'd like to use, you can specify it like this: from offchain import MetadataPipeline from offchain.web3.contract_caller import ContractCaller from offchain.web3.jsonrpc import EthereumJSONRPC rpc = EthereumJSONRPC ( provider_url = MY_PROVIDER_URL ) contract_caller = ContractCaller ( rpc = rpc ) pipeline = MetadataPipeline ( contract_caller = contract_caller )","title":"Using a custom RPC provider url"},{"location":"usage/customize/#using-custom-parsers","text":"By default, the pipeline runs with all collection, schema, and catch-all parsers. That said, you may find that you're only interested in using a subset of the parsers. Let's say you're only interested in parsing metadata for a specific collection. If this is the case, you can pass in a list of specific parser instances to run. For example, the following configuration runs the pipeline using only the ENS collection parser. from offchain import MetadataPipeline from offchain.metadata import ENSParser ens_parser = ENSParser () pipeline = MetadataPipeline ( parsers = [ ens_parser ])","title":"Using custom parsers"},{"location":"usage/customize/#using-custom-adapters","text":"By default, the pipeline is run with all available adapters. Each adapter has a default host prefix and is configured with the following args: {\"pool_connections\": 100, \"pool_maxsize\": 1000, \"max_retries\": 0} You can also customize the pipeline to only use a subset of the adapters. For instance, if you wanted to build a metadata indexer that only indexes onchain metadata, you may opt to only use the IPFS, ARWeave, and DataURI adapters. There are two ways to configure custom adapters for the pipeline:","title":"Using custom adapters"},{"location":"usage/customize/#specifying-adapter-configs","text":"from offchain.metadata.adapters import ARWeaveAdapter , DataURIAdapter , HTTPAdapter , IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import AdapterConfig , MetadataPipeline adapter_configs = [ AdapterConfig ( adapter_cls = ARWeaveAdapter , mount_prefixes = [ \"ar://\" ], host_prefixes = [ \"https://arweave.net/\" ], kwargs = { \"pool_connections\" : 100 , \"pool_maxsize\" : 1000 , \"max_retries\" : 0 }, ), AdapterConfig ( adapter_cls = DataURIAdapter , mount_prefixes = [ \"data:\" ]), ] pipeline = MetadataPipeline ( adapter_configs = adapter_configs )","title":"Specifying adapter configs"},{"location":"usage/customize/#mounting-custom-adapters","text":"from offchain.metadata.adapters import IPFSAdapter from offchain.metadata.pipelines.metadata_pipeline import MetadataPipeline pipeline = MetadataPipeline () pipeline . mount_adapter ( adapter = IPFSAdapter ( host_prefixes = [ MY_CUSTOM_IPFS_HOST ], pool_connections = 100 , pool_maxsize = 1000 , max_retries = 0 , ), url_prefixes = [ \"ipfs://\" , \"https://gateway.pinata.cloud/\" , \"https://ipfs.io/\" , ], )","title":"Mounting custom adapters"},{"location":"usage/overview/","text":"Usage Basic usage Fetching metadata for a single token from offchain import get_token_metadata metadata = get_token_metadata ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) metadata . name # -> 'antares the improbable' metadata . description # -> 'You are a WITCH who bathes in the tears of...' metadata . standard # -> OPENSEA_STANDARD metadata . attributes # -> [Attribute(trait_type='Skin Tone', ...] metadata . image # -> MediaDetails(size=2139693, sha256=None, uri='https://cryptocoven.s3.amazonaws.com/2048b255aa1d02045eef13cdd7100479.png', mime_type='image/png') metadata . additional_fields # -> [MetadataField(...), ...] Fetching metadata for multiple tokens from offchain import MetadataPipeline , Token pipeline = MetadataPipeline () token_1 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) token_2 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9560 ) metadatas = pipeline . run ([ token_1 , token_2 ]) Input The Token interface is how the metadata pipeline uniquely identifies an NFT. They are composed of four properties: The chain identifier: the network and chain that this token lives on, by default this is set to \"ETHEREUM-MAINNET\" The collection address: the contract address of the token's collection The token id: the unique identifier for a token within a collection The token URI: the url where the metadata information lives. If this is not passed in, the metadata pipeline will attempt to fetch it from the contract. Example of token 9559 from CryptoCoven on Ethereum Mainnet: from offchain import Token Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) Output The MetadataPipeline is run on a list of Token objects and outputs a list of equal length. Each item in the output list maps to the Token at the same index in the input list. If pipeline successfully fetches metadata for a token, the token should map to a Metadata object. The Metadata interface is a standardized representation of NFT metadata. Conversely, if the pipeline fails to fetch metadata for a token, it should map to a MetadataProcessingError object. The MetadataProcessingError interface defines contextual information for how and why processing metadata for a specific token failed. Pipeline components Pipeline : orchestrates the metadata fetching and normalizing process for multiple tokens. Adapter : parses the metadata url into an acceptable request format for the fetcher. Fetcher : makes requests to a given uri to fetch data. Parser : parses raw data into a standardized metadata format ContractCaller: makes RPC calls to contract view functions. How it works The MetadataPipeline is initialized with a ContractCaller , a Fetcher , a list of Adapters , and a list of Parsers . If no uri is passed in for a token, the pipeline will use the ContractCaller to attempt to fetch it from a tokenURI(uint256) view function on the contract. The pipeline use the Fetcher and Adapters to attempt to fetch metadata in form of raw JSON from the uri. The pipeline runs each parser in the order they were passed in. By default, the ordering is CollectionParsers , SchemaParsers , and then CatchallParsers . The pipeline will return the result of the first parser that is able to successfully parse the token, unless a metadata_selector_fn is specified. If no parser is able to successfully parse a token, the pipeline will return a MetadataProcessingError for that token.","title":"Overview"},{"location":"usage/overview/#usage","text":"","title":"Usage"},{"location":"usage/overview/#basic-usage","text":"","title":"Basic usage"},{"location":"usage/overview/#fetching-metadata-for-a-single-token","text":"from offchain import get_token_metadata metadata = get_token_metadata ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) metadata . name # -> 'antares the improbable' metadata . description # -> 'You are a WITCH who bathes in the tears of...' metadata . standard # -> OPENSEA_STANDARD metadata . attributes # -> [Attribute(trait_type='Skin Tone', ...] metadata . image # -> MediaDetails(size=2139693, sha256=None, uri='https://cryptocoven.s3.amazonaws.com/2048b255aa1d02045eef13cdd7100479.png', mime_type='image/png') metadata . additional_fields # -> [MetadataField(...), ...]","title":"Fetching metadata for a single token"},{"location":"usage/overview/#fetching-metadata-for-multiple-tokens","text":"from offchain import MetadataPipeline , Token pipeline = MetadataPipeline () token_1 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 ) token_2 = Token ( collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9560 ) metadatas = pipeline . run ([ token_1 , token_2 ])","title":"Fetching metadata for multiple tokens"},{"location":"usage/overview/#input","text":"The Token interface is how the metadata pipeline uniquely identifies an NFT. They are composed of four properties: The chain identifier: the network and chain that this token lives on, by default this is set to \"ETHEREUM-MAINNET\" The collection address: the contract address of the token's collection The token id: the unique identifier for a token within a collection The token URI: the url where the metadata information lives. If this is not passed in, the metadata pipeline will attempt to fetch it from the contract. Example of token 9559 from CryptoCoven on Ethereum Mainnet: from offchain import Token Token ( chain_identifier = \"ETHEREUM-MAINNET\" , collection_address = \"0x5180db8f5c931aae63c74266b211f580155ecac8\" , token_id = 9559 )","title":"Input"},{"location":"usage/overview/#output","text":"The MetadataPipeline is run on a list of Token objects and outputs a list of equal length. Each item in the output list maps to the Token at the same index in the input list. If pipeline successfully fetches metadata for a token, the token should map to a Metadata object. The Metadata interface is a standardized representation of NFT metadata. Conversely, if the pipeline fails to fetch metadata for a token, it should map to a MetadataProcessingError object. The MetadataProcessingError interface defines contextual information for how and why processing metadata for a specific token failed.","title":"Output"},{"location":"usage/overview/#pipeline-components","text":"Pipeline : orchestrates the metadata fetching and normalizing process for multiple tokens. Adapter : parses the metadata url into an acceptable request format for the fetcher. Fetcher : makes requests to a given uri to fetch data. Parser : parses raw data into a standardized metadata format ContractCaller: makes RPC calls to contract view functions.","title":"Pipeline components"},{"location":"usage/overview/#how-it-works","text":"The MetadataPipeline is initialized with a ContractCaller , a Fetcher , a list of Adapters , and a list of Parsers . If no uri is passed in for a token, the pipeline will use the ContractCaller to attempt to fetch it from a tokenURI(uint256) view function on the contract. The pipeline use the Fetcher and Adapters to attempt to fetch metadata in form of raw JSON from the uri. The pipeline runs each parser in the order they were passed in. By default, the ordering is CollectionParsers , SchemaParsers , and then CatchallParsers . The pipeline will return the result of the first parser that is able to successfully parse the token, unless a metadata_selector_fn is specified. If no parser is able to successfully parse a token, the pipeline will return a MetadataProcessingError for that token.","title":"How it works"}]}